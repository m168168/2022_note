10： 机器学习



## 1： NB（朴素贝叶斯）

朴素贝叶斯（Natvie Bayes）法是基于贝叶斯定理和特征条件独立假设的分类方法。

常用到的三个模型 

高斯模型、多项式模型、伯努利模型

**模型概述：**

朴素：

- 特征条件独立  
- 每个特征同等重要

贝叶斯：基于贝叶斯定理

1概率相关：

**先验概率**： 比如向女生表白成功的概率是20%，记为P(A)=20% 

**条件概率**：在事件B发生的情况下，事件A发生的概率，用P(A|B)表示，具体计算公式如下。如帅的前提下，向女生表白成功的概率为50%，记为P(A|B)=50%。 
$$
P(A|B) =\frac{P(AB)}{P(B)} \\
P(AB) = P(A|B)P(B)
$$
同理可得：在事件A发生的情况下，事件B发生的概率，用P(B|A)表示，具体计算公式如下： 
$$
P(B|A) =\frac{P(AB)}{P(A)} \\
P(AB) = P(B|A)P(A)
$$
**联合概率**：事件A和B同时发生的概率 。比如，长得帅且向女生表白成功的概率为1%，记为P(AB)=1%

**条件概率和联合概率之间的关系，可用下式表示**： 
$$
P(AB) = P(B|A)P(A) = P(A|B)P(B)
$$
所以就会有： 
$$
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
$$
 **全概率**： 如果事件A不是一个条件，而是一堆条件，这些条件互斥且能穷尽所有可能 。则对任意一个事件B则有 
$$
P(B) = \sum_{i=1}^mP(B|A_i)P(A_i)
$$
**贝叶斯准则:**
$$
如果已知P(B|A_i) ，要求P(A_i|B)，应用贝叶斯准则得到：\\
P(A_i|B)= \frac{P(B|A_i)P(A_i)}{P(B)} \\
要求解 P(A_i|B),只需要知道 P(A_i),P(B|A_i),因为对于同一个数据集，P(B)是一个常量，可以不参与计算。
$$
使用条件概率来分类

如果给定某个由 ![[公式]](https://www.zhihu.com/equation?tex=B_1%2CB_2%2C%C2%B7%C2%B7%C2%B7%2CB_n) 属性表示的数据点，那么该数据点来自类别 ![[公式]](https://www.zhihu.com/equation?tex=A_1) 的概率是多少？数据点来自类别 ![[公式]](https://www.zhihu.com/equation?tex=A_2) 的概率有事多少？可以应用贝叶斯准则得到： 

![[公式]](https://www.zhihu.com/equation?tex=P%28A_i%7CB_1%2CB_2%2C%C2%B7%C2%B7%C2%B7%2CB_n%29%3D%5Cfrac%7BP%28B_1%2CB_2%2C%C2%B7%C2%B7%C2%B7%2CB_n%7CA_i%29P%28A_i%29%7D%7BP%28B_1%2CB_2%2C%C2%B7%C2%B7%C2%B7%2CB_n%29%7D+%5C%5C) 

使用这些定义，可以定义贝叶斯分类准则：

- 如果 ![[公式]](https://www.zhihu.com/equation?tex=P%28A_1%7CB_1%2CB_2%2C...%2CB_n%29+%3E++P%28A_2%7CB_1%2CB_2%2C...%2CB_n%29) ， 则属于类别 ![[公式]](https://www.zhihu.com/equation?tex=A_1) 
- 如果 ![[公式]](https://www.zhihu.com/equation?tex=P%28A_1%7CB_1%2CB_2%2C...%2CB_n%29+%3C%3D++P%28A_2%7CB_1%2CB_2%2C...%2CB_n%29) ， 则属于类别 ![[公式]](https://www.zhihu.com/equation?tex=A_2) 

使用贝叶斯准测，可以通过已知的三个概率值来计算未知的概率值。

通过给定的输入 x,通过学习得到的模型可以计算后验概率分布。

优缺点：

优点： 在数据较少的情况下任然有效，可以处理多分类问题

缺点：

对输入数据的准备方式比较敏感

**朴素贝叶斯种类**

在sklearn中，朴素贝叶斯种类有三种，分别是GaussianNB、MultinomialNB和BernoulliNB。

高斯朴素贝叶斯(GaussianNB)

GaussianNB是先验为高斯分布（正态分布）的朴素贝叶斯，假设每个标签的数据都服从高斯分布（正态分布）。**正态分布的概率密度函数计算公式如下**：

![[公式]](https://www.zhihu.com/equation?tex=P%28X_i%3Dx_i%7CY%3DC_k%29%3D+%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%5Csigma_%7Bk%7D%5E2%7D%7De%5E%7B-%5Cfrac%7B%28x_i-%5Cmu_%7Bk%7D%29%5E2%7D%7B2%5Csigma_%7Bk%7D%7B%5E2%7D%7D%7D+%5C%5C) 

其中， ![[公式]](https://www.zhihu.com/equation?tex=C_k) 为Y的第k类类别。 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_%7Bk%7D) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma_%7Bk%7D%7B%5E2%7D) 为第k类样本在第i个属性上的取值的均值和方差。

**优点**：

- 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，
- 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已；
- 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）；
- 对缺失数据不太敏感，算法也比较简单，常用于文本分类；
- 朴素贝叶斯对结果解释容易理解；

**缺点：**

- 需要计算先验概率；
- 分类决策存在错误率；
- 对输入数据的表达形式很敏感；
- 由于**使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好**；

### sklearn中的GaussianNB实现

下面采用sklearn中的鸾尾花数据集，由于数据集都是连续属性，所以采用GaussianNB来进行实现，看下预测情况。

```python
# 导入包
from sklearn.naive_bayes import GaussianNB  # 高斯分布，假定特征服从正态分布的
from sklearn.model_selection import train_test_split # 数据集划分
from sklearn.metrics import accuracy_score

# 导入数据集
from sklearn import datasets
iris = datasets.load_iris()

# 拆分数据集,random_state:随机数种子
train_x,test_x,train_y,test_y = train_test_split(iris.data,iris.target,random_state=12) 

# 建模
gnb_clf = GaussianNB()
gnb_clf.fit(train_x,train_y)

# 对测试集进行预测
# predict()：直接给出预测的类别
# predict_proba()：输出的是每个样本属于某种类别的概率
predict_class = gnb_clf.predict(test_x)
# predict_class_proba = gnb_clf.predict_proba(test_x)
print("测试集准确率为：",accuracy_score(test_y,predict_class))
```

多项式朴素贝叶斯(MultinomialNB):

多项式朴素贝叶斯是先验为多项式分布的朴素贝叶斯。 它假设特征是由一个简单多项式分布生成的。多项分布可以描述各种类型样本出现次数的概率，因此多项式朴素贝叶斯非常适合用于描述出现次数的特征。**该模型常用于文本分类，特征表示的是次数，例如某个词语的出现次数。**

多项式分布

多项式分布来源于统计学中的多项式实验，这种实验可以解释为：实验包括n次重复试验，每次试验都有不同的可能结果。在任何给定的试验中，特定结果发生的概率是不变的。

**多项式分布公式**： 

![[公式]](https://www.zhihu.com/equation?tex=P%28X%3Dx_%7Bi%7D%7CY%3Dc%29%3D+%5Cfrac%7B%7CD_%7Bc%2Cx_i%7D%7C%2B%5Clambda%7D%7B%7CD_c%7C%2B%5Clambda+N_i%7D%5C%5C) 

其中， ![[公式]](https://www.zhihu.com/equation?tex=P%28X%3Dx_%7Bi%7D%7CY%3Dc%29) 表示c类别下第i个属性上取值为 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 的条件概率。 ![[公式]](https://www.zhihu.com/equation?tex=%7CD_%7Bc%2Cx_i%7D%7C) 是c类别下第i个属性上取值为 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 的样本数， ![[公式]](https://www.zhihu.com/equation?tex=%7CD_c%7C) 是c类的样本数。 ![[公式]](https://www.zhihu.com/equation?tex=N_i) 表示第i个属性可能的取值数。λ被称为平滑系数，令λ>0来防止训练数据中出现过的一些词汇没有出现在测试集中导致的0概率。如果λ=1，则这个平滑叫做拉普拉斯平滑，λ<1，叫做利德斯通平滑。

建一个简单多项式朴素贝叶斯（让所有的参数保持默认）的例子。

```python
 导⼊入需要的模块和库
from sklearn.naive_bayes import MultinomialNB
import numpy as np
# 建立数据集
X = np.random.randint(5, size=(6, 100))
y = np.array([1, 2, 3, 4, 5, 6])
# 建立一个多项式朴素贝叶斯分类器
mnb = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)
mnb.fit(X, y)
# 由于概率永远是在[0,1]之间，mnb给出的是对数先验概率，因此返回的永远是负值
# 类先验概率=各类的个数/类的总个数
print("类先验概率：", np.exp(mnb.class_log_prior_))

# 返回一个固定标签类别下的每个特征的对数概率log(P(Xi | y))
# print("每个特征的对数概率:",mnb.feature_log_prob_)
# '''重要属性：在fit时每个标签类别下包含的样本数。
# 当fit接口中的sample_weight被设置时，
# 该接口返回的值也会受到加权的影响'''
print("每个标签类别下包含的样本数:",mnb.class_count_)
print("预测的分类：", mnb.predict([X[2]]))  # 输出3
```

 伯努利朴素贝叶斯(BernoulliNB)

BernoulliNB就是先验为伯努利分布的朴素贝叶斯。假设特征的先验概率为二元伯努利分布，在文本分类中 ，就是一个特征有没有在一个文档中出现。

**伯努利分布公式如下**：  

![[公式]](https://www.zhihu.com/equation?tex=P%28X%3Dx_%7Bi%7D%7CY%3Dc%29%3D+P%28x_i%3D1%7CY%3Dc%29x_%7Bi%7D%2BP%28x_i%3D0%7CY%3Dc%29x_%7Bi%7D+%5C%5C) 

此时， ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bi%7D) 只能取0和1。

由于 ![[公式]](https://www.zhihu.com/equation?tex=x_i%3D0) ，所以上式可变为

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7DP%28X%3Dx_%7Bi%7D%7CY%3Dc%29%26%3DP%28x_i%3D1%7CY%3Dc%29%2A1%5C%5C%26%3D%5Cfrac%7B%7CD_%7Bc%2Cx_i%3D1%7D%7C+%2B%CE%BB%7D%7B%7CD_c%7C%2B%CE%BB%C3%972%7D%5Cend%7Baligned%7D%5C%5C) 

![[公式]](https://www.zhihu.com/equation?tex=%7CD_%7Bc%2Cx_i%3D1%7D%7C) 是c类别下第i个属性上取值为1的样本数， ![[公式]](https://www.zhihu.com/equation?tex=%7CD_c%7C) 是c类的样本数。2表示第i个属性可能的取值数，这里只有0和1两种取值，所以是2。



## **2： MaxEnt (最大熵模型)**

最大熵原理是概率模型学习的一个准则。

最大熵原理认为，在学习概率模型时，在所有可能 的概率分布中，熵最大的模型是最好的模型。

通常用约束条件来确定概率模型的集合，所以， **最大熵模型也可以表述为在满足约束条件的模型集合中选取熵最大的模型。**

定义：
$$
特征函数 f(x,y) 的经验分布\hat{p}(x,y)的期望值， E_{\hat{p}}(f)  \\
E_{\hat{p}}=\sum_{x,y}\hat{p}(x,y)f(x,y)  \\
特征函数f(x,y)关于模型P(y|x) 与经验分布 \hat{p(x)}的期望值，用E_P(f) \\
E_p(f)= \sum_{x,y}\hat{p}(x)P(y|x)f(x,y) \\
如果模型能获取训练数据中的信息，那么就可以假设这两个期望值相等
$$
![image-20210526214350713](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210526214350713.png)

引入拉格朗日乘子后得到：

![image-20210526214434845](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210526214434845.png)

![image-20210526214455511](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210526214455511.png)

![image-20210526215335456](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210526215335456.png)

![image-20210526215344269](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210526215344269.png)

![image-20210526215405844](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210526215405844.png)

## 3： SVM

最大 间隔分类器

<img src="C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210527201157083.png" alt="image-20210527201157083" style="zoom:50%;" />

超平面

支持向量  与其他元素无关

函数间隔：

![image-20210527201400046](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210527201400046.png)



**对参数 w 和 b 同时放大/缩小任意相同的倍数，**虽然不会影响分类的结果应因为 2wTx+2b=0 和 wTx+b=0 是一样的，但是 functional  margins 的值却会发生变化。 整个训练集的 functional margins 则被定义为：

![image-20210527201438894](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210527201438894.png)

几何间隔：

![image-20210527201533886](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210527201533886.png)

函数间隔和几何间隔的关系：

![image-20210527201546534](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210527201546534.png)

整个训练集上几何间隔 ：

![image-20210527201627332](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210527201627332.png)

![image-20210527201643536](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210527201643536.png)

![image-20210527201656463](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210527201656463.png)



![image-20210527201726627](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210527201726627.png)

无穷大的意识：当分类错误 ， alpha 设置成无穷大。

这个问题是：

第1步求： alpha 的最大，再求w 最小

根据对偶问题：



![image-20210527202420070](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210527202420070.png)

![image-20210527202518425](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210527202518425.png)

支持向量机，一个经久不衰的算法，高准确率，为避免过拟合提供了很好的理论保证，而且就算数据在原特征空间线性不可分，只要给个合适的核函数，它就能运行得很好。在动辄超高维的文本分类问题中特别受欢迎。可惜内存消耗大，难以解释，运行和调参也有些烦人，而随机森林却刚好避开了这些缺点，比较实用。

**优点**

- 可以解决高维问题，即大型特征空间；
- 解决小样本下机器学习问题；
- 能够处理非线性特征的相互作用；
- 无局部极小值问题；（相对于神经网络等算法）
- 无需依赖整个数据；
- 泛化能力比较强；

- 当观测样本很多时，效率并不是很高；
- 对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；
- 对于核函数的高维映射解释力不强，尤其是径向基函数；
- 常规SVM只支持二分类；
- **对缺失数据敏感**；

## 4： 决策树

![image-20210616145345354](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210616145345354.png)



决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted  tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。



每一步扩大树的规模的时候都是找当前步能使分类结果提升最明显的选择。

- 1： 特征的选择
- 2： 决策树的生成
- 3： 决策树修剪

ID3，C4.5 ，CART（Classification And Regression Tree）算法

注意： id3,c4.5使用的是多叉树， cart树使用的是二叉树

### ID3

ID3 算法中每个属性只考虑一次，所以规则的数目主要取决于属性集规模 和属性值的个数而非数据集大小

损失函数

信息增益是针对一个一个特征而言的，就是看一个特征![img](https://images2015.cnblogs.com/blog/870243/201608/870243-20160808145957340-363886996.png)，系统有它和没有它时的信息量各是多少，两者

的差值就是这个特征给系统带来的信息量，即**信息增益**。

在信息论中，期望信息越小，那么信息增益就越大，从而纯度就越高。ID3算法的核心思想就是以信息增益来度量属性的选择，选择分裂后信息增益最大的属性进行分裂。该算法采用自顶向下的贪婪搜索遍历可能的决策空间。

1.1 思想

1. 初始化特征集合和数据集合；
2. 计算数据集合信息熵和所有特征的条件熵，选择信息增益最大的特征作为当前决策节点；
3. 更新数据集合和特征集合（删除上一步使用的特征，并按照特征值来划分不同分支的数据集合）；
4. 重复 2，3 两步，若子集值包含单一特征，则为分支叶子节点。

1.2 划分标准

ID3 使用的分类标准是信息增益，它表示得知特征 A 的信息而使得样本集合不确定性减少的程度。

数据集的信息熵：

![[公式]](https://www.zhihu.com/equation?tex=H%28D%29%3D-%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Cfrac%7B%7CC_k%7C%7D%7B%7CD%7C%7Dlog_2%5Cfrac%7B%7CC_k%7C%7D%7B%7CD%7C%7D+%5C%5C) 

其中 ![[公式]](https://www.zhihu.com/equation?tex=C_k) 表示集合 D 中属于第 k 类样本的样本子集。

针对某个特征 A，对于数据集 D 的条件熵 ![[公式]](https://www.zhihu.com/equation?tex=H%28D%7CA%29) 为： 

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+H%28D%7CA%29+%26+%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7DH%28D_i%29+%5C%5C+%26+%3D-+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D%28%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Cfrac%7B%7CD_%7Bik%7D%7C%7D%7B%7CD_i%7C%7Dlog_2%5Cfrac%7B%7CD_%7Bik%7D%7C%7D%7B%7CD_i%7C%7D%29++%5C%5C+%5Cend%7Baligned%7D+%5C%5C) 

其中 ![[公式]](https://www.zhihu.com/equation?tex=D_i) 表示 D 中特征 A 取第 i 个值的样本子集， ![[公式]](https://www.zhihu.com/equation?tex=D_%7Bik%7D) 表示 ![[公式]](https://www.zhihu.com/equation?tex=D_i) 中属于第 k 类的样本子集。

信息增益 = 信息熵 - 条件熵：

![[公式]](https://www.zhihu.com/equation?tex=Gain%28D%2CA%29%3DH%28D%29-H%28D%7CA%29++%5C%5C) 

信息增益越大表示使用特征 A 来划分所获得的“纯度提升越大”

1.3 缺点

- ID3 没有剪枝策略，容易过拟合；
- 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；
- 只能用于处理离散分布的特征；
- 没有考虑缺失值。

### C4.5

C4.5 采用信息增益比来选择特征，克服了用信息增益选择属性时偏向选择取值多的属 性的不足；在树构造过程中进行剪枝；能够完成对连续属性的离散化处理；能够对不 完整数据进行处理。

**思想**

C4.5 相对于 ID3 的缺点对应有以下改进方式： 

- 引入悲观剪枝策略进行后剪枝； 
- 引入信息增益率作为划分标准； 
- 将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点； 
- 对于缺失值的处理可以分为两个子问题：
- 问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）
- 问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里） 
- 针对问题一，C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；   
- 针对问题二，C4.5 的做法是：将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中。

**划分标准**

利用信息增益率可以克服信息增益的缺点，其公式为

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+Gain_%7Bratio%7D%28D%2CA%29%26%3D%5Cfrac%7BGain%28D%2CA%29%7D%7BH_A%28D%29%7D+%5C%5C+H_A%28D%29+%26%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7Dlog_2%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D+%5Cend%7Baligned%7D+%5C%5C) 

![[公式]](https://www.zhihu.com/equation?tex=H_A%28D%29+) 称为特征 A 的固有值。

（特征的信息熵）

这里需要注意，信息增益率对可取值较少的特征有所偏好（分母越小，整体越大），因此 C4.5 并不是直接用增益率最大的特征进行划分，而是使用一个**启发式方法**：先从候选划分特征中找到信息增益高于平均值的特征，再从中选择增益率最高的。

### **剪枝策略**

为什么要剪枝：过拟合的树在泛化能力的表现非常差。

**理想的决策树有三种：**

**1.叶子节点数最少**

**2.叶子深度最小**

**3.叶子节点数最少且叶子节点深度最小。**

**预剪枝**

在节点划分前来确定是否继续增长，及早停止增长的主要方法有：

- 当树的深度达到一定的规模，则停止生长。
- 达到当前节点的样本数量小于某个阈值的时候。
- 计算每次分裂对测试集的准确性提升，当小于某个阈值，或不再提升甚至有所下降时，停止生长。
- 当信息增益，增益率和基尼指数增益小于某个阈值的时候不在生长。

优点：思想简单，算法高效，采用了贪心的思想，适合大规模问题。
缺点：提前停止生长，有可能存在欠拟合的风险。

**后剪枝**

后剪枝是先从训练集生成一颗完整的决策树，然后自底向上的对决策树进行剪枝，与预剪枝最大的不同就是：决策树是否生长完整。

1. 错误率降低剪枝（REP）
2. 悲观剪枝（PEP）
3. 代价复杂度剪枝（CCP）
4. 最小误差剪枝（MEP）
5. CVP （Critical Value Pruning）
6. OPP （Optimal Pruning）

错误率降低剪枝（REP）：

将数据分为训练集和测试集，用训练集去生成一颗完整的决策树，用测试集去剪枝。

该算法将树上的每个节点都作为剪枝的候选对象，通过如下步骤进行剪枝操作：
step1：删除以此节点为根节点的树，
step2：使其成为叶子结点，赋予该节点最常见的分类
step3：对比删除前和删除后的性能是否有所提升，如果有则进行删除，没有则保留。

优点：可以最大限度的保留树的各个节点，避免了欠拟合的风险。
缺点：相较于预剪枝的时间开销巨大。



C4.5 采用的**悲观剪枝方法**，用递归的方式从低往上针对每一个非叶子节点，评估用一个最佳叶子节点去代替这课子树是否有益。如果剪枝后与剪枝前相比其错误率是保持或者下降，则这棵子树就可以被替换掉。C4.5 通过训练数据集上的错误分类数量来估算未知样本上的错误率。

后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但同时其训练时间会大的多。

**缺点**

- 剪枝策略可以再优化；
- C4.5 用的是多叉树，用二叉树效率更高；
- C4.5 只能用于分类；
- C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；
- C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行



ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。C4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有： - 用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足； - 在树构造过程中进行剪枝； - 能处理非离散的数据； - 能处理不完整的数据。

**优点**

- 产生的分类规则易于理解，准确率较高。

**缺点**

- 在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效；
- C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行

（1）CART 既能是分类树，又能是分类树；

（2） 当 CART 是分类树时，采用 GINI 值作为节点分裂的依据；当 CART 是回归树时，采用 样本的最小方差作为节点分裂的依据。

（3）CART 是一棵二叉树。

### 基尼系数

数据集D的纯度可用基尼值来度量

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7DGini%28D%29%26%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Bp%28x_i%29%2A%281-p%28x_i%29%29%7D+%5C%5C%26%3D1-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%7Bp%28x_i%29%7D%5E2%7D%5Cend%7Baligned%7D++%5C%5C)

其中， ![[公式]](https://www.zhihu.com/equation?tex=p%28x_i%29) 是分类 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 出现的概率，n是分类的数目。Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，**Gini(D)越小，则数据集D的纯度越高**。



### 回归树：

连续特征

1. m个样本的连续特征A有m个值，从小到大排列 ![[公式]](https://www.zhihu.com/equation?tex=a_1%2Ca_2%2C...%2Ca_m) ，则CART取相邻两样本值的平均数做划分点，一共有m-1个，其中第i个划分点Ti表示为： ![[公式]](https://www.zhihu.com/equation?tex=T_i%3D%28a_i%2Ba_%7Bi%2B1%7D%29%2F2) 。
2. 分别计算以这m-1个点作为二元分类点时的基尼系数。选择基尼系数最小的点为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为 ![[公式]](https://www.zhihu.com/equation?tex=a_t) ，则小于 ![[公式]](https://www.zhihu.com/equation?tex=a_t) 的值为类别1，大于 ![[公式]](https://www.zhihu.com/equation?tex=a_t) 的值为类别2，这样就做到了连续特征的离散化。

注意的是，与ID3、C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性在后面还可以参与子节点的产生选择过程。



![image-20210616161723500](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210616161723500.png)



**总结：此方法的复杂度较高，尤其在每次寻找切分点时，需要遍历当前所有特征的所有可能取值，假如总共有F个特征，每个特征有N个取值，生成的决策树有S个内部节点，则该算法的时间复杂度为：O(F\*N\*S)。**

### 分类树：



![image-20210616163431365](C:\Users\Mrwang\AppData\Roaming\Typora\typora-user-images\image-20210616163431365.png)

**决策树自身的优点**

- 决策树易于理解和解释，可以可视化分析，容易提取出规则；
- 可以同时处理标称型和数值型数据；
- 比较适合处理有缺失属性的样本；
- 能够处理不相关的特征；
- 测试数据集时，运行速度比较快；
- 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。

**缺点**

- 容易发生过拟合（随机森林可以很大程度上减少过拟合）；
- 容易忽略数据集中属性的相互关联；
- 对于那些各类别样本数量不一致的数据，在决策树中，进行属性划分时，不同的判定准则会带来不同的属性选择倾向；信息增益准则对可取数目较多的属性有所偏好（典型代表ID3算法），而增益率准则（CART）则对可取数目较少的属性有所偏好，但CART进行属性划分时候不再简单地直接利用增益率尽心划分，而是采用一种启发式规则）（只要是使用了信息增益，都有这个缺点，如RF）。
- ID3算法计算信息增益时结果偏向数值比较多的特征。

**改进措施**

- 对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。
- 使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题；

## 5：EM算法

最大期望算法（Expectation Maximization Algorithm，又译期望最大化算法），是一种迭代算 法，用于含有隐变量（latent variable）的概率参数模型的最大似然估计或极大后验概率估计。 EM 经常用在机器学习和计算机视觉的数据聚类（Data Clustering）领域。

EM 算法经过两个步骤交替进行计算：

1. 第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；
2. 第二步是最大化（M），最大化在 E 步上求得的最大似然值来计算参数的值。

M 步上找到的参数估计值被用于下一个 E 步计算中，这个过程不断交替进行。

EM 的算法流程如下：

1.初始化分布参数

 2.重复直到收敛： 

E 步骤：估计未知参数的期望值，给出当前的参数估计。 

M 步骤：重新估计分布参数，以使得数据的似然性最大，给出未知变量的期望估计。

 将 EM 算法要优化的目标看作是以 Q, θ 为参数的函数，

那么 E-step 就是保持 θ 不变， 优化 Q； 

M-step 就是保持 Q 不变，优化 θ

Y 是观测遍历，Z 是隐变量

```
作者：Scofield
链接：https://www.zhihu.com/question/35866596/answer/236886066
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

一、监督：{

1.1 分类算法(线性和非线性)：{

    感知机

    KNN

    概率{
        朴素贝叶斯（NB）
        Logistic Regression（LR）
        最大熵MEM（与LR同属于对数线性分类模型）
    }

    支持向量机(SVM)

    决策树(ID3、CART、C4.5)

    assembly learning{
        Boosting{
            Gradient Boosting{
                GBDT
                xgboost（传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）；xgboost是Gradient Boosting的一种高效系统实现，并不是一种单一算法。）
            }
            AdaBoost
        }   
        Bagging{
            随机森林
        }
        Stacking
    }

    ……
}

1.2 概率图模型：{
    HMM
    MEMM（最大熵马尔科夫）
    CRF
    ……
}


1.3 回归预测：{
    线性回归
    树回归
    Ridge岭回归
    Lasso回归
    ……
}

……  
}


二、非监督：{
2.1 聚类：{
    1. 基础聚类
        K—mean
        二分k-mean
        K中值聚类
        GMM聚类
    2. 层次聚类
    3. 密度聚类
    4. 谱聚类()
}

2.2 主题模型:{
    pLSA
    LDA隐含狄利克雷分析
}

2.3 关联分析：{
    Apriori算法
    FP-growth算法
}

2.4 降维：{
    PCA算法
    SVD算法
    LDA线性判别分析
    LLE局部线性嵌入
}

2.5 异常检测：
……
}

三、半监督学习

四、迁移学习
```

## 6： 逻辑回归：

**优点：**

- 实现简单，广泛的应用于工业问题上；
- 分类时计算量非常小，速度很快，存储资源低；
- 便利的观测样本概率分数；
- 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题；
- 计算代价不高，易于理解和实现；

**缺点**：

- 当特征空间很大时，逻辑回归的性能不是很好；
- 容易**欠拟合**，一般准确度不太高
- 不能很好地处理大量多类特征或变量；
- 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须**线性可分**；
- 对于非线性特征，需要进行转换

**logistic回归应用领域：**

- 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。
- Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。
- 信用评估
- 测量市场营销的成功度
- 预测某个产品的收益
- 特定的某天是否会发生地震

## 7： **线性回归**

线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用**梯度下降法**对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： ![\hat{w} = (X^TX)^{-1}X^Ty](https://www.zhihu.com/equation?tex=%5Chat%7Bw%7D+%3D+%28X%5ETX%29%5E%7B-1%7DX%5ETy)

而在LWLR（局部加权线性回归）中，参数的计算表达式为: ![\hat{w} = (X^TWX)^{-1}X^TWy](https://www.zhihu.com/equation?tex=%5Chat%7Bw%7D+%3D+%28X%5ETWX%29%5E%7B-1%7DX%5ETWy)

由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。

**优点**： 实现简单，计算简单；

**缺点**： 不能拟合非线性数据.

## 8： 最近邻算法——KNN

多数投票 

非参数的，不具有显示的学习过程

3个基本要素：

- k值得选择
- 距离的度量
- 分类的决策

```
KNN即最近邻算法，其主要过程为：
1. 计算已知类别数据集中的点与当前点之间的距离（常见的距离度量有欧式距离，马氏距离等）；
2. 对上面所有的距离值进行排序(升序)；
3. 选前k个最小距离的样本；
4. 根据这k个样本的标签进行投票，得到最后的分类类别；
```

**KNN算法的优点**

- 理论成熟，思想简单，既可以用来做分类也可以用来做回归；
- 可用于非线性分类；
- 训练时间复杂度为O(n)；
- 对数据没有假设，准确度高，对outlier不敏感；
- KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练；
- KNN理论简单，容易实现；

**缺点**

- 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差；
- 需要大量内存；（全部训练集）
- 对于样本容量大的数据集计算量比较大（体现在距离计算上）；
- 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多；
- KNN每一次分类都会重新进行一次全局运算；
- k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择；

**KNN算法应用领域**

文本分类、模式识别、聚类分析，多分类领域

## 9-k-Means

是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k< n。 算法的核心就是要优化失真函数J,使其收敛到局部最小值但不是全局最小值。

关于K-Means聚类的文章，参见[机器学习算法-K-means聚类](https://link.zhihu.com/?target=http%3A//www.csuldw.com/2015/06/03/2015-06-03-ml-algorithm-K-means/)。关于K-Means的推导，里面可是有大学问的，蕴含着强大的EM思想。

**优点**

- 算法简单，容易实现 ；
- 算法速度很快；
- 对处理大数据集，该算法是相对可伸缩的和高效率的，因为它的复杂度大约是O(nkt)，其中n是所有对象的数目，k是簇的数目,t是迭代的次数。通常k<<n。这个算法**通常局部收敛**。
- 算法尝试找出使平方误差函数值最小的k个划分。当簇是密集的、球状或团状的，且簇与簇之间区别明显时，聚类效果较好。

**缺点**

- 对数据类型要求较高，适合数值型数据；
- 可能收敛到局部最小值，在大规模数据上收敛较慢
- 分组的数目k是一个输入参数，不合适的k可能返回较差的结果。
- 对初值的簇心值敏感，对于不同的初始值，可能会导致不同的聚类结果；
- 不适合于发现非凸面形状的簇，或者大小差别很大的簇。
- 对于”噪声”和孤立点数据敏感，少量的该类数据能够对平均值产生极大影响。



## 10: Adaboost:

Adaboost是一种加和模型，每个模型都是基于上一次模型的错误率来建立的，过分关注分错的样本，而对正确分类的样本减少关注度，逐次迭代之后，可以得到一个相对较好的模型。该算法是一种典型的boosting算法，其加和理论的优势可以使用Hoeffding不等式得以解释。有兴趣的同学可以阅读下自己之前写的这篇文章[AdaBoost算法详述](https://zhuanlan.zhihu.com/p/42915999).下面总结下它的优缺点。

- Adaboost是一种有很高精度的分类器。
- 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。
- 当使用简单分类器时，计算出的结果是可以理解的，并且弱分类器的构造极其简单。
- 简单，不用做特征筛选。
- 不易发生overfitting。

## 面试题目整理：

### 1： 模型过拟合原因：

在训练模型的时候，我们不知道训练集和测试集上的数据分布，在训练集上，样本往往存在噪音，如果我们在训练集上追求一个完美复杂的模型，参数非常多，会使得模型把训练集里面的误差都当做真实的数据分布，从而得到错误的数据分布估计，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）

- 增加数据、添加噪声
- early stooping
- 数据均衡（过采样、将采样）
- **正则化（L1，L2）**
  1.解空间形状：加入正则化项即为约束条件：形成不同形状的约束解空间。
  2 导数：L2的导数为2X，平滑。L1导数为X，-X，存在突变的极值点
  3.先验：加入正则化项相当于引入参数的先验知识：L1引入拉普拉斯，L2引入高斯分布
  L1可以做到特征筛选和得到稀疏解。L2加速训练
- **Dropout**
  减小参数规模
  随机丢弃产生不同网络，形成集成，解决过拟合，加速训练
- **Batch normolization**
  加快训练、消除梯度消失（爆炸）、防止过拟合 不适用太小batch、CNN



### 2： 欠拟合

但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。

### 3： 刻画模型复杂度（偏差和方差）

认为Error = Bias + Variance。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）

朴素贝叶斯：

它简单的假设了各个数据之间是无关的，是一个被**严重简化了的模型**。所以，对于这样一个简单模型，大部分场合都会Bias部分大于Variance部分，也就是说高偏差而低方差。

在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。

当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。

### 4：正则项

L1和L2是正则化项，又叫做罚项，是为了限制模型的参数，防止模型过拟合而加在损失函数后面的一项

**1.L1-norm（L1范数）**    **2.L2-norm（L2范数）**

L1-norm(范数)，也叫作最小绝对偏差（least absolute deviations, LAD），最小绝对误差（least absolute errors，LAE）.它是最小化目标值yi和估计值f(xi)绝对差值的求和

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190528220933175.png)

L2-norm(范数)也称为最小均方(least squares)，它是最小化目标值 yi 和估计值 f(xi) 差值的平方和：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190528220953932.png)

### 5：**L1和L2的比较**

**（1）鲁棒性（Robustness）：**

括起来就是L1对异常点不太敏感，而L2则会对异常点存在放大效果。

最小绝对值偏差的方法应用领域很广（L1-norm），相比最小均方的方法，它的鲁棒性更好，LAD能对数据中的异常点有很好的抗干扰能力，异常点可以安全的和高效的忽略，这对研究帮助很大。如果异常值对研究很重要，最小均方误差则是更好的选择。
对于L2-norm，由于是均方误差，如果误差>1的话，那么平方后，相比L-norm而言，误差就会被放大很多。因此模型会对样例更敏感。如果样例是一个异常值，模型会调整最小化异常值的情况，以牺牲其它更一般样例为代价，因为相比单个异常样例，那些一般的样例会得到更小的损失误差。

**（2）稳定性：**
概括起来就是对于新数据的调整，L1的变动很大，而L2的则整体变动不大。

https://blog.csdn.net/fantacy10000/article/details/90647686

### 6： 解决数据类别不平衡的方法：

- 把数据进行采用的过程中通过相似性同时生成并插样“少数类别数据”，叫做SMOTE算法

- **对数据先进行聚类**，再将大的簇进行随机欠采样或者小的簇进行数据生成
- 把监督学习变成无监督学习，舍弃掉标签把问题转化为一个无监督问题，如异常检测
- 先对多数类别进行随机的欠采样，并结合boosting算法进行集成学习

- 欠采样：随机欠采样、easysampling、KNN
- 过采样：随机过采样、SMOTE（人工合成）
- 数据增强
- 代价敏感学习：误分类代价不同
- 适合的评价指标：准确率、F值、AUC、G-Mean

### **7：交叉熵函数系列问题？与最大似然函数的关系和区别？**

交叉熵用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小。

交叉熵的公式为：![\sum_{k=1}^N p_k \log_2 \frac{1}{q_k}](http://www.zhihu.com/equation?tex=%5Csum_%7Bk%3D1%7D%5EN+p_k+%5Clog_2+%5Cfrac%7B1%7D%7Bq_k%7D)，其中![p_k](http://www.zhihu.com/equation?tex=p_k) 表示真实分布，![q_k](http://www.zhihu.com/equation?tex=q_k) 表示非真实分布。

交叉熵越低，表示非真实分布越接近真实分布，因此，交叉熵越低越好。

如何去衡量不同策略之间的差异呢？这就需要用到相对熵（KL散度，物理意义就是用来度量两个函数的相似度或者相近程度），用来衡量两个取值为正的函数或概率分布之间的差异，即：

KL(f(x) || g(x)) = ![\sum_{ x \in X} f(x) * \log_2 \frac{f(x)}{g(x)}](http://www.zhihu.com/equation?tex=%5Csum_%7B+x+%5Cin+X%7D+f%28x%29+%2A+%5Clog_2+%5Cfrac%7Bf%28x%29%7D%7Bg%28x%29%7D)

现在假设我们想知道某个策略和最优策略之间的差异，我们就可以用相对熵来衡量这两者之间的差异。即，相对熵 = 某个策略的交叉熵 - 信息熵（根据系统真实分布计算而得的信息熵，为最优策略），公式如下：

KL（p || q） = H（p，q） - H（p） = 

![ \sum_{k=1}^N p_k \log_2 \frac{1}{q_k} - \sum_{k=1}^N p_k \log_2 \frac{1}{p_k} = \sum_{k=1}^N p_k \log_2 \frac{p_k}{q_k}](http://www.zhihu.com/equation?tex=+%5Csum_%7Bk%3D1%7D%5EN+p_k+%5Clog_2+%5Cfrac%7B1%7D%7Bq_k%7D+-+%5Csum_%7Bk%3D1%7D%5EN+p_k+%5Clog_2+%5Cfrac%7B1%7D%7Bp_k%7D+%3D+%5Csum_%7Bk%3D1%7D%5EN+p_k+%5Clog_2+%5Cfrac%7Bp_k%7D%7Bq_k%7D)

 

1）交叉熵损失函数的物理意义：用于描述模型预测值与真实值的差距大小

2**）最小化交叉熵的本质就是对数似然函数的最大化**

3）对数似然函数的本质就是衡量在某个参数下，整体的估计和真实情况一样的概率，越大代表越相近；而损失函数的本质就是衡量预测值和真实值之间的差距，越大代表越不相近。

### **8： HMM vs. MEMM vs. CRF** 

首先，CRF，HMM，MEMM都常用来做序列标注的建模，像分词、词性标注、以及命名实体标注

隐马模型一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择，最大熵隐马模型则解决了隐马的问题，可以任意选择特征，但由于其在每一节都要进行归一化，所以只能找到局部最优值，同时也带来了标记偏置问题，即凡是训练语料中未出现的情况全都忽略掉。条件随机场很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行归一化，因此可以求得全局的最优值。

\1. HMM -> MEMM: HMM模型中存在两个假设：一是输出观察值之间严格独立，二是状态的转移过程中当前状态只与前一状态有关。但实际上序列标注问题不仅和单词相关，而且和观察序列的长度，单词的上下文，等等相关。MEMM解决了HMM输出独立性假设的问题。因为HMM只限定了在观测与状态之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，而MEMM引入自定义特征函数，不仅可以表达观测之间的依赖，还可以表示当前观测与前后多个状态之间的复杂依赖。

\2. MEMM->CRF:

- CRF不仅解决了HMM输出独立性假设的问题，还解决了MEMM的标注偏置问题，MEMM容易陷入局部最优是因为只在局部做归一化，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而CRF统计了全局概率，在做归一化时考虑了数据在全局的分布，而不是仅仅在局部归一化，这样就解决了MEMM中的标记偏置问题。使得序列标注的解码变得最优解。
- HMM、MEMM属于有向图，所以考虑了x与y的影响，但没将x当做整体考虑进去（这点问题应该只有HMM）。CRF属于无向图，没有这种依赖性，克服此问题。

### 9： 常见激活函数

- sigmoid和softmax
  sigmoid只做值非线性变化映射到(0,1)，用于二分类。
  softMax变化过程计算所有结果的权重，使得多值输出的概率和为1。用于多分类。 指数运算速度慢。梯度饱和消失。
- tanh函数
  双曲正切函数。以0为中心，有归一化的作用。
- ReLu和Leaky ReLu
  大于0为1，小于0为0，计算速度快。
  leaky输入为负时，梯度仍有值，避免死掉。

### 10： 处理海量数据方法

- Hash法：hash映射，hash统计+堆/归并/快速排序
- 双层桶法：重找中位数（划分数据、统计个数）
- Bit-map：为每个数分配bit,遍历改变状态
- Trie树、数据库
- 外排序
- map reduce
