# 个人笔记_技术栈

## 1:python

python 解释器执行py 文件的原理

- 第一阶段：Python解释器启动，此时就相当于启动了一个文本编辑器
- 第二阶段：Python解释器相当于文本编辑器，去打开test.py文件，从硬盘上将test.py的文件内容读入到内存中
- 第三阶段：Python解释器解释执行刚刚加载到内存中test.py的代码

### 字符编码

最早的字符编码为ASCII

<img src="pic/image-20220502174238159.png" alt="image-20220502174238159" style="zoom:50%;" />



ascii用1个字节（8位二进制）代表一个字符；

Unicode常用2个字节（16位二进制）代表一个字符，

生僻字需要用4个字节

内存中还使用Unicode编码，是因为历史遗留问题造成的,所以未来内存中使用的编码也将使用UTF-8编码替代Unicode编码。

~~~python
Python解释器会读取test.py的第一行内容，
#coding:utf-8或
＃-\\\*-coding:utf-8-\\\*-，以此决定以什么编码格式将代码读入内存
    
sys.getdefaultencoding()    
~~~





### **变量,  常量** 

- 驼峰体:`NameOfNick`
- 下划线:`name_of_nick`(推荐)

定义常量，变量名必须全大写,python中没有使用语法强制定义常量

常量池： 

python 实现int 的时候有个小整数池

变量存放到内存中， 垃圾回收机制

del 触发python 的垃圾回收机制

id() :  可以获取某一个变量所在的内存地址

垃圾回收机制（如果内存中某个值，没有变量名引用就是垃圾）

~~~python
year = 2021
print(id(year)) # 输出4499932720
print(year) # 输出2021

year = 2022
print(id(year)) # 输出4499932560
print(year) # 输出2022
~~~

引用计数 ：相同的变量值被赋予不同的变量名时

1. 变量被创建，变量值引用计数加1
2. 变量被引用，变量值引用计数加1
3. 变量作为参数传入到一个函数，变量值引用计数加2

减少引用计数的方法，共有以下4种：

1. 变量值对应的变量名被销毁
2. 变量值对应的变量名被赋予新的值
3. 变量值对应的变量名离开它的作用域
4. 变量值对应的变量名的容器被销毁

~~~python
import sys
# 引用计数初始值为3
print(sys.getrefcount(2021)) # 输出为3
year = 2021
print(sys.getrefcount(2021)) # 输出为4
height = 2021
print(sys.getrefcount(2021)) # 输出为5
del year
print(sys.getrefcount(2021)) # 输出为4


x = 'aaaa'  # 引用计数初始值为3
print(sys.getrefcount('aaaa'))# 输出为3
~~~

#### 变量的3个特征

print(x)  获取变量的变量值

id(x)   获取变量的id，可以理解成变量在内存中的地址

type(x) 获取变量的数据类型，下章会详细介绍数据类型

~~~python
x =11
y =11
print(x==y)  #ture
print(x is y)# 整数池的原因 ture
~~~

#### 赋值

~~~python
a = b = c = d = 10  #  链式赋值
x, y = y, x # 交叉赋值
~~~

#### 数据类型

数字类型 ：

 		int float   加减乘除、逻辑判断（大于、小于）

##### 字符串

字符串类型:

​		在单引号、双引号或三引号内包裹的一串字符。

​		需要注意的是：三引号内的字符可以换行，而单双引号内的字符不行列表类型

​		字符串只能+、*和逻辑比较

~~~python
u'unicode': unicode编码的字符串
b'101': 二进制编码的字符串
r'\n': 原生字符串，也就是说'\n'这是普通的两个字符，并没有换行的意思
   
移除空白strip()
切分split()
lstrip  # 截断
rstrip  # 截断
lower&upper
startswith&endswith
rsplit  # 从右开始切割
join()
replace()
isdigit()

find|rfind|index|rindex|count
center|ljust|rjust|zfill
expandtabs()
captalize()、swapcase()、title()

isdecimal(): 检查字符串是否值包含十进制字符，如果是返回True，否则返回False。
isdigit(): 如果字符串只包含数字则返回True，否则返回False。
isnumeric(): 如果字符串中只包含数字字符，则返回True，否则返回False。
    
isspace
~~~

##### 列表：

~~~python
# list之删除
name_list = ['nick', 'jason', 'tank', 'sean']
del name_list[2]
# insert
name_list.insert(1, 'handsome') 
# pop()
name_list.pop(1)
#remove()
name_list.remove('nick')
# count
name_list.count('nick')
# index()
name_list.index('nick')
# clear()
name_list.clear()
# copy()
name_list.copy()

#extend()
name_list2 = ['nick handsome']
name_list.extend(name_list2)
# reverse()
name_list.reverse()
# sort()
name_list.sort()
~~~

##### 元组 tuple

~~~python
索引取值
切片（顾头不顾尾，步长）
长度len
成员运算in和not in
循环
count
index
~~~

##### 集合 set

去重，无序

~~~python
s = {1, 2, 1, 'a'}  # s = set({1,2,'a'})


pythoners = {'jason', 'nick', 'tank', 'sean'}
linuxers = {'nick', 'egon', 'kevin'}
# |并集
pythoners|linuxers  # 两种方式
pythoners.union(linuxers)
#交集 &
pythoners&linuxers
pythoners.intersection(linuxers)
#差集 - 
pythoners-linuxers
pythoners.difference(linuxers)
#^	对称差集
pythoners^linuxers
pythoners.symmetric_difference(linuxers)
# >、>=  <、<=

# set之add()
s = {1, 2, 'a'}
s.add(3)
#remove()
difference_update()
# set之discard()
s.discard(3)
# isdisjoint()
pythoners.isdisjoint(linuxers)


~~~





##### 字典dict：

~~~python
dic = {'a': 1, 'b': 2}
# dic之成员运算in和not in
'a' in dic
del dic['a']
dic.pop('a')  # 指定元素删除
dic.popitem() # popitem() 方法随机返回并删除字典中的一对键和值(一般删除末尾对)。
dic.keys()
dic.values()
dic.items()
for k, v in dic.items():  # items可以换成keys()、values()
    print(k, v)
    
dic.get('a')  
# update
dic1 = {'a': 1, 'b': 2}
dic2 = {'c': 3}
dic1.update(dic2)
print(f"dic1: {dic1}")

#fromkeys()
dic = dict.fromkeys(['name', 'age', 'sex'], None)
#setdefault()

~~~

##### 布尔类型：

True、False通常情况不会直接引用，需要使用逻辑运算得到结果。

**0、None、空、False的布尔值为Fals**

##### 解压缩：

解压缩可以这样理解：超市打包是把多个商品放在一起，解压缩其实就是解包把多个商品一次性拿出来。

~~~python
name_list = ['nick', 'egon', 'jason', 'tank']
x, y, z, a = name_list
x, _, z, _ = name_list  # _相当于告诉计算机不要了，不能以_开头和结尾
# 因此字典也是可以的，但是字典解压缩的是key。
info = {'name': 'nick', 'age': 18}
x, y = info 
print(x, y) #name age
~~~

#### 迭代器

如何判断一个对象是可迭代对象呢？

可以使用isinstance()判断一个对象是否是Iterable对象：

for循环的对象统称为可迭代对象：Iterable。

1. 凡是可作用于for循环的对象都是Iterable类型
   凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列
2. 集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象

方法是通过collections.abc模块的Iterable类型判断：

~~~python
from collections.abc import Iterable
for i, value in enumerate(['A', 'B', 'C'])
	pass
#for循环里，同时引用了两个变量，在Python里是很常见的，比如下面的代码
for x, y in [(1, 1), (2, 4), (3, 9)]:
     print(x, y)

import itertools #无限”迭代器
natuals = itertools.count(1)
count()会创建一个无限的迭代器
cycle()会把传入的一个序列无限重复下
cs = itertools.cycle('ABC') # 注意字符串也是序列的一种
repeat()负责把一个元素无限重复下去，不过如果提供第二个参数就可以限定重复次数：
无限序列虽然可以无限迭代下去，但是通常我们会通过takewhile()等函数根据条件判断来截取出一个有限的序列：
ns = itertools.takewhile(lambda x: x <= 10, natuals)
chain()可以把一组迭代对象串联起来，形成一个更大的迭代器：
groupby()把迭代器中相邻的重复元素挑出来放在一起：
~~~

集合数据类型，如list、tuple、dict、set、str等

一类是generator，包括生成器和带yield的generator function。



#### 生成器

generator 生成器都是Iterator对象

Iterator对象表示的是一个数据流，数据流看做是一个有序序列，但我们却不能提前知道序列的长度

~~~python
#yield关键字
def fib(max):
    n, a, b = 0, 0, 1
    while n < max:
        yield b
        a, b = b, a + b
        n = n + 1
    return 'done'
1: 用for 循环不会保错
2： next 最后会报错
while True:
     try:
        x = next(g)
        print('g:', x)
    except StopIteration as e:
         print('Generator return value:', e.value)
        break
~~~

#### 装饰器

由于函数也是一个对象，而且函数对象可以被赋值给变量，所以，通过变量也能调用该函数。

在代码运行期间动态增加功能的方式，称之为“装饰器” Decorator

~~~python
f = sum() # 函数对象 f 
f(1,2,3)  # 函数调用
#函数对象有一个__name__属性，可以拿到函数的名字
#decorator就是一个返回函数的高阶函数。所以，我们要定义一个能打印日志的decorator
def log(func):
    def wrapper(*args, **kw):
        print('call %s():' % func.__name__)  # 日志功能
        return func(*args, **kw)
    return wrapper

Python的@语法，把decorator置于函数的定义处：
~~~

#### 枚举

~~~python
from enum import Enum
Month = Enum('Month', ('Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'))

from enum import Enum, unique #  @unique装饰器可以帮助我们检查保证没有重复值。
@unique
class Weekday(Enum):
    Sun = 0 # Sun的value被设定为0
    Mon = 1
    Tue = 2
    Wed = 3
    Thu = 4
    Fri = 5
    Sat = 6
~~~



#### 格式化输出

占位符

~~~python
age = 19
print('My name is xxx，my age is '+str(age)) 
print('My name is xxx，my age is', age)
print('My name is '+name+' my age is '+str(age))
#%s（针对所有数据类型）、%d（仅仅针对数字类型）  占位符
print('my name is %s my age is %s' % (name, age))
~~~

format格式化

~~~python
name = 'nick'
age = 19
print("Hello, {}. You are {}.".format(name, age))
print("Hello, {1}. You are {0}-{0}.".format(age, name))
print("Hello, {name}. You are {age}-{age}.".format(age=age, name=name))
~~~

f-String格式化的方式

~~~python
name = "nick"
age = 19
print(f"Hello, {name}. You are {age}.")
salary = 6.6666
print(f'{salary:.2f}')
~~~

#### 运算

~~~python
# /有零有整除，得到一个浮点型
print(10 / 3)  # 3.3333333333333335
# 地板除，只取整数部分
print(10 // 3) # 3
print(10 // 4) # 2
# %：取余
print(10 % 3) # 1 
# **，幂
print(10**3)
~~~

#### 运算符号

~~~python
==
!=
<>
> < >= <=

#赋值运算符号
+=  -=  *=  /= %= **= //=

# 逻辑运算符
and 
or 
not 
# 身份运算符
is 判断两个标识符， 是不是引用同一个对象， 类型 id(x)==id(y)
is not
is和==的区别：
is用于判断两个变量引用对象是否为同一个(是否在同一块内存空间中)， 
==用于判断引用变量的值是否相等
x = 257
y = x
z = 257
print(f'x is y:{x is y}') #	x is y:True
print(f'x == y:{x == y}') #  x == y:True
print(f'x is z:{x is z}') #  x is z:False
print(f'x == z:{x == z}')  # x == z:True
# 位运算符
&
|
^
~
<<
>>
# 成员运算符号
in 
not in 
~~~

### 异常处理

为了保证程序的健壮性与容错性，即在遇到错误时程序不会崩溃，我们需要对异常进行处理

错位类型

- 语法错误
- 运行时错误

~~~python
#基本语法为
try:
    被检测的代码块
except 异常类型：
    try中一旦检测到异常，就执行这个位置的逻辑
    
# 万能异常Exception    
try:
    int(s1)
except Exception as e:
    print(e)
# 多分支后来一个Exception
s1 = 'hello'
try:
    int(s1)
except IndexError as e:
    print(e)
except KeyError as e:
    print(e)
except ValueError as e:
    print(e)
except Exception as e:
    print(e)
else :
    print('try内代码块没有异常则执行我')

# 异常的最终执行
s1 = 'hello'
try:
    int(s1)
except IndexError as e:
    print(e)
except KeyError as e:
    print(e)
except ValueError as e:
    print(e)
#except Exception as e:
#    print(e)
else:
    print('try内代码块没有异常则执行我')
finally:
    print('无论异常与否，都会执行该模块，通常是进行清理工作')
    
    
# 自定义异常    
class EgonException(BaseException):
    def __init__(self, msg):
        self.msg = msg

    def __str__(self):
        return self.msg


try:
    raise EgonException('抛出异常，类型错误')
except EgonException as e:
    print(e)    
    
~~~

~~~python
try:
    pass
except  Exception as e:
    pass 
finally:
    pass 
用try来运行这段代码，如果执行出错，则后续代码不会继续执行，而是直接跳转至错误处理代码，即except语句块，执行完except后，如果有finally语句块，则执行finally语句块，至此，执行完毕


Python的错误其实也是class，所有的错误类型都继承自BaseException，所以在使用except时需要注意的是，它不但捕获该类型的错误，还把其子类也“一网打尽”。比如：

try:
    foo()
except ValueError as e:
    print('ValueError')
except UnicodeError as e:
    print('UnicodeError')
第二个except永远也捕获不到UnicodeError，因为UnicodeError是ValueError的子类，如果有，也被第一个except给捕获了。  

凡是用print()来辅助查看的地方，都可以用断言（assert）来替代：
logging.basicConfig(level=logging.INFO)


~~~

raise: 手动引发异常 

raise Exception(‘desrciption error’)

raise nameError(‘description error’)





### 断言 assert

```python
assert 1 == 1
```

### 拷贝

拷贝（赋值）、浅拷贝、深拷贝

**都是针对可变类型数据而言的**（可变类型值变id不变）

~~~python
l1 = ['a', 'b', 'c', ['d', 'e', 'f']]
l2 = l1
l1.append('g')

#浅拷贝

#l2是l1的浅拷贝对象，
#  则l1内的不可变元素发生了改变，l2不变；
#  如果l1内的可变元素发生了改变，则l2会跟着改变。
l2 = copy.copy(l1) # l2 不变
l1.append('g')     
# l2 ['a', 'b', 'c', ['d', 'e', 'f']]
#
l1[3].append('g' ) 
#l2 会发生改变  ['a', 'b', 'c', ['d', 'e', 'f', 'g']]


#深拷贝

l1 = ['a', 'b', 'c', ['d', 'e', 'f']]
l2 = copy.deepcopy(l1)

~~~

### 文件

文件操作的基础模式有三种（默认的操作模式为r模式）：

- r模式为read

- w模式为write

- a模式为append

  文件读写内容的格式有两种（默认的读写内容的模式为b模式）：

  - t模式为text
  - b模式为bytes
  - b模式读写文件，一定不能加上encoding参数，因为二进制无法再编码

  可读、可写

  - r+t: 可读、可写
  - w+t: 可写、可读
  - a+t: 可追加、可读

~~~python
f = open('32.txt', mode='rt', encoding='utf8')
#f.read()读取文件指针会跑到文件的末端，如果再一次读取，读取的将是空格。
#f.read()一次性读取文件的所有内容，如果文件非常大的话，可能会造成内存爆掉
#f.readline()/f.readlines()读取文件内容。
f.readable()# 判断文件是否可读


f = open('34w.txt', mode='wt', encoding='utf8')
f = open('34a.txt', mode='at', encoding='utf8') # 则在文件的末端写入内容
f.write('nick 帅的我五体投地')
f.flush()  # 立刻将文件内容从内存刷到硬盘
f.close()


~~~

文件的指针移动

~~~python
f.seek(0,2)  # 切换到文件末尾
f.tell()  #  每次统计都是从文件开头到当前指针所在位置
f.read(n) #n代表的是字符个数，
f.truncate(n) #截断文件，所以文件的打开方式必须可 f
~~~

##### 文件修改

文件的数据是存放于硬盘上的，因而只存在覆盖、不存在修改这么一说，我们平时看到的修改文件，都是模拟出来的效果，具体的说有两种实现方式。

**方式1：**

将硬盘存放的该文件的内容全部加载到内存，在内存中是可以修改的，修改完毕后，再由内存覆盖到硬盘（word，vim，nodpad++等编辑器）。

**方式2：**

将硬盘存放的该文件的内容一行一行地读入内存，修改完毕就写入新文件，最后用新文件覆盖源文件。

##### with 管理文件操作上下文

open打开文件后我们还需要手动释放文件对操作系统的占用

Python提供的上下文管理工具——with open()

~~~python
with open()方法不仅提供自动释放操作系统占用的方法，并且with open可以使用逗号分隔，一次性打开多个文件，实现文件的快速拷贝。

with open('32.txt', 'rb') as fr, \
        open('35r.txt', 'wb') as fw:
    f.write(f.read())
~~~

并不是只有open()函数返回的fp对象才能使用with语句。实际上，任何对象，只要正确实现了上下文管理，就可以用于with语句。

实现上下文管理是通过__enter__和__exit__这两个方法实现的。例如，下面的class实现了这两个方法：

~~~python
class Query(object):

    def __init__(self, name):
        self.name = name

    def __enter__(self):
        print('Begin')
        return self
    
    def __exit__(self, exc_type, exc_value, traceback):
        if exc_type:
            print('Error')
        else:
            print('End')
    
    def query(self):
        print('Query info about %s...' % self.name)
~~~

编写__enter__和__exit__仍然很繁琐，因此Python的标准库contextlib提供了更简单的写法，上面的代码可以改写如下：
from contextlib import contextmanager
@contextmanager

### 序列化

把变量从内存中变成可存储或传输的过程称之为序列化，在Python中叫pickling，
把变量内容从序列化的对象重新读到内存里称之为反序列化，即unpickling。

~~~python
import pickle
1:pickle.dumps()方法把任意对象序列化成一个bytes，然后，就可以把这个bytes写入文件。
或者用另一个方法pickle.dump()直接把对象序列化后写入一个file-like Object：

f = open('dump.txt', 'wb')
pickle.dump(d, f)
f.close()
f = open('dump.txt', 'rb')
d = pickle.load(f)

2: json

JSON类型	Python类型
{}	dict
[]	list
"string"	str
1234.56	int或float
true/false	True/False
null	None
import json
son.dumps(d)
 json.loads(json_str)
~~~



### 函数 function



无参函数

有参函数

空函数  pass

函数返回值

- return是一个函数结束的标志，函数内可以有多个return，只要执行到return，函数就会执行。
- return的返回值可以返回任意数据类型
- return的返回值无个数限制，即可以使用逗号隔开返回多个值

#### 函数的参数

1： 函数名其实就是指向一个函数对象的引用，完全可以把函数名赋给一个变量，相当于给这个函数起了一个“别名”：

2： 如果没有`return`语句，函数执行完毕后也会返回结果，只是结果为`None`

3：函数可以同时返回多个值，但其实就是一个tuple:

4：定义默认参数要牢记一点：默认参数必须指向不变对象！

在函数定义阶段括号内定义的参数，称之为形式参数，简称形参，本质就是变量名

在函数调用阶段括号内传入的参数，称之为实际参数，简称实参，本质就是变量的值

默认形参 在定义阶段，就已经被赋值

特点：在定义阶段就已经被赋值，意味着在调用时可以不用为其赋值。

1. 位置形参必须放在默认形参的左边。

2. 默认形参的值只在定义阶段赋值一次，也就是说默认参数的值在函数定义阶段就已经固定了。

   ##### 可变长形参之 	*

指的是在调用函数时，传入的参数个数可以不固定

允许在list或tuple前面加一个`*`号，把list或tuple的元素变成可变参数传进去

##### 可变长形参之	**

关键字参数允许你传入0个或任意个含参数名的参数，这些关键字参数在函数内部自动组装为一个dict

##### 命名关键字参数

参数定义的顺序必须是：必选参数、默认参数、可变参数、命名关键字参数和关键字参数

~~~python
def sum_self(*args):
   pass
def func(**kwargw):  #会将溢出的关键字实参全部接收，然后存储字典的形式，然后把字典赋值给后的参数
    print(kwargw)
    
~~~

#### 函数对象的四大功能

~~~python
#1.引用
f = func
#2.当作参数传给一个函数
foo(func)
#3.可以当作函数的返回值
def foo(x):
    return x
res = foo(func)
# 4.可以当作容器类型的元素
function_list = [func]

~~~

##### 高阶函数  闭包

既然变量可以指向函数，函数的参数能接收变量，那么一个函数就可以接收另一个函数作为参数，这种函数就称之为高阶函数。

~~~python
“MapReduce: Simplified Data Processing on Large Clusters”

def lazy_sum(*args):
    def sum():
        ax = 0
        for n in args:
            ax = ax + n
        return ax
    return sum
当一个函数返回了一个函数后，其内部的局部变量还被新函数引用，所以，闭包用起来简单，实现起来可不容易

返回闭包时牢记一点：返回函数不要引用任何循环变量，或者后续会发生变化的变量。 
~~~

#### 特殊函数

##### Map ,reduce

map : 函数接收两个参数，一个是函数，一个是Iterable
将传入的函数依次作用到序列的每个元素，并把结果作为新的Iterator返回

list(map(str, [1, 2, 3, 4, 5, 6, 7, 8, 9]))

reduce
把一个函数作用在一个序列[x1, x2, x3, ...]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算

reduce(f, [x1, x2, x3, x4]) = f(f(f(x1, x2), x3), x4)

~~~python
from functools import reduce
DIGITS = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}
def char2num(s):
    return DIGITS[s]
def str2int(s):
    return reduce(lambda x, y: x * 10 + y, map(char2num, s))
~~~

##### filter

函数用于过滤序列 筛选”函数

filter()也接收一个函数和一个序列。和map()不同的是，filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素。

~~~python
def not_empty(s):
    return s and s.strip()
list(filter(not_empty, ['A', '', 'B', None, 'C', '  ']))
# 结果: ['A', 'B', 'C']
~~~

##### sorted

~~~python
sorted([36, 5, -12, 9, -21], key=abs)
~~~

##### lambda 

在传入函数时，有些时候，不需要显式地定义函数，直接传入匿名函数更方便。

##### 偏函数

int()函数还提供额外的base参数，默认值为10。如果传入base参数，就可以做N进制的转换：
import functools
就是帮助我们创建一个偏函数的，不需要我们自己定义int2()，可以直接使用下面的代码创建一个新的函数int2
int2 = functools.partial(int, base=2)
当函数的参数个数太多，需要简化时，使用functools.partial可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单。





### 名称空间（name spaces）

在内存中有一块内存存储变量名与变量间的绑定关系的空间，而这个空间称为名称空间

#### 1.1 内置名称空间

内置名称空间：存放Pyhton解释器自带的名字，如`int、float、len`

生命周期：在解释器启动时生效，在解释器关闭时失效

#### 1.2 全局名称空间

全局名称空间：除了内置和局部的名字之外，其余都存放在全局名称空间，

生命周期：在文件执行时生效，在文件执行结束后失效

#### 1.3 局部名称空间

局部名称空间：用于存放函数调用期间函数体产生的名字

生命周期：在文件执行时函数调用期间时生效，在函数执行结束后失效

<img src="pic/image-20220502225652625.png" alt="image-20220502225652625" style="zoom: 33%;" />

### 作用域

全局作用域：全局有效，全局存活，包含内置名称空间和全局名称空间

局部作用域：局部有小，临时存储，只包含局部名称空间。

global 关键字

nonlocal关键字

**注意**

1. 在局部想要修改全局的可变类型，不需要任何声明，可以直接修改。
2. 在局部如果想要修改全局的不可变类型，需要借助global声明，声明为全局的变量，即可直接修改。

### IO

1: 读取文件

open
read

2： 内存文件

StringIO	顾名思义就是在内存中读写str。
BytesIO

~~~python
from io import StringIO
f = StringIO()
f.write('hello')
f.getvalue()
~~~









### 模块

模块可以看成是一堆函数的集合体

一个py文件内部就可以放一堆函数，如果这个py文件的文件名为`module.py`，模块名则是`module`

模块的4种形式

1. 自定义模块：如果你自己写一个py文件，在文件内写入一堆函数，则它被称为自定义模块，即使用python编写的.py文件

2. 第三方模块：已被编译为共享库或DLL的C或C++扩展

3. 内置模块：使用C编写并链接到python解释器的内置模块

4. 包：把一系列模块组织到一起的文件夹（注：文件夹下有一个__init__.py文件，该文件夹称之为包）

   **首次导入模块发生了3件事：**

   1. 以模块为准创造一个模块的名称空间
   2. 执行模块对应的文件，将执行过程中产生的名字都丢到模块的名称空间
   3. 在当前执行文件中拿到一个模块名

~~~python
import 
from ... import
import

~~~

#### 循环导入的问题

~~~python
# m1.py
print('from m1.py')
from m2 import x
y = 'm1'

# m2.py
print('from m2.py')
from m1 import y
x = 'm2'

# run.py
import m1  

如果运行run.py，则会报错ImportError: cannot import name 'y'
如果运行m1.py，则会报错ImportError: cannot import name 'x'
如果运行m2.py，则会报错ImportError: cannot import name 'y'

# m1.py 可以使用函数定义阶段只识别语法的特性解决循环导入的问题
print('from m1.py')
def func1():
    from m2 import x
    print(x)
y = 'm1'
~~~

最好的解决方法是不要出现循环导入

#### 模块搜索路径的顺序

导入模块时查找模块的顺序是：

1. 先从内存中已经导入的模块中寻找
2. 内置的模块
3. 环境变量sys.path中找

~~~python
import sys
print(f"sys.path: {sys.path}")
sys.path的第一个值是当前执行文件的所在的文件夹
sys.path.append(path)
~~~

编写好的一个python文件可以有两种用途：

1. 脚本，一个文件就是整个程序，用来被执行
2. 模块，文件中存放着一堆功能，用来被导入使用

#### 编译python文件

为了提高加载模块的速度，而绝非运行速度

python解释器会在__pycache__目录中下缓存每个模块编译后的版本

格式为：module.version.pyc

在CPython3.3版本下spam.py模块会被缓存成

__pycache__/spam.cpython-33.pyc

```python
# python解释器在以下两种情况下不检测缓存

1. 如果是在命令行中被直接导入模块，则按照这种方式，每次导入都会重新编译，并且不会存储编译后的结果（python3.3以前的版本应该是这样）
    python -m spam.py

2. 如果源文件不存在，那么缓存的结果也不会被使用，如果想在没有源文件的情况下来使用编译后的结果，则编译后的结果必须在源目录下

sh-3.2  # ls
__pycache__ spam.py
sh-3.2  # rm -rf spam.py 
sh-3.2  # mv __pycache__/spam.cpython-36.pyc ./spam.pyc
sh-3.2  # python3 spam.pyc 
spam
 

# 提示：

1. 模块名区分大小写，foo.py与FOO.py代表的是两个模块
2. 你可以使用-O或者-OO转换python命令来减少编译模块的大小
    -O转换会帮你去掉assert语句
    -OO转换会帮你去掉assert语句和__doc__文档字符串
    由于一些程序可能依赖于assert语句或文档字符串，你应该在在确认需要
    的情况下使用这些选项。
3. 在速度上从.pyc文件中读指令来执行不会比从.py文件中读指令执行更快，只有在模块被加载时，.pyc文件才是更快的
4. 只有使用import语句是才将文件自动编译为.pyc文件，在命令行或标准输入中指定运行脚本则不会生成这类文件，因而我们可以使用compieall模块为一个目录中的所有模块创建.pyc文件

模块可以作为一个脚本（使用python -m compileall）编译Python源  
python -m compileall /module_directory 递归着编译
如果使用python -O -m compileall /module_directory -l则只一层

命令行里使用compile()函数时，自动使用python -O -m compileall
  
详见：https://docs.python.org/3/library/compileall.html#module-compileall
```

```python
import compileall
compileall.compile_dir('$dir')
批量生成.pyc文件
```

### 包

包是模块的一种形式，包的本质就是一个含有`.py`的文件的文件夹。

导入包发生的三件事：

1. 创建一个包的名称空间
2. 由于包是一个文件夹，无法执行包，因此执行包下的.py文件，将执行过程中产生的名字存放于包名称空间中（即包名称空间中存放的名字都是来自于.py）
3. 在当前执行文件中拿到一个名字aaa，aaa是指向包的名称空间的

导入包就是在导入包下的.py，并且可以使用以下两种方式导入：

1. import ...
2. from ... import...

相对导入：

- .代表当前被导入文件所在的文件夹
- ..代表当前被导入文件所在的文件夹的上一级
- ...代表当前被导入文件所在的文件夹的上一级的上一级

1. 包内所有的文件都是被导入使用的，而不是被直接运行的
2. 包内部模块之间的导入可以使用绝对导入（以包的根目录为基准）与相对导入（以当前被导入的模块所在的目录为基准）,推荐使用相对导入
3. 当文件是执行文件时，无法在该文件内用相对导入的语法，只有在文件时被当作模块导入时，该文件内才能使用相对导入的语法
4. 凡是在导入时带点的，点的左边都必须是一个包，`import aaa.bbb.m3.f3`错误

### 常用模块

#### time

```python
import time
time_stamp = time.time()
format_time = time.strftime("%Y-%m-%d %X")
结构化的时间(struct time):
   共有9个元素共九个元素，分别为(年，月，日，时，分，秒，一年中第几周，一年中第几天，夏令时)

```

<img src="pic/image-20220504104241930.png" alt="image-20220504104241930" style="zoom:50%;" />

```python
# datetime模块可以看成是时间加减的模块
import datetime
```

#### random

```python
import random
# 大于0且小于1之间的小数
print(random.random())
# 大于等于1且小于等于3之间的整数
print(random.randint(1, 3))
# 大于等于1且小于3之间的整数
print(random.randrange(1, 3))
# 大于1小于3的小数，如1.927109612082716
print(random.uniform(1, 3))
# 列表内的任意一个元素，即1或者‘23’或者[4,5]
print(random.choice([1, '23', [4, 5]]))
# random.sample([], n)，列表元素任意n个元素的组合，示例n=2
print(random.sample([1, '23', [4, 5]], 2))
lis = [1, 3, 5, 7, 9]
# 打乱l的顺序,相当于"洗牌"
random.shuffle(lis)
```

#### os

|                方法                 | 详解                                                         |
| :---------------------------------: | ------------------------------------------------------------ |
|             os.getcwd()             | 获取当前工作目录，即当前python脚本工作的目录路径             |
|         os.chdir("dirname")         | 改变当前脚本工作目录；相当于shell下cd                        |
|              os.curdir              | 返回当前目录: ('.')                                          |
|              os.pardir              | 获取当前目录的父目录字符串名：('..')                         |
|  os.makedirs('dirname1/dirname2')   | 可生成多层递归目录                                           |
|      os.removedirs('dirname1')      | 若目录为空，则删除，并递归到上一级目录，如若也为空，则删除，依此类推 |
|         os.mkdir('dirname')         | 生成单级目录；相当于shell中mkdir dirname                     |
|         os.rmdir('dirname')         | 删除单级空目录，若目录不为空则无法删除，报错；相当于shell中rmdir dirname |
|        os.listdir('dirname')        | 列出指定目录下的所有文件和子目录，包括隐藏文件，并以列表方式打印 |
|             os.remove()             | 删除一个文件                                                 |
|   os.rename("oldname","newname")    | 重命名文件/目录                                              |
|      os.stat('path/filename')       | 获取文件/目录信息                                            |
|               os.sep                | 输出操作系统特定的路径分隔符，win下为"",Linux下为"/"         |
|             os.linesep              | 输出当前平台使用的行终止符，win下为"\t\n",Linux下为"\n"      |
|             os.pathsep              | 输出用于分割文件路径的字符串 win下为;,Linux下为:             |
|               os.name               | 输出字符串指示当前使用平台。win->'nt'; Linux->'posix'        |
|      os.system("bash command")      | 运行shell命令，直接显示                                      |
|             os.environ              | 获取系统环境变量                                             |
|        os.path.abspath(path)        | 返回path规范化的绝对路径                                     |
|         os.path.split(path)         | 将path分割成目录和文件名二元组返回                           |
|        os.path.dirname(path)        | 返回path的目录。其实就是os.path.split(path)的第一个元素      |
|       os.path.basename(path)        | 返回path最后的文件名。如何path以／或\结尾，那么就会返回空值。即os.path.split(path)的第二个元素 |
|        os.path.exists(path)         | 如果path存在，返回True；如果path不存在，返回False            |
|         os.path.isabs(path)         | 如果path是绝对路径，返回True                                 |
|        os.path.isfile(path)         | 如果path是一个存在的文件，返回True。否则返回False            |
|         os.path.isdir(path)         | 如果path是一个存在的目录，则返回True。否则返回False          |
| os.path.join(path1[, path2[, ...]]) | 将多个路径组合后返回，第一个绝对路径之前的参数将被忽略       |
|       os.path.getatime(path)        | 返回path所指向的文件或者目录的最后存取时间                   |
|       os.path.getmtime(path)        | 返回path所指向的文件或者目录的最后修改时间                   |
|        os.path.getsize(path)        | 返回path的大小                                               |

~~~python
import os
os.name # 操作系统类型
os.uname()
os.environ  环境变量
os.environ.get('PATH')
#3：操作文件和目录
#查看当前目录的绝对路径:
os.path.abspath('.')
os.path.split('/Users/michael/testdir/file.txt') #要拆分路径时，也不要直接去拆字符串
os.path.splitext() #可以直接让你得到文件扩展名，很多时候非常方便：
os.rename('test.txt', 'test.py')
os.remove('test.py')

os.system("unzip dev_data.json.zip") # 执行命令
os.environ['JAVA_HOME'] = r'C:\servies\Java\jdk8'  
os.environ['SPARK_HOME'] = r'D:\software\spark-2.2.0-bin-hadoop2.7'
os.environ['PYTHONPATH'] = r'D:\software\spark-2.2.0-bin-hadoop2.7\python'
~~~





#### sys

|        方法        |                             详解                             |
| :----------------: | :----------------------------------------------------------: |
|      sys.argv      |           命令行参数List，第一个元素是程序本身路径           |
| sys.modules.keys() |                  返回所有已经导入的模块列表                  |
|   sys.exc_info()   | 获取当前正在处理的异常类,exc_type、exc_value、exc_traceback当前处理的异常详细信息 |
|    sys.exit(n)     |                 退出程序，正常退出时exit(0)                  |
|   sys.hexversion   |     获取Python解释程序的版本值，16进制格式如：0x020403F0     |
|    sys.version     |                 获取Python解释程序的版本信息                 |
|     sys.maxint     |                         最大的Int值                          |
|   sys.maxunicode   |                       最大的Unicode值                        |
|    sys.modules     |       返回系统导入的模块字段，key是模块名，value是模块       |
|      sys.path      |    返回模块的搜索路径，初始化时使用PYTHONPATH环境变量的值    |
|    sys.platform    |                     返回操作系统平台名称                     |
|     sys.stdout     |                           标准输出                           |
|     sys.stdin      |                           标准输入                           |
|     sys.stderr     |                           错误输出                           |
|  sys.exc_clear()   |        用来清除当前线程所出现的当前的或最近的错误信息        |
|  sys.exec_prefix   |              返回平台独立的python文件安装的位置              |
|   sys.byteorder    | 本地字节规则的指示器，big-endian平台的值是'big',little-endian平台的值是'little' |
|   sys.copyright    |                   记录python版权相关的东西                   |
|  sys.api_version   |                      解释器的C的API版本                      |

~~~python
#如何获取当前模块的文件名
print(__file__)
import sys
print(sys.argv)
print(sys.executable)
[x for x in os.listdir('.') if os.path.isdir(x)]
#要列出所有的.py文件，也只需一行代码：
[x for x in os.listdir('.') if os.path.isfile(x) and os.path.splitext(x)[1]=='.py']
~~~





#### pathlib

Python 并没有提供 __file__ 这个概念,他是 Pycharm 提供的

```python
import pathlib
print(pathlib.Path.cwd().parent)  #获取某一个文件的父目录
print(os.path.dirname(os.path.dirname(os.getcwd())))  # /Users
#获取当前文件路径
print(os.getcwd())  # '/Users/mac'
print(pathlib.Path.cwd())  # PosixPath('/Users/mac')

#路径拼接
os.path.join(os.path.dirname(os.getcwd()), '路径拼接', '真麻烦')
pathlib.Path.cwd().parent.joinpath(*paths)

```

<img src="pic/image-20220504114323617.png" alt="image-20220504114323617" style="zoom:50%;" />

```python
# python语言实现

Path.iterdir()  # 遍历目录的子目录或者文件

Path.is_dir()  # 判断是否是目录

Path.glob()  # 过滤目录(返回生成器)

Path.resolve()  # 返回绝对路径

Path.exists()  # 判断路径是否存在

Path.open()  # 打开文件(支持with)

Path.unlink()  # 删除文件或目录(目录非空触发异常)
```

```python
# python语言实现

Path.parts  # 分割路径 类似os.path.split(), 不过返回元组
Path.drive  # 返回驱动器名称
Path.root  # 返回路径的根目录
Path.anchor  # 自动判断返回drive或root
Path.parents  # 返回所有上级目录的列表
```

#### shutil模块

高级的文件、文件夹、压缩包处理模块

```python
import shutil

# shutil.copyfileobj(fsrc, fdst[, length])，将文件内容拷贝到另一个文件中
shutil.copyfileobj(open('old.xml', 'r'), open('new.xml', 'w'))
```

```python
shutil.make_archive(base_name, format, ...)，创建压缩包并返回文件路径，例如：zip、tar
```

#### json和pickle 

把对象(变量)从内存中变成可存储或传输的过程称之为序列化

序列化的优点：

1. 持久保存状态：内存是无法永久保存数据的，当程序运行了一段时间，我们断电或者重启程序，内存中关于这个程序的之前一段时间的数据（有结构）都被清空了。但是在断电或重启程序之前将程序当前内存中所有的数据都保存下来（保存到文件中），以便于下次程序执行能够从文件中载入之前的数据，然后继续执行，这就是序列化。
2. 跨平台数据交互：序列化时不仅可以把序列化后的内容写入磁盘，还可以通过网络传输到别的机器上，如果收发的双方约定好实用一种序列化的格式，那么便打破了平台/语言差异化带来的限制，实现了跨平台数据交互。

#### logging

```
print(logging.__file__)
```

```python
import logging

logging.debug('调试信息')
logging.info('正常信息')
logging.warning('警告信息')
logging.error('报错信息')
logging.critical('严重错误信息')
```

v1版本无法指定日志的级别；无法指定日志的格式；只能往屏幕打印，无法写入文件。因此可以改成下述的代码。

```python
import logging

# 日志的基本配置

logging.basicConfig(filename='access.log',
                    format='%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S %p',
                    level=10)

logging.debug('调试信息')  # 10
logging.info('正常信息')  # 20
logging.warning('警告信息')  # 30
logging.error('报错信息')  # 40
logging.critical('严重错误信息')  # 50
```

```python
可在logging.basicConfig()函数中可通过具体参数来更改logging模块默认行为，可用参数有:
    
filename：用指定的文件名创建FiledHandler（后边会具体讲解handler的概念），这样日志会被存储在指定的文件中。
filemode：文件打开方式，在指定了filename时使用这个参数，默认值为“a”还可指定为“w”。
format：指定handler使用的日志显示格式。
datefmt：指定日期时间格式。
level：设置rootlogger（后边会讲解具体概念）的日志级别
stream：用指定的stream创建StreamHandler。可以指定输出到sys.stderr,sys.stdout或者文件，默认为sys.stderr。若同时列出了filename和stream两个参数，则stream参数会被忽略。


format参数中可能用到的格式化串：

%(name)s Logger的名字
%(levelno)s 数字形式的日志级别
%(levelname)s 文本形式的日志级别
%(pathname)s 调用日志输出函数的模块的完整路径名，可能没有
%(filename)s 调用日志输出函数的模块的文件名
%(module)s 调用日志输出函数的模块名
%(funcName)s 调用日志输出函数的函数名
%(lineno)d 调用日志输出函数的语句所在的代码行
%(created)f 当前时间，用UNIX标准的表示时间的浮 点数表示
%(relativeCreated)d 输出日志信息时的，自Logger创建以 来的毫秒数
%(asctime)s 字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒
%(thread)d 线程ID。可能没有
%(threadName)s 线程名。可能没有
%(process)d 进程ID。可能没有
%(message)s用户输出的消息
```

logging模块包含四种角色：logger、Filter、Formatter对象、Handler

1. logger：产生日志的对象
2. Filter：过滤日志的对象
3. Formatter对象：可以定制不同的日志格式对象，然后绑定给不同的Handler对象使用，以此来控制不同的Handler的日志格式
4. Handler：接收日志然后控制打印到不同的地方，FileHandler用来打印到文件中，StreamHandler用来打印到终端

**日志配置文件**

my_logging.py

```python
import os
import logging.config
# 定义三种日志输出格式 开始
standard_format = '[%(asctime)s][%(threadName)s:%(thread)d][task_id:%(name)s][%(filename)s:%(lineno)d]' \
                  '[%(levelname)s][%(message)s]'  # 其中name为getLogger()指定的名字；lineno为调用日志输出函数的语句所在的代码行
simple_format = '[%(levelname)s][%(asctime)s][%(filename)s:%(lineno)d]%(message)s'
id_simple_format = '[%(levelname)s][%(asctime)s] %(message)s'
# 定义日志输出格式 结束

logfile_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # log文件的目录，需要自定义文件路径 # atm
logfile_dir = os.path.join(logfile_dir, 'log')  # C:\Users\oldboy\Desktop\atm\log

logfile_name = 'log.log'  # log文件名，需要自定义路径名

# 如果不存在定义的日志目录就创建一个
if not os.path.isdir(logfile_dir):  # C:\Users\oldboy\Desktop\atm\log
    os.mkdir(logfile_dir)

# log文件的全路径
logfile_path = os.path.join(logfile_dir, logfile_name)  # C:\Users\oldboy\Desktop\atm\log\log.log
# 定义日志路径 结束

# log配置字典
LOGGING_DIC = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'standard': {
            'format': standard_format
        },
        'simple': {
            'format': simple_format
        },
    },
    'filters': {},  # filter可以不定义
    'handlers': {
        # 打印到终端的日志
        'console': {
            'level': 'DEBUG',
            'class': 'logging.StreamHandler',  # 打印到屏幕
            'formatter': 'simple'
        },
        # 打印到文件的日志,收集info及以上的日志
        'default': {
            'level': 'INFO',
            'class': 'logging.handlers.RotatingFileHandler',  # 保存到文件
            'formatter': 'standard',
            'filename': logfile_path,  # 日志文件
            'maxBytes': 1024 * 1024 * 5,  # 日志大小 5M  (*****)
            'backupCount': 5,
            'encoding': 'utf-8',  # 日志文件的编码，再也不用担心中文log乱码了
        },
    },
    'loggers': {
        # logging.getLogger(__name__)拿到的logger配置。如果''设置为固定值logger1，则下次导入必须设置成logging.getLogger('logger1')
        '': {
            # 这里把上面定义的两个handler都加上，即log数据既写入文件又打印到屏幕
            'handlers': ['default', 'console'],
            'level': 'DEBUG',
            'propagate': False,  # 向上（更高level的logger）传递
        },
    },
}



def load_my_logging_cfg():
    logging.config.dictConfig(LOGGING_DIC)  # 导入上面定义的logging配置
    logger = logging.getLogger(__name__)  # 生成一个log实例
    logger.info('It works!')  # 记录该文件的运行状态
    
    return logger


if __name__ == '__main__':
    load_my_logging_cfg()
```

使用日志

```python
import time
import logging
import my_logging  # 导入自定义的logging配置

logger = logging.getLogger(__name__)  # 生成logger实例


def demo():
    logger.debug("start range... time:{}".format(time.time()))
    logger.info("中文测试开始。。。")
    for i in range(10):
        logger.debug("i:{}".format(i))
        time.sleep(0.2)
    else:
        logger.debug("over range... time:{}".format(time.time()))
    logger.info("中文测试结束。。。")


if __name__ == "__main__":
    my_logging.load_my_logging_cfg()  # 在你程序文件的入口加载自定义logging配置
    demo()
    
    
    
```

type

~~~
int、long、float: 整型、长整形、浮点型
bool、str: 布尔型、字符串类型
List、 Tuple、 Dict、 Set:列表、元组、字典、集合
Iterable、Iterator:可迭代类型、迭代器类型
Generator：生成器类型
~~~

#### collections

1:namedtuple是一个函数，它用来创建一个自定义的tuple对象， 一个点的二维坐标就可以表示成：

2:deque
使用list存储数据时，按索引访问元素很快，但是插入和删除元素就很慢了，因为list是线性存储，数据量大的时候，插入和删除效率很低。
deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈：

deque除了实现list的append()和pop()外，还支持appendleft()和popleft()，这样就可以非常高效地往头部添加或删除元素。

3:defaultdict
使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict

4: OrderedDict 保持Key的顺序
  OrderedDict的Key会按照插入的顺序排列，不是Key本身排序

5:ChainMap
6: Counter



~~~python

from collections import namedtuple
from collections import deque
from collections import defaultdict
from collections import OrderedDict
from collections import Counter

#namedtuple 创建一个自定义的tuple对象
Point = namedtuple('Point', ['x', 'y'])

#deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈：
#defaultdict
dd = defaultdict(lambda: 'N/A')
#OrderedDict 保持Key的顺序，可以用OrderedDict 
#会按照插入的顺序排列，不是Key本身排序
od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])

#Counter是一个简单的计数器，例如，统计字符出现的个数：




~~~

#### tqdm

https://blog.csdn.net/winter2121/article/details/111356587

~~~python
tqdm()主要参数默认值与解释
iterable=None，可迭代对象。如上一节中的range(20)
desc=None，传入str类型，作为进度条标题。如上一节中的desc='It\'s a test'
total=None，预期的迭代次数。一般不填，默认为iterable的长度。
leave=True，迭代结束时，是否保留最终的进度条。默认保留。
file=None，输出指向位置，默认是终端，一般不需要设置。
ncols=None，可以自定义进度条的总长度
unit，描述处理项目的文字，默认’it’，即100it/s；处理照片设置为’img’，则为100img/s
postfix，以字典形式传入详细信息，将显示在进度条中。例如postfix={'value': 520}
unit_scale，自动根据国际标准进行项目处理速度单位的换算，例如100000it/s换算为100kit/s

tqdm()的返回值:
    tqdm()的返回值是一个可迭代对象，迭代的每一个元素就是iterable的每一个参数。


~~~



#### re

~~~python
[^a-z]反取
re.findall("[^a-z]", "匹配s规则这s个字符串是否s匹配f规则则re则则则")  # 反取，匹配出除字母外的字符
*元字符	0个或多个原本字符
+元字符	前面的一个字符可以是1个或多个原本字符
?元字符	匹配一个字符0次或1次
{}元字符	(范围)
. 		任意字符
{0,}匹配前一个字符0或多次,等同于*元字符
{+,}匹配前一个字符1次或无限次,等同于+元字符
{0,1}匹配前一个字符0次或1次,等同于?元字符

[]元字符(字符集)	对应的位置可以是字符集中任意字符
[^]	非，反取，匹配出除[^]里面的字符，^元字符如果写到字符集里就是反取
\d	\D它相当于类[^0-9]
\d+	匹配一位或者多位数的数字时用
\s	匹配任何空白字符，它相当于类[\t\n\r\f\v]
\S	匹配任何非空白字符，它相当于类[^\t\n\r\f\v]
\w	匹配包括下划线在内任何字母数字字符，它相当于类[a-zA-Z0-9_]
\W	匹配非任何字母数字字符包括下划线在内，它相当于类[^a-zA-Z0-9_]

()元字符	(分组)
(adc)*	*控制的前导字符就是()里的整体内容，不再是前导一个字符
|元字符(或)


一种是直接在函数里书写规则，推荐使用
a = re.findall("匹配规则", "这个字符串是否有匹配规则的字符")

# 将正则表达式编译成Pattern对象
pattern = re.compile(r'hello')
# 使用Pattern匹配文本，获得匹配结果，无法匹配时将返回None
match = pattern.match('hello world!')

在Python的正则表达式中，有一个参数为re.S。它表示 “.” 的作用扩展到整个字符串，包括“\n”
c = re.findall('hello(.*?)world', a, re.S)
 re.I 不区分大小写
 re.sub

~~~

##### re模块中常用功能函数

1. 一种是直接在函数里书写规则，推荐使用
2. 另一种是先将正则表达式的字符串形式编译为Pattern实例，然后使用Pattern实例处理文本并获得匹配结果

###### re.match

从头匹配一个符合规则的字符串，从起始位置开始匹配，匹配成功返回一个对象，未匹配成功返回None

match(pattern, string, flags=0)

- pattern： 正则模型
- string ： 要匹配的字符串
- falgs ： 匹配模式

match()函数 与 search()函数基本是一样的功能，不一样的就是match()匹配字符串开始位置的一个符合规则的字符串，search()是在字符串全局匹配第一个合规则的字符串

###### re.search

search,浏览全部字符串，匹配第一符合规则的字符串，浏览整个字符串去匹配第一个，未匹配成功返回None

search(pattern, string, flags=0)

- pattern： 正则模型
- string ： 要匹配的字符串
- falgs ： 匹配模式

###### re.findall

findall(pattern, string, flags=0)

- pattern： 正则模型
- string ： 要匹配的字符串
- falgs ： 匹配模式

一旦匹配成，再次匹配，是从前一次匹配成功的，后面一位开始的，也可以理解为匹配成功的字符串，不在参与下次匹配

######  re.split

根据正则匹配分割字符串，返回分割后的一个列表

split(pattern, string, maxsplit=0, flags=0)

- pattern： 正则模型
- string ： 要匹配的字符串
- maxsplit：指定分割个数
- flags ： 匹配模式

###### re.sub

替换匹配成功的指定位置字符串

sub(pattern, repl, string, count=0, flags=0)

- pattern： 正则模型
- repl ： 要替换的字符串
- string ： 要匹配的字符串
- count ： 指定匹配个数
- flags ： 匹配模式

###### re.subn

替换匹配成功的指定位置字符串,并且返回替换次数，可以用两个变量分别接受

subn(pattern, repl, string, count=0, flags=0)

- pattern： 正则模型
- repl ： 要替换的字符串
- string ： 要匹配的字符串
- count ： 指定匹配个数
- flags ： 匹配模式

###### 注意事项

r 原生字符

回类型为表达式对象的，如：<_sre.SRE_Match object; span=(6, 7), match='a'>，

1. 正则表达式，返回类型为表达式对象的，如：<_sre.SRE_Match object; span=(6, 7), match='a'>，返回对象时，需要用正则方法取字符串，方法有：
   1. group() # 获取匹配到的所有结果，不管有没有分组将匹配到的全部拿出来，有参取匹配到的第几个如2
   2. groups() # 获取模型中匹配到的分组结果，只拿出匹配到的字符串中分组部分的结果
   3. groupdict() # 获取模型中匹配到的分组结果，只拿出匹配到的字符串中分组部分定义了key的组结果

其他模块

~~~
numpy pandas matplotlib
~~~

### class

注意类中定义变量使用驼峰体

曾经定义函数，函数只检测语法，不执行代码，但是定义类的时候，类体代码会在类定义阶段就立刻执行，并且会产生一个类的名称空间，也就是说类的本身其实就是一个容器/名称空间，是用来存放名字的，这是类的用途之一

**属性查找**

- 首先从自身查找，没找到往类中找，类中没有则会报错。即对象的属性查找顺序为：自身--》类--》报错

- 类名称空间中定义的数据属性和函数属性都是共享给所有对象用的
- 对象名称空间中定义的只有数据属性，而且是对象所独有的数据属性

类中定义的函数，**类**确实可以使用，但其实类定义的函数大多情况下都是绑定给**对象**用的，所以在类中定义的函数都应该自带一个参数self



#### 属性权限控制

~~~python
class后面紧接着是类名，即Student，类名通常是大写开头的单词

class Student(object):   # 所有类最终都会继承的类 object
    def __init__(self ,name ,score) :
        self.__name = name
        self.score = score
        pass 
    def get_name(self):
        return self.__name
    def set_score(self, score):
        self.__score = score
    pass
#访问限制
1：如果要让内部属性不被外部访问，可以把属性的名称前加上两个下划线__
2：如果外部代码要获取name和score怎么办？
可以给Student类增加get_name和get_score这样的方法：
 def get_name(self):
        return self.__name
    
3：#双下划线开头的实例变量是不是一定不能从外部访问呢？
 不能直接访问__name是因为Python解释器对外把__name变量改成了_Student__name，
 所以，仍然可以通过_Student__name来访问__name变量：  
bart = Student('Bart Simpson', 59)
bart.get_name() 'Bart Simpson' 
bart.__name = 'New Name' # 设置__name变量！
bart.__name
表面上看，外部代码“成功”地设置了__name变量，但实际上这个__name变量和class内部的__name变量不是一个变量！内部的__name变量已经被Python解释器自动改成了_Student__name，而外部代码给bart新增了一个__name变量。不信试试：

4: 当我们定义了一个类属性后，这个属性虽然归类所有，但类的所有实例都可以访问到
del s.name # 如果删除实例的name属性
在编写程序的时候，千万不要对实例属性和类属性使用相同的名字，因为相同名称的实例属性将屏蔽掉类属性，但是当你删除实例属性后，再使用相同的名称，访问到的将是类属性。
~~~

```python

class.__dict__
print(object.__dict__)
class OldboyStudent:
    school = 'oldboy'  # 对所有实例共享 如果类的属性改了，则其他对象的属性也会随之改变
    def __init__(self, name, age, gender):
        """调用类的时候自动触发"""
        self.name = name
        self.age = age
        self.gender = gender
        print('*' * 50)
    def choose_course(self):
        print('is choosing course')
stu1 = OldboyStudent('nick', 18, 'male')        
通过上述现象可以发现，调用类时发生两件事：
创造一个空对象
自动触发类中__init__功能的执行，将stu1以及调用类括号内的参数一同传入   
```

#### 类的分类

新式类

- 继承了object的类以及该类的子类，都是新式类
- Python3中所有的类都是新式类

经典类

- 没有继承object的类以及该类的子类，都是经典类
- 只有Python2中才有经典类

Java和C#中子类只能继承一个父类，而Python中子类可以同时继承多个父类，如A(B,C,D)

如果继承关系为菱形结构，即子类的父类最后继承了同一个类，那么属性的查找方式有两种：

- 经典类下：深度优先
- 广度优先：广度优先
- 经典类：一条路走到黑，深度优先

python到底是如何实现继承的，对于你定义的每一个类，python会计算出一个方法解析顺序(MRO)列表，这个MRO列表就是一个简单的所有基类的线性顺序列表，如：

为了实现继承，python会在MRO列表上从左到右开始查找基类，直到找到第一个匹配这个属性的类为止。

而这个MRO列表的构造是通过一个**C3线性化算法**来实现的。我们不去深究这个算法的数学原理，它实际上就是合并所有父类的MRO列表并遵循如下三条准则:

1. 子类会先于父类被检查
2. 多个父类会根据它们在列表中的顺序被检查
3. 如果对下一个类存在两个合法的选择，选择第一个父类



通过对象调用绑定到对象的方法，会有一个自动传值的过程，即自动将当前对象传递给方法的第一个参数（self，一般都叫self，也可以写成别的名称）；若是使用类调用，则第一个参数需要手动传值。

```python
class Person:
    country = "China"
    def __init__(self, name, age):
        self.name = name
        self.age = age
    def speak(self):
        print(self.name + ', ' + str(self.age))
p = Person('Kitty', 18)
print(p.__dict__)
print(Person.__dict__['speak'])  #<function Person.speak at 0x000001FCDBDC33A0>
speak即为绑定到对象的方法，这个方法不在对象的名称空间中，而是在类的名称空间中。
通过对象调用绑定到对象的方法，会有一个自动传值的过程，即自动将当前对象传递给方法的第一个参数（self，一般都叫self，也可以写成别的名称）；若是使用类调用，则第一个参数需要手动传值。
p.speak()  # 通过对象调用
Person.speak(p)  # 通过类调用
```

#### 类的绑定方法

类中使用 @classmethod 修饰的方法就是绑定到类的方法。这类方法专门为类定制。通过类名调用绑定到类的方法时，会将类本身当做参数传给类方法的第一个参数。

```python
class Operate_database():
    host = '192.168.0.5'
    port = '3306'
    user = 'abc'
    password = '123456'
    
    @classmethod
    def connect(cls):  # 约定俗成第一个参数名为cls，也可以定义为其他参数名
        print(cls)
        print(cls.host + ':' + cls.port + ' ' + cls.user + '/' + cls.password)
Operate_database.connect()
Operate_database().connect()  # 输出结果一致
<class '__main__.Operate_database'>
192.168.0.5:3306 abc/123456
```

在类内部使用 @staticmethod 修饰的方法即为非绑定方法，这类方法和普通定义的函数没有区别，不与类或对象绑定，谁都可以调用，且没有自动传值的效果。

```python
import hashlib
class Operate_database():
    def __init__(self, host, port, user, password):
        self.host = host
        self.port = port
        self.user = user
        self.password = password

    @staticmethod
    def get_passwrod(salt, password):
        m = hashlib.md5(salt.encode('utf-8'))  # 加盐处理
        m.update(password.encode('utf-8'))
        return m.hexdigest()

hash_password = Operate_database.get_passwrod('lala', '123456')  # 通过类来调用
print(hash_password)
p = Operate_database('192.168.0.5', '3306', 'abc', '123456')
hash_password = p.get_passwrod(p.user, p.password)  # 也可以通过对象调用
print(hash_password)
简而言之，非绑定方法就是将普通方法放到了类的内部。
```

如果函数体代码需要用外部传入的类（class），则应该将该函数定义成绑定给类的方法

如果函数体代码需要用外部传入的对象(object)，则应该将该函数定义成绑定给对象的方法

#### 封装

1. 方法的封装
2. 属性的封装

封装数据的主要原因是：保护隐私

封装方法的主要原因是：隔离复杂度

在python中用双下划线的方式实现隐藏属性（设置成私有的）

```python
class A:
    __N = 0  # 类的数据属性就应该是共享的,但是语法上是可以把类的数据属性设置成私有的如__N,会变形为_A__N

    def __init__(self):
        self.__X = 10  # 变形为self._A__X

    def __foo(self):  # 变形为_A__foo
        print('from A')

    def bar(self):
        self.__foo()  # 只有在类内部才可以通过__foo的形式访问到.
```

这种自动变形的特点：

1. 类中定义的__x只能在内部使用，如self.__x，引用的就是变形的结果。
2. 这种变形其实正是针对内部的变形，在外部是无法通过__x这个名字访问到的。
3. 在子类定义的__x不会覆盖在父类定义的__x，因为子类中变形成了：_子类名__x,而父类中变形成了：_父类名__x，即双下滑线开头的属性在继承给子类时，子类是无法覆盖的。

python并不会真的阻止你访问私有的属性，模块也遵循这种约定，如果模块中的变量名_private_module以单下划线开头，那么from module import \*时不能被导入该变量，但是你from module import _private_module依然是可以导入该变量的

其实很多时候你去调用一个模块的功能时会遇到单下划线开头的(socket._socket,sys._home,sys._clear_type_cache),这些都是私有的，原则上是供内部调用的，作为外部的你，一意孤行也是可以用的，只不过显得稍微傻逼一点点

#### 继承

- 继承是一种新建类的方式，新建的类称为子类，被继承的类称为父类
- 继承的特性是：子类会遗传父类的属性
- 继承是类与类之间的关系

继承有什么好处？
1：最大的好处是子类获得了父类的全部功能
2：子类和父类都存在相同的run()方法时，我们说，子类的run()覆盖了父类的run()，在代码运行的时候，总是会调用子类的run()。

3：在继承关系中，如果一个实例的数据类型是某个子类，那它的数据类型也可以被看做是父类。但是，反过来就不行：

```python
class Parent1:
    pass
class Parent2:
    pass
class Sub1(Parent1, Parent2): # 多继承
    pass
print(Sub1.__bases__) # 使用__bases__方法可以获取对象继承的类

```

#### 派生

子类中新定义的属性的这个过程叫做派生

并且需要记住子类在使用派生的属性时始终以自己的为准

```python
class OldboyPeople:
    """由于学生和老师都是人，因此人都有姓名、年龄、性别"""
    school = 'oldboy'

    def __init__(self, name, age, gender):
        self.name = name
        self.age = age
        self.gender = gender


class OldboyStudent(OldboyPeople):
    """由于学生类没有独自的__init__()方法，因此不需要声明继承父类的__init__()方法，会自动继承"""

    def choose_course(self):
        print('%s is choosing course' % self.name)


class OldboyTeacher(OldboyPeople):
    """由于老师类有独自的__init__()方法，因此需要声明继承父类的__init__()"""

    def __init__(self, name, age, gender, level):
        OldboyPeople.__init__(self, name, age, gender)
        self.level = level  # 派生

    def score(self, stu_obj, num):
        print('%s is scoring' % self.name)
        stu_obj.score = num


stu1 = OldboyStudent('tank', 18, 'male')
tea1 = OldboyTeacher('nick', 18, 'male', 10)
```

**派生方法二(super)**

- 严格以来继承属性查找关系
- super()会得到一个特殊的对象，该对象就是专门用来访问父类中的属性的（按照继承的关系）
- super().__init__(不用为self传值)
- super的完整用法是super(自己的类名,self),在python2中需要写完整，而python3中可以简写为super()

#### 组合

```python
class Course:
    def __init__(self, name, period, price):
        self.name = name
        self.period = period
        self.price = price

    def tell_info(self):
        msg = """
        课程名：%s
        课程周期：%s
        课程价钱：%s
        """ % (self.name, self.period, self.price)
        print(msg)
class OldboyPeople:
    school = 'oldboy'
    def __init__(self, name, age, sex):
        self.name = name
        self.age = age
        self.sex = sex
class OldboyStudent(OldboyPeople):
    def __init__(self, name, age, sex, stu_id):
        OldboyPeople.__init__(self, name, age, sex)
        self.stu_id = stu_id
    def choose_course(self):
        print('%s is choosing course' % self.name)
python = Course('python全栈开发', '5mons', 3000)
python.tell_info()

```

#### 多态

多态指的是一类事物有多种形态，（一个抽象类有多个子类，因而多态的概念依赖于继承）

1. 序列数据类型有多种形态：字符串，列表，元组
2. 动物有多种形态：人，狗，猪

```python
import abc
class Animal(metaclass=abc.ABCMeta):  # 同一类事物:动物
    @abc.abstractmethod  # 上述代码子类是约定俗称的实现这个方法，加上@abc.abstractmethod装饰器后严格控制子类必须实现这个方法
    def talk(self):
        raise AttributeError('子类必须实现这个方法')
class People(Animal):  # 动物的形态之一:人
    def talk(self):
        print('say hello')
class Dog(Animal):  # 动物的形态之二:狗
    def talk(self):
        print('say wangwang')
class Pig(Animal):  # 动物的形态之三:猪
    def talk(self):
        print('say aoao')

peo2 = People()
pig2 = Pig()
d2 = Dog()
peo2.talk()
pig2.talk()
d2.talk()
```

**多态与多态性是两种概念**

#### **多态性**

是指具有不同功能的**函数**可以使用相同的函数名，这样就可以用一个函数名调用不同内容的函数。在面向对象方法中一般是这样表述多态性：向不同的对象发送同一条消息，不同的对象在接收时会产生不同的行为（即方法）。也就是说，每个对象可以用自己的方式去响应共同的消息。所谓消息，就是调用函数，不同的行为就是指不同的实现，即执行不同的函数。

```python
# 多态性：一种调用方式，不同的执行效果（多态性）
def func(obj):
    obj.run()
func(peo1)
func(pig1)
func(d1)
```

```python
# 多态性依赖于：继承
# 多态性：定义统一的接口
def func(obj):  # obj这个参数没有类型限制，可以传入不同类型的值
    obj.talk()  # 调用的逻辑都一样，执行的结果却不一样
func(peo2)
func(pig2)
func(d2)
```

**多态性的好处**

1. 增加了程序的灵活性：以不变应万变，不论对象千变万化，使用者都是同一种形式去调用，如func(animal)
2. 增加了程序额可扩展性：通过继承Animal类创建了一个新的类，使用者无需更改自己的代码，还是用func(animal)去调用

多态：同一种事物的多种形态，动物分为人类，猪类（在定义角度）
多态性：一种调用方式，不同的执行效果（多态性）

#### property装饰器

- property装饰器用于将被装饰的**方法伪装**成一个数据属性，在使用时可以不用加括号而直接使用

- 通过使用property属性，能够简化调用者在获取数据的流程

```python
class Goods(object):
    @property
    def size(self):
        return 100
g = Goods()
print(g.size) #100

Python的property属性的功能是：property属性内部进行一系列的逻辑计算，最终将计算结果返回。
```

property属性的定义和调用要注意一下几点：

1. 定义时，在实例方法的基础上添加 @property 装饰器；**并且仅有**一个self参数

2. 调用时，无需括号

##### property属性的两种方式

1. 装饰器 即：在方法上应用装饰器（推荐使用）
2. 类属性 即：在类中定义值为property对象的类属性（Python2历史遗留

Python中的类有经典类和新式类，新式类的属性比经典类的属性丰富。（ 如果类继object，那么该类是新式类 ）

经典类，具有一种 @property 装饰器：

```python
# ############### 定义 ###############
class Goods:
    @property
    def price(self):
        return "laowang"
# ############### 调用 ###############
obj = Goods()
result = obj.price  # 自动执行 @property 修饰的 price 方法，并获取方法的返回值
print(result)
```

**新式类，具有三种 @property 装饰器：**

- **经典类中的属性只有一种访问方式，其对应被 @property 修饰的方法**
- **新式类中的属性有三种访问方式，**

并分别对应了三个被 @property、@方法名.setter、@方法名.deleter 修饰的方法

计算 修改 删除 

```python
class Goods(object):
    def __init__(self):
        # 原价
        self.original_price = 100
        # 折扣
        self.discount = 0.8
    @property
    def price(self):
        # 实际价格 = 原价 * 折扣
        new_price = self.original_price * self.discount
        return new_price
    @price.setter
    def price(self, value):
        self.original_price = value
    @price.deleter
    def price(self):
        print('del')
        del self.original_price
obj = Goods()
print(obj.price)  # 获取商品价格  对某一个属性的操作 
obj.price = 200  # 修改商品原价
print(obj.price)
del obj.price  # 删除商品原价
```

**创建值为property对象的类属性**

当使用类属性的方式创建property属性时，经典类和新式类无区别

property方法中有个四个参数

1. 第一个参数是方法名，调用 对象.属性 时自动触发执行方法
2. 第二个参数是方法名，调用 对象.属性 ＝ XXX 时自动触发执行方法
3. 第三个参数是方法名，调用 del 对象.属性 时自动触发执行方法
4. 第四个参数是字符串，调用 对象.属性.__doc__ ，此参数是该属性的描述信息

```python
#coding=utf-8
class Foo(object):
    def get_bar(self):
        print("getter...")
        return 'laowang'

    def set_bar(self, value):
        """必须两个参数"""
        print("setter...")
        return 'set value' + value

    def del_bar(self):
        print("deleter...")
        return 'laowang'

    BAR = property(get_bar, set_bar, del_bar, "description...")
obj = Foo()
obj.BAR  # 自动调用第一个参数中定义的方法：get_bar
obj.BAR = "alex"  # 自动调用第二个参数中定义的方法：set_bar方法，并将“alex”当作参数传入
desc = Foo.BAR.__doc__  # 自动获取第四个参数中设置的值：description...
del obj.BAR  # 自动调用第三个参数中定义的方法：del_bar方法
```

由于类属性方式创建property属性具有3种访问方式，我们可以根据它们几个属性的访问特点，分别将三个方法定义为对同一个属性：获取、修改、删除

```python
class Goods(object):
    def __init__(self):
        # 原价
        self.original_price = 100
        # 折扣
        self.discount = 0.8

    def get_price(self):
        # 实际价格 = 原价 * 折扣
        new_price = self.original_price * self.discount
        return new_price

    def set_price(self, value):
        self.original_price = value

    def del_price(self):
        del self.original_price

    PRICE = property(get_price, set_price, del_price, '价格属性描述...')

obj = Goods()
obj.PRICE  # 获取商品价格
obj.PRICE = 200  # 修改商品原价
del obj.PRICE  # 删除商品原价

```

私有属性添加getter和setter方法

```python
class Money(object):
    def __init__(self):
        self.__money = 0

    def getMoney(self):
        return self.__money

    def setMoney(self, value):
        if isinstance(value, int):
            self.__money = value
        else:
            print("error:不是整型数字")

    # 定义一个属性，当对这个money设置值时调用setMoney,当获取值时调用getMoney
    money = property(getMoney, setMoney)
a = Money()
a.money = 100  # 调用setMoney方法
print(a.money)  # 调用getMoney方法
```

```python
class Money(object):
    def __init__(self):
        self.__money = 0

    # 使用装饰器对money进行装饰，那么会自动添加一个叫money的属性，当调用获取money的值时，调用装饰的方法
    @property
    def money(self):
        return self.__money

    # 使用装饰器对money进行装饰，当对money设置值时，调用装饰的方法
    @money.setter
    def money(self, value):
        if isinstance(value, int):
            self.__money = value
        else:
            print("error:不是整型数字")
a = Money()
a.money = 100
print(a.money)
```

```python3
静态方法的使用场景：

如果在方法中不需要访问任何实例方法和属性，纯粹地通过传入参数并返回数据的功能性方法，那么它就适合用静态方法来定义，它节省了实例化对象的开销成本，往往这种方法放在类外面的模块层作为一个函数存在也是没问题的，而放在类中，仅为这个类服务。例如下面是微信公众号开发中验证微信签名的一个例子，它没有引用任何类或者实例相关的属性和方法
。
```





#### class 高阶

##### 判断类型

动态语言和静态语言最大的不同，就是函数和类的定义，不是编译时定义的，而是运行时动态创建的。

**type instance issubclass**

一个明显的区别是在判断子类

type()不会认为子类是一种父类类型；isinstance()会认为子类是一种父类类型

```python
class Foo(object):
    pass
class Bar(Foo):
    pass
print(type(Foo()) == Foo)  #True
print(type(Bar()) == Foo) # False
# isinstance参数为对象和类
print(isinstance(Bar(),Foo)) # True
class Parent:
    pass
class Sub(Parent):
    pass
print(issubclass(Sub, Parent)) #true
print(issubclass(Parent, object))
```

##### 反射

反射就是通过字符串来操作类或者对象的属性

- 反射本质就是在使用内置函数，其中反射有以下四个内置函数：

1. hasattr：判断一个方法是否存在与这个类中

2. getattr：根据字符串去获取obj对象里的对应的方法的内存地址，加"()"括号即可执行

3. setattr：通过setattr将外部的一个函数绑定到实例中

4. delattr：删除一个实例或者类中的方法

   ```python
   class People:
       country = 'China'
       def __init__(self, name):
           self.name = name
       def eat(self):
           print('%s is eating' % self.name)
   peo1 = People('nick')
   
   print(hasattr(peo1, 'eat'))  # peo1.eat
   print(getattr(peo1, 'eat'))  # peo1.eat
   print(getattr(peo1, 'xxxxx', None))
   setattr(peo1, 'age', 18)  # peo1.age=18
   delattr(peo1, 'name')  # del peo1.name
   ```

反射就是通过字符串的形式，导入模块；通过字符串的形式，去模块寻找指定函数，并执行。利用字符串的形式去对象（模块）中操作（查找/获取/删除/添加）成员，一种基于字符串的事件驱动！

hasattr(object, name)

说明：判断对象object是否包含名为name的特性（hasattr是通过调用getattr(ojbect, name)是否抛出异常来实现的）

setattr(object, name, value)

这是相对应的getattr()。参数是一个对象,一个字符串和一个任意值。字符串可能会列出一个现有的属性或一个新的属性。这个函数将值赋给属性的。该对象允许它提供。例如,setattr(x,“foobar”,123)相当于x.foobar = 123。

delattr(object, name)

与setattr()相关的一组函数。参数是由一个对象(记住python中一切皆是对象)和一个字符串组成的。string参数必须是对象属性名之一。该函数删除该obj的一个由string指定的属性。delattr(x, 'foobar')=del x.foobar

```python
r = hasattr(commons, xxx)  # 判断某个函数或者变量是否存在
print(r)

setattr(commons, 'age', 18)  # 给commons模块增加一个全局变量age = 18，创建成功返回none

setattr(commons, 'age', lambda a: a + 1)  # 给模块添加一个函数

delattr(commons, 'age')  # 删除模块中某个变量或者函数
```

```python
class Foo:
    x = 1
    def __init__(self, y):
        self.y = y
    def __getattr__(self, item):
        print('----> from getattr:你找的属性不存在')
    def __setattr__(self, key, value):
        print('----> from setattr')
        # self.key = value  # 这就无限递归了,你好好想想
        # self.__dict__[key] = value  # 应该使用它
    def __delattr__(self, item):
        print('----> from delattr')
        # del self.item  # 无限递归了
        self.__dict__.pop(item)
f1 = Foo(10)

```

- 不存在的属性访问，触发__getattr__

__getattribute__

- 查找属性无论是否存在，都会执行

- 当__getattribute__与__getattr__同时存在，只会执行__getattrbute__，除非__getattribute__在执行过程中抛出异常AttributeError

##### 描述符

__get__()：调用一个属性时，触发

__set__()：为一个属性赋值时，触发

__delete__()：采用del删除属性时，触发

```python
class Foo:  # 在python3中Foo是新式类，它实现了__get__()，__set__()，__delete__()中的一个三种方法的一个，这个类就被称作一个描述符
    def __get__(self, instance, owner):
        pass

    def __set__(self, instance, value):
        pass

    def __delete__(self, instance):
        pass
```

- 包含这三个方法的新式类称为描述符，由这个类产生的实例进行属性的调用/赋值/删除，并不会触发这三个方法

```python
class Str:
    """描述符Str"""
    def __get__(self, instance, owner):
        print('Str调用')
    def __set__(self, instance, value):
        print('Str设置...')
    def __delete__(self, instance):
        print('Str删除...')
class Int:
    """描述符Int"""
    def __get__(self, instance, owner):
        print('Int调用')
    def __set__(self, instance, value):
        print('Int设置...')
    def __delete__(self, instance):
        print('Int删除...')
class People:
    name = Str()
    age = Int()
    def __init__(self, name, age):  # name被Str类代理，age被Int类代理
        self.name = name
        self.age = age
# 何地？：定义成另外一个类的类属性
# 何时？：且看下列演示
p1 = People('alex', 18) 
#Str设置...
#Int设置...


```

###### 两种描述符

数据描述符

- 至少实现了__get__()和__set__()

非数据描述符

- 没有实现__set__()

1. 描述符本身应该定义成新式类，被代理的类也应该是新式类

2. 必须把描述符定义成这个类的类属性，不能为定义到构造函数中

3. 要严格遵循该优先级，优先级由高到底分别是
   1.类属性
   2.数据描述符
   3.实例属性
   4.非数据描述符
   5.找不到的属性触发__getattr__()


###### 使用描述符号

- 众所周知，python是弱类型语言，即参数的赋值没有类型限制，下面我们通过描述符机制来实现**类****型限制功能**

```python
class Str:
    def __init__(self, name):
        self.name = name
    def __get__(self, instance, owner):
        print('get--->', instance, owner)
        return instance.__dict__[self.name]

    def __set__(self, instance, value):
        print('set--->', instance, value)
        instance.__dict__[self.name] = value
    def __delete__(self, instance):
        print('delete--->', instance)
        instance.__dict__.pop(self.name)
class People:
    name = Str('name')
    def __init__(self, name, age, salary):
        self.name = name
        self.age = age
        self.salary = salary
p1 = People('nick', 18, 3231.3)
print(p1.__dict__)
print(p1.name)  
#get---> <__main__.People object at 0x107a86198> <class '__main__.People'> nick
p1.name = 'nicklin'
#set---> <__main__.People object at 0x107a86198> nicklin
del p1.name 
delete---> <__main__.People object at 0x107a86198>
```

**限制属性的类型**

```python
class Typed:
    def __init__(self, name, expected_type):
        self.name = name
        self.expected_type = expected_type
    def __get__(self, instance, owner):
        print('get--->', instance, owner)
        if instance is None:
            return self
        return instance.__dict__[self.name]
    def __set__(self, instance, value):
        print('set--->', instance, value)
        if not isinstance(value, self.expected_type):
            raise TypeError('Expected %s' % str(self.expected_type))
        instance.__dict__[self.name] = value
    def __delete__(self, instance):
        print('delete--->', instance)
        instance.__dict__.pop(self.name)
class People:
    name = Typed('name', str)
    age = Typed('name', int)
    salary = Typed('name', float)

    def __init__(self, name, age, salary):
        self.name = name
        self.age = age
        self.salary = salary


try:
    p1 = People(123, 18, 3333.3)
except Exception as e:
    print(e)
p1 = People('nick', 18, 3333.3)    
```

######  类的装饰器

```python
class Typed:
    def __init__(self, name, expected_type):
        self.name = name
        self.expected_type = expected_type
    def __get__(self, instance, owner):
        print('get--->', instance, owner)
        if instance is None:
            return self
        return instance.__dict__[self.name]
    def __set__(self, instance, value):
        print('set--->', instance, value)
        if not isinstance(value, self.expected_type):
            raise TypeError('Expected %s' % str(self.expected_type))
        instance.__dict__[self.name] = value
    def __delete__(self, instance):
        print('delete--->', instance)
        instance.__dict__.pop(self.name)
def typeassert(**kwargs):
    def decorate(cls):
        print('类的装饰器开始运行啦------>', kwargs)
        for name, expected_type in kwargs.items():
            setattr(cls, name, Typed(name, expected_type)) # 反射
        return cls
    return decorate
# 有参：
#1.运行typeassert(...)返回结果是decorate，此时参数都传给kwargs #2.People=decorate(People)
@typeassert(name=str, age=int, salary=float) 
class People:
    def __init__(self, name, age, salary):
        self.name = name
        self.age = age
        self.salary = salary
print(People.__dict__)
p1 = People('nick', 18, 3333.3)
```



- 描述符是可以实现大部分python类特性中的底层魔法，包括@classmethod，@staticmethd，@property甚至是__slots__属性
- 描述符是很多高级库和框架的重要工具之一，描述符通常是使用到装饰器或者元类的大型框架中的一个组件.

https://www.cnblogs.com/nickchen121/p/10990781.html





##### 析构方法

- __del__也称之为析构方法
- __del__会在对象被删除之前自动触发

##### __slots__

slots__是一个类变量，变量值可以是列表，元祖，或者可迭代对象，也可以是一个字符串(意味着所有实例只有一个数据属性)

当你定义__slots__后，__slots__就会为实例使用一种更加紧凑的内部表示。实例通过一个很小的固定大小的数组来构建，而不是为每个实例定义一个字典，这跟元组或列表很类似。在__slots__中列出的属性名在内部被映射到这个数组的指定小标上。使用__slots__一个不好的地方就是我们不能再给实例添加新的属性了，只能使用在__slots__中定义的那些属性名

- 注意：__slots__的很多特性都依赖于普通的基于字典的实现。另外，定义了__slots__后的类不再 支持一些普通类特性了，比如多继承。大多数情况下，你应该只在那些经常被使用到 的用作数据结构的类上定义__slots__比如在程序中需要创建某个类的几百万个实例对象 。
- 关于__slots__的一个常见误区是它可以作为一个封装工具来防止用户给实例增加新的属性。尽管使用__slots__可以达到这样的目的，但是这个并不是它的初衷。它更多的是用来作为一个内存优化工具。

```python
class Foo:
    __slots__='x'


f1=Foo()
f1.x=1
f1.y=2#报错
print(f1.__slots__) #f1不再有__dict__

class Bar:
    __slots__=['x','y']
    
n=Bar()
n.x,n.y=1,2
n.z=3#报错
```

- f1与f2都没有属性字典__dict__了，统一归__slots__管，节省内存

每开辟一个对象，定义一个字典，slots 节省内存

##### __doc__

- 返回类的注释信息

##### __call__

- 对象后面加括号时，触发执行。
- 注：构造方法的执行是由创建对象触发的，即：对象 = 类名() ；而对于 __call__ 方法的执行是由对象后加括号触发的，即：对象() 或者 类()()

##### __new__ __init__

__new__方法的第一个参数是这个类，而其余的参数会在调用成功后全部传递给__init__方法初始化，

__new__方法（第一个执行）先于__init__方法执行：

```python
class A:
    pass
class B(A):
    def __new__(cls):
        print("__new__方法被执行")
        return super().__new__(cls)
    def __init__(self):
        print("__init__方法被执行")
b = B()
```

可以发现__new__方法是传入类(cls)，而__init__方法传入类的实例化对象(self)，而有意思的是，__new__方法返回的值就是一个实例化对象

（ps:如果__new__方法返回None，则__init__方法不会被执行，并且返回值只能调用父类中的__new__方法，而不能调用毫无关系的类的__new__方法）

##### __str__

- 打印时触发

```python
class Foo:
    def __init__(self, name, age):
        """对象实例化的时候自动触发"""
        self.name = name
        self.age = age
    def __str__(self):
        print('打印的时候自动触发，但是其实不需要print即可打印')
        return f'{self.name}:{self.age}'  # 如果不返回字符串类型，则会报错
obj = Foo('nick', 18)
print(obj)  # obj.__str__() # 打印的时候就是在打印返回值
```

##### __repr__

- str函数或者print函数--->obj.__str__()
- repr或者交互式解释器--->obj.__repr__()
- 如果__str__没有被定义，那么就会使用__repr__来代替输出
- 注意：这俩方法的返回值必须是字符串，否则抛出异常

```python
class School:
    def __init__(self, name, addr, type):
        self.name = name
        self.addr = addr
        self.type = type

    def __repr__(self):
        return 'School(%s,%s)' % (self.name, self.addr)

    def __str__(self):
        return '(%s,%s)' % (self.name, self.addr)

s1 = School('oldboy1', '北京', '私立')
print('from repr: ', repr(s1))
print('from str: ', str(s1))
print(s1)
s1  # jupyter属于交互式
```

##### iter next

```python
class Foo:
    def __init__(self, x):
        self.x = x

    def __iter__(self):
        return self

    def __next__(self):
        self.x += 1  # 死循环
        return self.x
f = Foo(3)
for i in f:
    print(i)
```

**加上StopIteration异常** 

```python
class Foo:
    def __init__(self, start, stop):
        self.num = start
        self.stop = stop

    def __iter__(self):
        return self
    def __next__(self):
        if self.num >= self.stop:
            raise StopIteration
        n = self.num
        self.num += 1
        return n
f = Foo(1, 5)
from collections import Iterable, Iterator
print(isinstance(f, Iterator))

```

**模拟range** 

```python
class Range:
    def __init__(self, n, stop, step):
        self.n = n
        self.stop = stop
        self.step = step

    def __next__(self):
        if self.n >= self.stop:
            raise StopIteration
        x = self.n
        self.n += self.step
        return x
    def __iter__(self):
        return self
for i in Range(1, 7, 3):
    print(i)
```

**斐波那契数列**

```python
class Fib:
    def __init__(self):
        self._a = 0
        self._b = 1
    def __iter__(self):
        return self
    def __next__(self):
        self._a, self._b = self._b, self._a + self._b
        return self._a
f1 = Fib()
for i in f1:
    if i > 100:
        break
    print('%s ' % i, end='')
```

##### __module__  class

- __module__ 表示当前操作的对象在那个模块

- __class__表示当前操作的对象的类是什么

```python
print(obj.__class__)  # 即：输出类
print(obj.__module__)  # 即：输出模块
```

##### 上下文管理协议

为了让一个对象兼容with语句，必须在这个对象的类中声明__enter__和__exit__方法

1. 使用with语句的目的就是把代码块放入with中执行，with结束后，自动完成清理工作，无须手动干预
2. 在需要管理一些资源比如文件，网络连接和锁的编程环境中，可以在__exit__中定制自动释放资源的机制，你无须再去关系这个问题，这将大有用处

##### 元类

用class关键字定义的类本身也是一个对象，负责产生该对象的类称之为元类

```python
class Foo:  # Foo=元类()
    pass
```

- 元类是负责产生类的，所以我们学习元类或者自定义元类的目的：是为了控制类的产生过程，还可以控制对象的产生过程

**内置函数exec(储备)**

```python
cmd = """
x=1
print('exec函数运行了')
def func(self):
    pass
"""
class_dic = {}
# 执行cmd中的代码，然后把产生的名字丢入class_dic字典中
exec(cmd, {}, class_dic)
print(class_dic)
```

**class 创建类**

- 如果说类也是对象，那么用class关键字的去创建类的过程也是一个实例化的过程，该实例化的目的是为了得到一个类，调用的是元类
- 用class关键字创建一个类，用的默认的元类type，因此以前说不要用type作为类别判断

```python
class People:  # People=type(...)
    country = 'China'

    def __init__(self, name, age):
        self.name = name
        self.age = age

    def eat(self):
        print('%s is eating' % self.name)
print(type(People))

```

###### type实现

- 创建类的3个要素：类名，基类，类的名称空间
- People = type(类名，基类，类的名称空间)

```python
class_name = 'People'  # 类名
class_bases = (object, )  # 基类
# 类的名称空间
class_dic = {}
class_body = """
country='China'
def __init__(self,name,age):
    self.name=name
    self.age=age
def eat(self):
    print('%s is eating' %self.name)
"""
exec(
    class_body,
    {},
    class_dic,
)
print(class_name)
print(class_bases)
print(class_dic)  # 类的名称空间
obj1 = People1(1, 2)
```

###### 自定义元类控制类的创建

```python
class Mymeta(type):
    def __call__(self, *args, **kwargs):
        print(self)  # self是People
        print(args)  # args = ('nick',)
        print(kwargs)  # kwargs = {'age':18}
        # return 123
        # 1. 先造出一个People的空对象，申请内存空间
        # __new__方法接受的参数虽然也是和__init__一样，但__init__是在类实例创建之后调用，而 __new__方法正是创建这个类实例的方法。
        obj = self.__new__(self)  # 虽然和下面同样是People，但是People没有，找到的__new__是父类的
        # 2. 为该对空对象初始化独有的属性
        self.__init__(obj, *args, **kwargs)
        # 3. 返回一个初始化好的对象
        return obj
```



#### dataclass

```python
@dataclass(frozen=True)  # 这个参数在对象初始化后，会禁止更改值 ,自动初始化
class Data:
    name: Any
    value: Any = 42
	# 引入field后， 改动下面这行，使用默认工厂函数来初始化默认值
    my_list: List[int] = field(default_factory=list) # 定义空列表
https://zhuanlan.zhihu.com/p/419778289
    
    
```





### 线程 和进程

多任务：

并发： cpu 轮循
并行  同时执行 多核
进程： process 是分配资源的最小单位，它是操作系统进行资源分配和调度运行的基本单位
线程： 是计算机可以被调度cpu最小的单位
一个进程中的多个线程可以共享该进程的资源
多进程 完成多任务
GIL Global interpreter lock cpython
GIL 锁是cpython 解释器特有的一个全局解释器锁， 控制一个进程中同一时刻只有一个线程可以被cpu调度
列表，字典 常见对象的是线程安全的 
计算密集型： 多进程
IO密集型 ： 多线程 文件

###  设计模型

~~~markdown
封装  继承 多态  接口 
接口：  继承类要实现接口方法
from abc import ABCMeta , abstractmethod
@abstractmethod
继承抽象类， 必须实现抽取方法 


solid 原则
开放封闭式原则：
里氏替换原则：
依赖倒置原则：
接口隔离原则
单一职责原则
设计模型分类：
创建型模式5种： 
工厂模型 ，抽象工厂， 创建者模式 ，原型模式， 单例模式
结构模式7种：
适配器模式，桥模式， 组合模式， 装饰模式， 外观模式， 享元模式， 代理模式
行为型模式： 11 种
解释器模式，责任链模式，命令模式， 迭代器模式， 中介模式，备忘录模式，观察者模式，状态模式，策略模式，访问者模式， 模板方法模式，

1简单工厂模式
不能直接向客服端暴露对象创建的实现细节，而是通过一个工厂类来负责创建产品的实例 
角色：
工厂角色  creator
抽象产品角色 product
具体产品角色 Concreate Prodext

2工厂方法模式
抽象工厂角色 Creator
具体工厂角色  Concrete creator
抽象产品角色 product
具体产品角色 Concreate Prodext
优点： 
1：每个具体产品都对应一个具体工厂类， 不需要修改工厂类代码
2： 隐藏了对象创建的实现细节
缺点： 每增加一个具体产品类，就必须增加一个相应的具体工厂类

3: 抽象工厂模式
内容： 定义一个工类接口， 让工厂子类来创建一系列相关或者相互依赖的对象
相比工厂方法模式，抽象工厂模式中的每个具体的工厂都生产一套产品
抽象工厂角色 Creator
具体工厂角色  Concrete creator
抽象产品角色 product
具体产品角色 Concreate Prodext
客服端 
每个工厂创建一套完整的产品系列 ，使得易于交换产品系列
有利于产品的一致性
缺点： 难以支持新种类 抽象产品 

4： 建造者模式 
内容：  将一个复杂的对象构建与它的表示分离 ， 使得同样的构建过程可以创建不同的表示
角色： 
抽象创建者  builder
具体的建造者 concrete builder
指挥者 Direcotr
产品 Product
优点： 隐藏了一个产品的内部结构和装配过程
将构造代码与表示代码分开
可以对构造过程进行更精细的控制


5：单例模式 Single
1：类创建对象 ，在内存中只有唯一个实例
2： 每一次实例化生成对象， 内存地址是相同的

优点： 
对唯一实例受控访问
单例相当于全局变量， 但防止了命名空间被污染


__new__ :
是object 基类提供的内置静态方法
在内存中为对象分配空间
返回对象的引用
python 解释器在获得对象引用后， 将引用作为第一个出参传递给__init__方法

~~~

#### 创建型模式5种

##### 简单工厂模式

~~~python
from abc import ABCMeta ,abstractmethod
class Payment(metaclass = ABCMeta):   #抽象产品角色 product
    @abstractmethod
    def pay(self,money):
        pass
class AliPay(Payment):  #具体产品角色 Concreate Prodext
    def pay(self, money):
        print('alipay')
class WechatPay(Payment): #具体产品角色 Concreate Prodext
    def pay(self, money):
        print('wechatpay')    
class PaymentFactory:  # 简单工厂模式   工厂角色  creator
    def creat_payment(self,method):
        if method == 'alipay':
            return Alipau()
        elif method =='wechat':
            return WechatPay()
        else :
            raise TypeError('')  
~~~

##### 工厂方法模式

~~~python
from abc import ABCMeta ,abstractmethod
class Payment(metaclass = ABCMeta):   #抽象产品角色 product
    @abstractmethod
    def pay(self,money):
        pass
class AliPay(Payment):  #具体产品角色 Concreate Prodext
    def pay(self, money):
        print('alipay')
class WechatPay(Payment): #具体产品角色 Concreate Prodext
    def pay(self, money):
        print('wechatpay')    
class PaymentFactory(metaclass=ABCMeta):  #  抽象工厂角色 Creator
    @abstractmethod
    def creat_payment(self,method):
        pass
class AlipayFactory(PaymentFactory): #具体工厂角色  Concrete creator
    def creat_payment(self):
        return AliPay()
class WechatPayFactory(PaymentFactory):
    def creat_payment(self):
        return WechatPay()

~~~

##### 抽象工厂模式

~~~python
#  生产一部手机 ，  手机壳， CPU， OS 进制组装 ，其中每个类对象都有不同的种类
# 相比工厂方法模式， 抽象工厂模式中每个具体的工厂都生产一套产品
from abc import ABCMeta ,abstractmethod
# 抽象产品
class CPU(metaclass = ABCMeta):
    @abstractmethod
    def show_cpu(self):
        pass
class OS(metaclass = ABCMeta):
    @abstractmethod
    def show_os(self):
        pass
# 抽象工厂
class PhoneFactory(metaclass = ABCMeta):
    @abstractmethod
    def make_cpu(self):
        pass
    @abstractmethod
    def make_os(self):
        pass
# 具体产品
class SnapDragonCpu(CPU):
    def show(self):
        print('snapDragon cpu')
class AppleCpu(CPU):
    def show(self):
        print('AppleCpu cpu')
class Anroid(OS):
    def show_os(self):
        print('android os')
class IOS(OS):
    def show_os(self):
        print('IOS os') 
# 具体工厂   多个工厂 
class MiFacotry(PhoneFactory):  
    def make_cpu(self):
        return SnapDragonCpu()
    def make_os(self):
        return Anroid()
class HuaweiFactory(PhoneFactory):
   	def make_cpu(self):
        return SnapDragonCpu()
    def make_os(self):
        return Anroid() 
# 客服端 
class Phone:
    def __init__(self, cpu,os):
        self.cpu = cpu
        self.os = os
~~~

##### 建造者模式 

~~~python
from abc ABCMEta ,abstractmethod
class Player:
    def __init__(self, face = None , body=None ,arm =None ,leg =NOne):
        self.face =face 
        self.body = body
        self.arm = arm
        self.leg = leg
     def __str__(self):
        return "%s ,%s,%s,%s,"%(self.face,...)
class PlayBuilder(metaclass = ABcMeta):
    @abstractmethod
    def build_face(self):
        pass
    @abstractmethod
    def build_body(self):
        pass
class SexyGirlBuilder(PlayBuilder):
    def __init__(self):
        self.player =Player()
    def build_face(self):
        self.player.face = 'beautiful face'
    def build_body(self):
        self.player.body = '苗条'
        
class MonsterPlayer(PlayBuilder):
    def __init__(self):
        self.player =Player()
    def build_face(self):
        self.player.face = 'monster face'
    def build_body(self):
        self.player.body = '苗条'  
class PlayerDirector:  # 控制组装顺序
    def build_player(self,builder):
        builder.build_body()
        builder.build_face()
        return builder.player()
#client
builder = SexyGirlBuilder()
director = PlayerDirector()
p = director.build_player(builder)
~~~

##### 单例模式

~~~python
class Singleton:
    def __init(self):
        pass 
    def __new__(cls ,*arg ,**kwargs):
        if not hasattr(cls,'_instance'):
            cls._isntance = super(Singleton,cls).__new__(cls) # 调用父类创建
         return cls._instance
class Myclass(Singleton):
    def __init__(self,a):
        self.a = a 
a = Myclass(10)
b = Myclass(20)
~~~

#### 结构型模式

##### 适配器模式

复用代码： 继承 和组合

内容： 将一个类的接口转换成客户希望的另一个接口，适配器模式使得原本由于接口不兼容而不能在一起工作的那些类可以一起工作

两种实现方式：

类适配器： 使用多继承

对象适配器： 使用组合

角色： 目标接口 target

待适配的类： Adaptee

适配器： Adapter

适用场景： 想要用一个类，但是它的接口不符合你的要求

~~~~python
from abc import ABCMeta, abstractmethod
class Payment(metaclass=ABCMeta):
    @abstractmethod
    def pay(self,money):
        pass
class Alipay(Payment):
    def pay(self,money):
        print('aliapy %d'%money)
class WechatPay(Payment):
    def pay(self,money):
        print('WechatPay %d' % money)
class BankPay:
    def cost(self,money):
        print('bankpay %d'%money)
# 适配器类
class NewBankPay(Payment,BankPay):
    def pay(self,money):
        self.cost(money)

pay = Alipay()
pay.pay(100)
pay = BankPay()
# pay.pay()
pay = NewBankPay()
pay.pay(100) # 接口统一
#对象适配器 
class PaymentAdapter(Payment):
    def __init__(self,payment):
        self.payment = payment
    def pay(self,money):
        self.payment.cost(money)
pay = PaymentAdapter(BankPay())
pay.pay(100)

~~~~

##### 桥模式：

内容： 将一个事物的两个维度分离，使其都可以独立的变化

角色： 抽象 abstraction

细化抽象： refinedabstraction

实现者： Implementor

具体实现者： ConcreteImplementor 

应用场景： 当事物有两个维度上的表现，两个维度都可以扩展

优点： 抽象和实现的分离，优秀的扩展能力 

~~~python
from abc import  ABCMeta,abstractmethod

# 将形状 和颜色 松耦合 ， 可以方便后面进行扩展 
class Shape(metaclass=ABCMeta):
    def __init__(self,color):
        self.color = color
    @abstractmethod
    def draw(self):
        pass
class Color(metaclass=ABCMeta):
    @abstractmethod
    def paint(self,shape):
        pass
class Rectangle(Shape):
    name ='rectangle'
    def draw(self):
        self.color.paint(self)
class Cicle(Shape):
    name = 'cicle'
    def draw(self):
        self.color.paint(self)

class Red(Color):
    def paint(self,shape):
        print('red %s'%shape.name)

class Green(Color):
    def paint(self,shape):
        print('green %s'%shape.name)

shape = Rectangle(Red())
shape.draw()
shape2 = Cicle(Green())
shape2.draw()
~~~

##### 组合模式

内容： 将对象组合成树形结构，以表示 部分-整体的层次结构，组合模式使得用户对单个对象和组合对象的使用具有一执性。

角色： 

抽象组件 Componet

叶子组件： Leaf

复合组件： Composite

客服端： Client

适用场景： 表示对象部分-整体的层次结构  特别是结构是递归的

用户希望忽略组合对象与单个对象的不同，用户可以统一的使用组合结构中的所有对象

优点： 定义了包含基本对象和组合对象的类层次结构

简化客服端代码，客服端可以统一的使用组合对象和单个对象

更容易增加新类型的组件

~~~python
from abc import ABCMeta,abstractmethod

#抽象组件
class Graphic(metaclass=ABCMeta):
    @abstractmethod
    def draw(self):
        pass
# 叶子组件
class Point(Graphic):
    def __init__(self,x,y):
        self.x = x
        self.y = y
    def __str__(self):
        return "点(%s,%s)"%(self.x,self.y)
    def draw(self):
        print(str(self))
# 叶子组件
class Line(Graphic):
    def __init__(self,p1,p2):
        self.p1 = p1
        self.p2 = p2
    def __str__(self):
        return "Line (%s,%s)" % (self.p1, self.p2)
    def draw(self):
        print(str(self))
# 复合组件 
class Picture(Graphic):
    def __init__(self,iterable):
        self.children = []
        for g in iterable:
            self.add(g)
    def add(self,graphic):
        self.children.append(graphic)
    def draw(self):
        print('复合图形')
        for g in self.children:
            g.draw()
        print('复合图形')


p1 = Point(1,2)
l1 = Line(Point(1,1),Point(2,2))
print(l1)
l2 = Line(Point(3,3),Point(4,4))
pic1 = Picture([p1,l1,l2])
pic1.draw()

p2 = Point(22,2)
l3 = Line(Point(4,4),Point(2,2))
print(l1)
l4 = Line(Point(5,5),Point(4,4))
pic2 = Picture([p2,l3,l4])

pic = Picture([pic1,pic2])
pic.draw()

~~~

##### 外观模式

内容： 为子系统中的一组接口提供一个一致的界面，外观模式定义了一个高层接口，这个接口使得这一子系统更加容易使用

角色： 外观facade

子系统： subsystem classees

优点： 减少系统相互依赖，提高了灵活性，提高了安全性

~~~python
# 子系统
class CPU:
    def run(self):
        print('cpu run')
    def stop(self):
        print('cpu stop')
class Disk:
    def run(self):
        print('Disk run')
    def stop(self):
        print('Disk stop')
class Memory:
    def run(self):
        print('Memory run')
    def stop(self):
        print('Memory stop')

 # 外观 facade
class Computer:
    def __init__(self):
        self.cpu = CPU()
        self.disk = Disk()
        self.memory = Memory()
    def run(self):
        self.cpu.run()
        self.disk.run()
        self.memory.run()
    def stop(self):
        self.cpu.stop()
        self.memory.stop()
        self.disk.stop()

#client
computer = Computer()
computer.run()
computer.stop()
~~~



###  python 配置文件 setup



##### 编码规划

~~~markdown
如无特殊情况, 文件一律使用 UTF-8 编码
如无特殊情况, 文件头部必须加入#--coding:utf-8--标识

常量: 命名应使用大写字母加下划线的方式
变量: 统一使用下划线命名或驼峰命名，但不可混用
user_info_list 
userInfoList

模块级函数和类定义之间空两行；
类成员函数之间空一行；

from 引擎模块 import *
from python模块 import *
from 自定模块 import *

import 引擎模块
import python模块
import 自定模块

类名使用驼峰(CamelCase)命名风格，首字母大写，私有类可用一个下划线开头
函数名小驼峰命名法,私有函数在函数前加一个下划线_


包：小写 小写，多个单词用下划线分割
规范的命名包会有个小白点，不规范的就是普通文件夹


xx: 公有变量
_x: 单前置下划线,私有化属性或方法，from somemodule import *禁止导入,类对象和子类可以访问
__xx：双前置下划线,避免与子类中的属性命名冲突，无法在外部直接访问(名字重整所以访问不到)
_xx_:双前后下划线,用户名字空间的魔法对象或属性。例如:_init_ , __ 不要自己发明这样的名字
xx_:单后置下划线,用于避免与Python关键词的冲突
~~~

### 软件开发的目录规范

```python
ATM/
|-- core/
|   |-- src.py  # 业务核心逻辑代码
|
|-- api/
|   |-- api.py  # 接口文件
|
|-- db/
|   |-- db_handle.py  # 操作数据文件
|   |-- db.txt  # 存储数据文件
|
|-- lib/
|   |-- common.py  # 共享功能
|
|-- conf/
|   |-- settings.py  # 配置相关
|
|-- bin/
|   |-- run.py  # 程序的启动文件，一般放在项目的根目录下，因为在运行时会默认将运行文件所在的文件夹作为sys.path的第一个路径，这样就省去了处理环境变量的步骤
|
|-- log/
|   |-- log.log  # 日志文件
|
|-- requirements.txt # 存放软件依赖的外部Python包列表，详见https://pip.readthedocs.io/en/1.1/requirements.html
|-- README  # 项目说明文件
```

~~~python
import os
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
~~~

## 规划化布置一个项目

~~~
《Fluent Python》或者《Effective Python》
《Robust Python》

~~~

Pythonic：code style

喜欢是放肆，但爱是克制 ,写脚本可以为所欲为，但做项目就要按规矩来

https://pythonmana.com/2022/121/202205011311087768.html

http://www.cosmicpython.com/book/part1.html



### 配置文件：

1. 将配置信息写在python 文件中 

   ~~~python
   不推荐
   config.py  写成字典的形式
   """ 
   @file: raw_configs.py 
   """
   # mysql 配置
   DATABASE_CONFIG = {
       'host': 'localhost',
       'dbname': 'test',
       'user': 'user',
       'password': 'password',
       'port': 3306
   }
   # 读取配置信息
   import raw_configs as config
   assert config.DATABASE_CONFIG['host'] == 'localhost'
   assert config.DATABASE_CONFIG['user'] == 'user'
   assert config.DATABASE_CONFIG['password'] == 'password'
   assert config.DATABASE_CONFIG['dbname'] == 'test'
   
   
   ~~~

   2. 利用外部配置文件

   ini yaml json 

   ~~~python
   # config.ini
   [DATABASE]
   HOST = 'localhost'
   DBNAME = 'test'
   USER = 'user'
   PASSWORD = 'password'
   PORT = 3306
   # config.json
   {
     "DATABASE": {
       "host": "localhost",
       "dbname": "test",
       "user": "user",
       "password": "password",
       "port": 3306
     }
   }
   ~~~

   ~~~
   yaml文件规则
   
   区分大小写；
   使用缩进表示层级关系；
   使用空格键缩进，而非Tab键缩进
   缩进的空格数目不固定，只需要相同层级的元素左侧对齐；
   文件中的字符串不需要使用引号标注，但若字符串包含有特殊字符则需用引号标注；
   注释标识为#
   ~~~

   3. Dynaconf：Pyhton项目的动态配置

   

```
import argparse
parser = argparse.ArgumentParser()

parser.add_argument('--model_name', type=str, help='model_name[bert_bilstm_crf, bert_crf, bert_ce]', required=False,default=model)

parser.add_argument('--device', type=int, help='which gpu to use',
                                required=False, default=-1)
args = parser.parse_args()
```



python setup.py build 

python setup.py install



## 小技巧：

Python将二维数组/多维数组转换为一维

~~~Python

import numpy as np
mulArrays = [[1,2,3],[4,5,6],[7,8,9]]
print(list(np.array(mulArrays).flatten()))


mulArrays = [[1,2,3],[4,5,6],[7,8,9]]
print(list(np.concatenate(array.reshape((-1,1),order="F"))))

mulArrays = [[1,2,3],[4,5,6],[7,8,9]]
print(sum(mulArrays,[])) #[1, 2, 3, 4, 5, 6, 7, 8, 9]

mulArrays = [[1,2,3],[4,5,6],[7,8,9]]
print([i for arr in mulArrays for i in arr]) #[1, 2, 3, 4, 5, 6, 7, 8, 9]

import operator
from functools import reduce
mulArrays = [[1,2,3],[4,5,6],[7,8,9]]
print(reduce(operator.add, mulArrays))

from itertools import chain
mulArrays = [[1,2,3],[4,5,6],[7,8,9]]
print(list(chain.from_iterable(mulArrays))) #[1, 2, 3, 4, 5, 6, 7, 8, 9]
~~~

文件按照修改时间排序 

~~~python

# 注意，这里使用lambda表达式，将文件按照最后修改时间顺序升序排列
# os.path.getmtime() 函数是获取文件最后修改时间
# os.path.getctime() 函数是获取文件最后创建时间

 file_list = os.listdir(save_path)
    if not file_list:
        return
    else :
        file_list = sorted(file_list,key=lambda  x: os.path.getctime(os.path.join(save_path,x)),reverse=True)
~~~





## 2: numpy



~~~python
增加维度 np.newaxis
降维 np.ravel()
~~~









## 3 pandas

### 去重

~~~python
# 去除重复值
df.duplicated()
df.drop_duplicates()
data.drop(["Cabin","Name","Ticket"],inplace=True,axis=1)
data["Age"] = data["Age"].fillna(data["Age"].mean())
data = data.dropna()
data["Embarked"] = data["Embarked"].apply(lambda x: labels.index(x))
drop_duplicates(inplace = True) # 删除以后，要恢复索引
df.melt()

# 缺失值按均值填充
for col in list(df.columns[df.isnull().sum() > 0]):
    mean_val = df[col].mean()
    df[col].fillna(mean_val, inplace=True)
  # 删除不分析的列
columns =["col_name"]
df.drop(columns,axis=1,inplace=True)

~~~

~~~markdown
1: describe()方法来对表格中的数据做一个概括性的统计分析，
df.describe(percentiles=[0.05, 0.25, 0.75, 0.95]) # 分位数
要是表格中既包含了离散型数据，也包含了连续型的数据，默认的话，
describe()是会针对连续型数据进行统计分析
df2.describe(include=["object"]) 指定让其强制统计分析离散型数据或者连续型数据
include = ['object','number','all']
2: idxmin()和idxmax()方法是用来查找表格当中最大/最小值的位置，返回的是值的索引

3: value_counts()方法主要用于数据表的计数以及排序 ,同时里面也还可以利用参数normalize=True，来计算不同值的计数占比
df['col_name'].value_counts(ascending=True,normalize=True)
4: 数据分组
cut()方法以及qcut()方法来对表格中的连续型数据分组
ages = np.array([2,3,10,40,36,45,58,62,85,89,95,18,20,25,35,32])
pd.cut(ages, 5)  pd.cut(ages, 5, labels=[u"婴儿",u"少年",u"青年",u"中年",u"老年"])
pd.qcut(ages, [0,0.5,1], labels=['小朋友','大孩子'])

5:引用函数
pipe()
首先我们来看pipe()这个方法，我们可以将自己定义好的函数，以链路的形式一个接着一个传给我们要处理的数据集上
apply()和applymap()
agg()和transform()

apply()方法可以对表格中的数据按照行或者是列方向进行处理，
apply()方法和applymap()方法
df.apply(np.mean,axis=1)
df.apply(lambda x: x.max() - x.min(),axis=1)

agg()方法和transform()方法
agg()方法本意上是聚合函数，我们可以将用于统计分析的一系列方法都放置其中，并且放置多个

df.agg(np.sum)
当然，当中的np.sum部分也可以用字符串来表示，例如
df.agg("sum")
df.agg(["sum", "mean", "median"])
df.agg(["sum", lambda x: x.mean()])
与此同时，我们在agg()方法中添加字典，实现不同的列使用不同的函数方法
df.agg({"A": "sum", "B": "mean"})
df.agg({"A": ["sum", "min"], "B": "mean"})

~~~

~~~python
def extract_city_name(df):
    df["state_name"] = df["state_and_code"].str.split(",").str.get(0)
    return df
def add_country_name(df, country_name=None):
    df["state_and_country"] = df["state_name"] + country_name
    return df
df_p = pd.DataFrame({"city_and_code": ["Arizona, AZ"]})
df_p = pd.DataFrame({"state_and_code": ["Arizona, AZ"]})
df_p.pipe(extract_city_name).pipe(add_country_name, country_name="_USA")

def normalize(x):
    return (x - x.mean()) / x.std()
df.apply(normalize)

#apply()方法作用于数据集当中的每个行或者是列，而applymap()方法则是对数据集当中的所有元素都进行处理
def add_A(x):
    return "A" + str(x)
df = pd.DataFrame({'key1' : ['a', 'c', 'b', 'b', 'd'],
                   'key2' : ['one', 'two', 'three', 'two', 'one'],
                   'data1' : np.arange(1, 6),
                   'data2' : np.arange(10,15)})
df.applymap(add_A).applymap(lambda x: x.split("A")[1])        

~~~

### 索引和列名的重命名 排序

~~~python
1: 重命名
df1 = pd.DataFrame(np.random.randn(5, 3), columns=["A", "B", "C"],
                   index = ["a", "b", "c", "d", "e"])
                   
df1.rename(columns={"A": "one", "B": "two", "C": "three"},
                 index={"a": "apple", "b": "banana", "c": "cat"})
                 
df1.rename({"A": "one", "B": "two", "C": "three"}, axis = "columns")
df1.rename({"a": "apple", "b": "banana", "c": "cat"}, axis = "index")对行的重命名则可以这么来做

2： 排序
df1.sort_values(by = "col_name1")
df1.sort_values(by = ["col_name1","col_name2",...],ascending=False)
3: 数据转换
最后涉及到的是数据类型的转换，在这之前，我们先得知道如何来查看数据的类型
通过dtypes属性来查看数据的类型
而通过astype()方法来实现数据类型的转换
df2["B"].astype("int64")


也可以根据相对应的数据类型来进行筛选，运用pandas当中的
select_dtypes方法，我们先来创建一个数据集包含了各种数据类型的
df = pd.DataFrame(
    {
        "string_1": list("abcde"),
        "int64_1": list(range(1, 6)),
        "uint8_1": np.arange(3, 8).astype("u1"),
        "float64_1": np.arange(4.0, 9.0),
        "bool1": [True, False, True, True, False],
        "bool2": [False, True, False, False, True],
        "dates_1": pd.date_range("now", periods=5),
        "category_1": pd.Series(list("ABCDE")).astype("category"),
    }
)
df.dtypes
df.select_dtypes(include=[bool])
df.select_dtypes(include=['int64'])
~~~

~~~
# dict  将一个dict 转换成df  ,按照key ， value 长度可以不一样
df_label =  pd.DataFrame(dict([(k, pd.Series(v)) for k, v in dict.items()]))
df_laebl.index = [id]*len(df_label)
~~~



## 4 画图

## 5 linux

~~~
 lshw -c video  # interl 显卡 
 lspci  | grep -i vga
 lspci -v -s 
    nvidia-smi#
 chmod +x preprocess.sh
 ps aux | grep wang*
ps ef
ps -ef | grep kafaka
top -p port
 
 ps aux | sort -k4,4nr | head -n 10 
~~~



~~~
https://www.cnblogs.com/yjd_hycf_space/p/7730690.html
在 Linux 系统中，~ 代表的是用户的主文件夹，即 “/home/用户名” 这个目录
cd
cat -n file_name
tac 不能带行号输出
ls -la

grep
find
cut 提取第几列 
cut -f 1 -d:

cp
mv
rm
ps aux
kill 
pwd
chmod
touch test1
mkdir
echo 用来打印变量、文本内容
# 2文件类操作 
head   tail  more less
head -n15 test.txt  #展示15行
head 用来显示档案的开头至标准输出中，默认 head 命令打印其相应文件的开头 10 

wc 
-c 统计字节数
-l 统计行数
-m 统计字符数
-w 统计词数，一个字被定义为由空白、跳格或换行字符分隔的字符串
date
cal命令可以用来显示公历
which  使用 which 命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。
which ls 
whereis 
whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。

nl   nl命令在linux系统中用来计算文件中行号。nl 可以将输出的文件内容自动的加上行号！

diff  比较两个文件的不同

tar
tar -cvf test.tar test.txt   #打包  tar -cvf 包名  文件名
$tar -xvf test.tar  #解包  tar -xvf 包名 
$tar -zcvf test.tgz test.txt  #压缩  tar -czvf 包名 文件名
$tar -zxvf test.tgz   #解压  tar -xzvf 包名

gzip
unzip

#3进程管理类（工作常用）
ps -a  #查看所有进程
ps -ef  #查看进程的环境变量和程序间的关系
ps -aux | grep kafka  #与grep联用查找某进程
ps -ef | grep kafka #先使用ps查找进程kafka，

df
显示磁盘空间使用情况。获取硬盘被占用了多少空间，目前还剩下多少空间等信息，
df -h

du
令也是查看使用空间的，但是与 df 命令不同的是 : du 命令是对文件和目录磁盘使用的空间的查看.
top
显示当前系统正在执行的进程的相关信息，包括进程 ID、内存占用率、CPU 占用率等

telnet命令用于远端登入。

netstat 命令用于显示网络状态
netstat -a  #列出所有端口使用情况
netstat -nu  #显示当前UDP连接状况
netstat -apu  #显示UDP端口号的使用情况
netstat -i  #显示网卡列表

ifconfig
hostname
hostname -i   #查看主机ip

wget
curl

#4 特殊字符类
| 管道 (pipeline) ,连结上个指令的标准输出，做为下个指令的标准输入
[] 
> 
|| 代表 or 逻辑的符号
？ 匹配一个任意的字符
& 单一个& 符号，且放在完整指令列的最后端，即表示将该指令列放入后台中工作。

#5系统信息
arch 显示机器的处理器架构
uname -m 显示机器的处理器架构
uname -r 显示正在使用的内核版本 
dmidecode -q 显示硬件系统部件 - (SMBIOS / DMI) 
hdparm -i /dev/hda 罗列一个磁盘的架构特性 
hdparm -tT /dev/sda 在磁盘上执行测试性读取操作 
cat /proc/cpuinfo 显示CPU info的信息 
cat /proc/interrupts 显示中断 
cat /proc/meminfo 校验内存使用 
cat /proc/swaps 显示哪些swap被使用 
cat /proc/version 显示内核的版本 
cat /proc/net/dev 显示网络适配器及统计 
cat /proc/mounts 显示已加载的文件系统 
lspci -tv 罗列 PCI 设备 
lsusb -tv 显示 USB 设备 
date 显示系统日期 
cal 2007 显示2007年的日历表 
date 041217002007.00 设置日期和时间 - 月日时分年.秒 
clock -w 将时间修改保存到 BIOS 


~~~



~~~shell
# 关闭防火墙
systemctl stop firewalld.service 
# 查看安装
rpm -qa | grep ssh  
# 统计字数
wc -lwl filename
ls -sh filename  # 查看某个文件的大小
wc -l |grep
~~~

##### 查看命令

~~~shell
du -h --max-depth=1
free -g  查看系统资源
ps auxw 显示系统资源占用情况
ps auxw|sort -rn -k4|head -5 
PID ：进程ID
USER ：用户名
PR ：优先级
NI ：负值表示高优先级，正值表示低优先级。
VIRT ：虚拟内存
RES ： 真实内存
SHR ：共享内存
S ：进程状态 D=不可中断的睡眠状态； R=运行； S=睡眠 ；T=跟踪/停止； Z=僵尸进程
ps -ef | grep nginx
kill -9  id 

查看文件的前几行和后几行 
tail -n 10 vocab.txt
head -n 10 file_path
sed -n '5, 10p' filename   查看 9 到10行

统计当前目录下文件的个数（不包括目录）
ls -l | grep "^-" | wc -l
统计当前目录下文件的个数（包括子目录）
ls -lR| grep "^-" | wc -l
ls -lR | grep "^d" | wc -l查看某目录下文件夹(目录)的个数（包括子目录）
find -name "*.js" | wc -l  例如这里需要找 js 文件的数量：
wc -l
这个命令可以跨服务器docker传输文件。 
scp -P 映射出来的22端口 文件名  root@192.168.150.16:目标文件夹  
例如：scp -P 10055 model_best.pth root@192.168.150.16:/code-online/attorney_v3/resources/models/pick_content_extraction
~~~

##### 文件上传和下载

~~~shell
yum install lrzsz 
sz 下载
rz 上传

~~~

终端：

~~~
screen -S windowName
ctrl+a+d 回到主screen（让某个screen后台运行）
screen -ls
screen -r id  回到某个后台运行的screen
screen -d id  让某个screen离线
screen -S screen_id -X quit
~~~



##### vi

~~~shell
gg  调到一句话的前面 
G 最后一行 
gg           ： 跳转到文件头
Shift+g   ： 跳转到文件末尾
行数+gg ： 跳转到指定行，例跳转到123行：123gg

~~~



## 6 docker k8s

https://blog.csdn.net/huangjhai/article/details/118854733

1： 基本操作

~~~
docker container --help
docker rm container_id 
docker histroy image_name #列出镜像地的变更历史
CMD：Dockerﬁle 中可以有多个CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换！
ENTRYPOINT： docker run 之后的参数会被当做参数传递给 ENTRYPOINT，之后形成新的命令组合！

docker exec -it tomcat01 ip addr # 在启动的docker 里面执行命令
~~~





1： docker install 

~~~
查看系统的内核：
uname -r
下载需要的安装包
yum install -y yum-utils
卸载旧的版本
yum remove docker  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-engine
设置镜像的仓库                  
yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo  
下载需要的安装包
yum makecache fast
docker-ce 是社区版，docker-ee 企业版
yum install docker-ce docker-ce-cli containerd.io
启动Docker
systemctl start docker
# 查看当前版本号，是否启动成功
docker version
# 设置开机自启动
systemctl enable docker
配置阿里云镜像加速
docker images
# 1. 卸载依赖
yum remove docker-ce docker-ce-cli containerd.io
# 2. 删除资源  . /var/lib/docker是docker的默认工作路径
rm -rf /var/lib/docker

依次执行官方的这四条命令

docker version          #查看docker的版本信息
docker info             #查看docker的系统信息,包括镜像和容器的数量
docker 命令 --help       #帮助命令(可查看可选的参数)
docker COMMAND --help

docker search 搜索镜像 eg: docker search --filter=STARTs=300 收藏数
docker pull 镜像名[:tag] 下载镜像
docker pull mysql:5.7
docker rmi 删除镜像
docker rmi -f  $(docker images -aq) 删除全部的镜像id
docker rm -f $(docker ps -aq) # 删除所有的容器
如拉取一个centos镜像
docker pull centos
docker run [可选参数] image

#参数说明
--name="名字"           指定容器名字
-d                     后台方式运行
-it                    使用交互方式运行,进入容器查看内容
-p                     指定容器的端口
(-p ip:主机端口:容器端口  配置主机端口映射到容器端口
-p 主机端口:容器端口
-p 容器端口
)
-P      

退出容器命令：
exit 停止并退出容器（后台方式运行则仅退出）
#Ctrl+P+Q  不停止容器退出
docker ps 

#docker ps 
     # 列出当前正在运行的容器
-a   # 列出所有容器的运行记录
-n=? # 显示最近创建的n个容器
-q   # 只显示容器的编号

docker rm 容器id                 #删除指定的容器,不能删除正在运行的容器,强制删除使用 rm -f
docker rm -f $(docker ps -aq)   #删除所有的容器
docker ps -a -q|xargs docker rm #删除所有的容器

docker start 容器id          #启动容器
docker restart 容器id        #重启容器
docker stop 容器id           #停止当前运行的容器
docker kill 容器id           #强制停止当前容器


docker ps // 查看所有正在运行容器 
$ docker stop containerId // containerId 是容器的ID 
$ docker ps -a // 查看所有容器 $ docker ps -a -q // 查看所有容器ID 
$ docker stop $(docker ps -a -q) //  stop停止所有容器 
$ docker rm $(docker ps -a -q) //   remove删除所有容器

日志的查看
docker logs --help
常用：
docker logs -tf 容器id
docker logs --tail number 容器id #num为要显示的日志条数

 查看容器中进程信息
 docker top c703b5b1911f
 docker inspect 容器id
 docker exec -it c703b5b1911f /bin/bash
 docker attach c703b5b1911f
   docker exec 进入容器后开启一个新的终端，可以在里面操作
   docker attach 进入容器正在执行的终端，不会启动新的进程
 
 #拷贝容器的文件到主机中
docker cp 容器id:容器内路径  目的主机路径

docker run -d -p 8088:9000 --restart=always -v /var/run/docker.sock:/var/run/docker.sock --privileged=true portainer/portainer

# 提交修改后的镜像 ， 生成一份新的
docker commit -a='author' -m='info' id new_name:[tag]

# 在主机中查看容器info 
docker inspect 容器id
# 容器数据卷 将容器里面的数据挂载到本地磁盘 -v 
 -v 主机目录：容器目录
 
 -p 可以多个
docker run -it -v /home/docker:/home centos /bin/bash 

# mysql 数据持久化
docker run  -d -p 3310:3306 -v /home/mysql/conf:/etc/mysql/conf.d -v /home/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql mysql:5.7


具名和匿名挂载 
匿名挂载：  docker volume --help -v 只写了容器里面的路径
docker run -d -P(随机映射) --name nginx01 -v /etc/nginx nginx 
~~~

![image-20220820232211992](pic/image-20220820232211992.png)

![image-20220820232328111](pic/image-20220820232328111.png)

![image-20220820232536859](pic/image-20220820232536859.png)

### 多个容器共享数据





### 2 制作自己的镜像

~~~
docker build -f /root/myfile -t mycentos:01 . # .表示当前路径 

# docker file
FROM centos
MAINTAINER xiaoxiao_shutong<1000@qq.com>
ENV work_path /home
WORKDIR $work_path
RUN yum -y install vim 
RUN yum -y install net-tools
EXPOSE 80
CMD echo $work_path
CMD echo "--------end----"
CMD /bin/bash

# 制作镜像 
docker build -f /root/test_docker_file -t mycentos:0.1 .

~~~

### docker 的状态

~~~
docker create ： 创建容器后，不立即启动运行，容器进入初建状态；

docker run ： 创建容器，并立即启动运行，进入运行状态；

docker start ： 容器转为运行状态；

docker stop ： 容器将转入停止状态；

docker kill ： 容器在故障（死机）时，执行 kill（断电），容器转入停止状态，这种操作容易丢失数据，

除非必要，否则不建议使用；

docker restart ： 重启容器，容器转入运行状态；

docker pause ： 容器进入暂停状态；

docker unpause ： 取消暂停状态，容器进入运行状态；

docker rm ： 删除容器，容器转入删除状态（如果没有保存相应的数据库，则状态不可见）

~~~



![image-20220821100602999](pic/image-20220821100602999.png)



### 3: docker  network

~~~
ip addr 
docker run -d -P --name tomcat01 tomcat
docker exec -it tomcat01 ip addr

# evth-pair 连接
Docker容器网络就很好的利用了Linux虚拟网络技术，在本地主机和容器内分别创建一个虚拟接口，并让他们彼此联通（这样一对接口叫veth pair）；
~~~

<img src="pic/image-20220821144915854.png" alt="image-20220821144915854" style="zoom:50%;" />

### 容器互联

~~~
docker network --help 

思考一个场景，我们编写一个微服务，数据库连接地址原来是使用ip的，如果ip变化就不行了，那我们能不能使用服务名访问呢？
--Link
docker run -d -P --name tomcat03 --link tomcat02 tomcat
docker exec -it tomcat03 ping tomcat02 # 能ping 通
#再来测试，tomcat02 是否可以ping tomcat03    反向也ping不通，其他也ping 不通

# 所以这里其实就是配置了一个 hosts 地址而已！12
# 原因：--link的时候，直接把需要link的主机的域名和ip直接配置到了hosts文件中了。

~~~

#### 4 自定义网络

~~~
docker network ls # 查看所有网络
#默认网络 docker0
docker run -d -P tomcat01 --net brideg tomxcat 

docker network create --help 

docker neowrok create --driver bridge --subnet 162.168.0.0/16 --gatway 192.168.0.1 mynet

~~~

#### 实验

~~~
GPU 还需要安装nvidia-container-toolkit
# Add the package repositories
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# 构建基础镜像 
nmcli dev show | grep 'DNS'
/etc/docker/daemon.json
{
  "dns": ["myDNS"]
}
du --max-depth=1 -h

docker build -f Dockerfile --network="host" -t test:0.1 .
~~~



## 7	sql

### 基本知识：

数据库系统实现整体数据结构化，这是数据库的主要特征之一。

数据的共享性高，冗余度低且易扩充

数据独立性高

数据由数据库管理系统统一管理和控制

数据的安全性security保护

防止不合法使用造成数据泄露和破坏

数据的完整性integrity

并发 concurrency 控制

数据库恢复 recovery

###  关系模型

实体 entity

属性 attribute

码 key 唯一标识实体属性集

实体型 entity type

实体集 entity set

联系 relationship

实体-联系方法  Entity-Relationship approch E-R

**关系操作：**
查询 query

选择 select 投影 project 连接 join 除 divide 并 union 差 except 交intersection

插入 insert

删除 delete

修改 update

### 关系的完整性

实体完整性  

primary key主码属性不能为空

参照完整性

用户自定义完整性

~~~sql
定义表：
create table table_name (
 col1 char(10) primary key , 
 col2 smallint   ,
 col3 smallint   ,
   /*表级完整性约束条件 col1 是外码 ，被参照表是table_name2 */
 foreign key (col1) references table_name2(col) 
   /* primary key(col1,col2,...)*/
) ;
修改表：
alter table <table_nale>
   [	add column<新列名><数据类型>[完整性约束] ]
   [ 	add <表级完整性约束>]
   [	drop [column] <列名>[cascade][restrict]]
   [	drop constraint <完整性约束>[restrict| cascade] ]
   [	alter column <列名> <数据类型>]
删除表
drop table <表名>[restrict | cascade]
restrict : 删除表是有条件的，不能被其他表引用， check , foreign key ,不能有视图触发器，存储过程，或者函数

索引的建立和删除

表的数据量非常大的时候，查询操作非常耗时，建立索引是加快查询的手段
索引有多种类型，
顺序文件上的索引
B+树索引
散列hash
位图索引
创建索引：
create [unique ] [cluster] index <索引名>
on <表名>（<列名>[次序],...）  desc asc（默认是升序）
修改索引：
alter index <旧索引名> rename to <新索引名>
删除索引
drop index <索引名>

数据查询
select [all | distinct] <目标列表达式> [,<目标列表达式>]...
from <表名或者视图名>[,<表名或视图名>...]|(select 语句)[as]<别名>
[where <条件表达式>]
[group by <列名1>[having <条件表达式>]]  //结果按照列名1的值进行分组，属性值相等的元素分为一个组， having 是加入一定的条件
[order by <列名2>[asc | desc ]]   // 按照一个属性列默认升序排序，asc desc 

distinct :  查询某一列，出现重复值， 
where :  短语作用于基本表或者视图 having : 短语作用于组，从中选择满足条件的组
> < = != , and ,or ,not in , between 20 and 30 
字符串匹配： like , % ,_ ,

聚集函数：
count(*)
count([distinct | all]<列明>)
sum()
avg()
max()
min()
注意： where 字句中不能使用聚集函数作为表达式，只能作用于select 和group by 中的having 字句

连接查询
1： 等值连接 （嵌套循环连接） 从第1个表第1条数据 ，去另一个表从头到位匹配，然后组成一个元组。
ex:
select Student.Sno,Sname
from Student , SC 
where Student.Sno = SC.Sno and SC.Cno='2' and SC.Grade > 90
2： 自身连接
	例如查询某个字段的前一个字段  先修课的先修课
3： 外连接
select Student.Sno,Sname
from Student left outer join SC on(Student.Sno = SC.Sno)
4: 多表连接
select Student.Sno,Sname
from Student , SC ,Coure
where Student.Sno = SC.Sno and SC.Cno = Course.Cno
5: 嵌套查询
   select-from-where 定义为查询快
   将一个查询快嵌套在另一个查询快的where ， 或者having 短语中，称为嵌套查询
   注意： order by 字句只能对最终的查询结果排序，字句查询中不能使用order by
  1: 带有 in 谓词的子查询
  2： any all   > all   >=all !=all 
  eg:
  	select Sname ,Sage
  	from Studetn
  	where Sagg < ANY(select Sage from student where Sdept= 'CS')
  	and Sdep <>'CS'
  	首先处理子查询，然后在处理父查询
  3 union、union all、or
  union 联合查询针对每一个查询结果，必须保证列数量、列数据类型及查询顺序一致
  group by 分组查询，根据一个或多个列对结果集进行分组，一般配合聚合函数使用
  
  select  col_1 ,col_2 , ... ,聚合函数...
  from table_name where 条件语句 group by 分组字段  ..having 分组条件
  eg :
  select red_num1 ,count(red_num1)
  from dlt where create_at >='2021-11-01' group by red_num1 hvaing count(red_num1)>=2 ;

其中，having 和 where 使用上有下面区别：
where
在 group by 分组前执行，将查询结果按照条件过滤数据
需要注意的是，where 无法与聚合函数一起使用
having
只能配合 group by 使用，在分组之后执行，用于过滤满足条件的组
需要注意的是，分组是一个耗时的操作，建议在分组前使用 where 对数据进行一次过滤，然后再进行分组
~~~



### 基本操作

~~~
关系性数据库： mysql  postgre  oracle 
redis :
查询内容时添加缓存 ， 
查询数据库之前先查询缓存  ，
https://blog.csdn.net/qq_34090008/article/details/110223773?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.pc_relevant_default&spm=1001.2101.3001.4242.1&utm_relevant_index=3

大规模数据如何检索:
如何解决单点故障；(lvs、F5、A10、Zookeep、MQ)
如何保证数据安全性；(热备、冷备、异地多活)
如何解决检索难题；(数据库代理中间件：mysql-proxy、Cobar、MaxScale等;)
如何解决统计分析问题；(离线、近实时)
为解决以上问题，从源头着手分析，通常会从以下方式来寻找方法：
1、存储数据时按有序存储；
2、将数据和索引分离；
3、压缩数据；
~~~

~~~sql
left join(左联接) 返回包括左表中的所有记录 和右表中联结字段相等的记录  左所有
right join(右联接) 返回包括右表中的所有记录和左表中联结字段相等的记录  
inner join(等值连接) 只返回两个表中联结字段相等的行   相等的
union  连接两个查询  唯一
union all  全部
date_add(data,1)

# 计算某点到 avg j
SELECT b1.business_id, SQRT(POWER(b1.latitude - b2.avg_lat, 2) +POWER(b1.longitude - b2.avg_long, 2)) as dist
FROM b_etl b1 
INNER JOIN 
(SELECT state, AVG(latitude) as avg_lat, AVG(longitude) as avg_long
FROM b_etl GROUP BY state) b2
ON b1.state = b2.state 
ORDER BY dist DESC


SELECT b.* FROM 
b_etl b 
INNER JOIN outlier o 
ON b.business_id = o.business_id 
WHERE o.dist<10

SELECT state, city, stars, review_count, explode(categories) AS category FROM business

SELECT state, city, stars, review_count, REPLACE(category, ' ','')as new_category FROM part_business_1

SELECT business_id, explode(categories) AS category FROM business

SELECT new_category, AVG(review_count)as avg_review_count 
FROM part_business 
GROUP BY new_category 
ORDER BY avg_review_count DESC

select t1.date,t1.cases - t2.cases as caseIncrease,t1.deaths - t2.deaths as deathIncrease 
from ustotal t1,ustotal t2 
where t1.date = date_add(t2.date,1)

select date,state,sum(cases) as totalCases,sum(deaths) as totalDeaths,round(sum(deaths)/sum(cases),4) as deathRate 
from usInfo 
where date = to_date('2020-05-19','yyyy-MM-dd') 
group by date,state


select 1 as sign,date,'USA' as state,round(sum(totalDeaths)/sum(totalCases),4) as deathRate 
from eachStateInfo 
group by date 
union select 2 as sign,date,state,deathRate 
from eachStateInfo


SELECT Country,SUM(Quantity) AS sumOfQuantity
FROM data 
GROUP BY Country
ORDER BY sumOfQuantity DESC
LIMIT 10


SELECT Country,COUNT(DISTINCT InvoiceNo) AS countOfReturnInvoice 
FROM data 
WHERE InvoiceNo LIKE 'C%' 
GROUP BY Country 
ORDER BY countOfReturnInvoice DESC 
LIMIT 10


SELECT StockCode,AVG(DISTINCT UnitPrice) AS avgUnitPrice,SUM(Quantity) AS sumOfQuantity 
FROM data 
GROUP BY StockCode

~~~

~~~http
https://mp.weixin.qq.com/s?__biz=MzI1NzI5NDM4Mw==&mid=2247484048&idx=1&sn=49f970d4421cfd8783808c9d63379074&chksm=ea18ebd0dd6f62c68988bfb894d4e4d9e9bdc2d0d44595f5a98d2df59debfec5ea43541c23d1&token=1376660398&lang=zh_CN&scene=21#wechat_redirect
~~~

SET @i=0;  # 更新主键id 
UPDATE table_name SET `id`=(@i:=@i+1); 
crate table new_name as select * from table_old  #复制一个表



## 8 redis

~~~
wget http://download.redis.io/redis-stable.tar.gz
tar xvzf redis-stable.tar.gz
yum -y install gcc gcc-c++ libstdc++-devel
make MALLOC=libc
make install 
~~~





## 9 es



~~~
倒排索引

索引 ：  dataset
https://blog.csdn.net/u011863024/article/details/115721328

索引 Index 
类型 type
文档 document  JSON 
字段 fild 
映射 Mapping 是处理数据的方式 ，和规则方面做的一些限制 ，
分片 shards 
副本 Replicas  分片的副本 高可用 
分配 allocation  


倒排索引
分词器


安装

基本使用 

高级应用


~~~





## 10 hive

~~~shell
Hive支持基本数据类型和复杂类型,
基本数据类型主要有数值类型(INT、FLOAT、DOUBLE ) 、布尔型和字符串
复杂数据类型：
ARRAY: 有序字段
MAP: 无序字段
STRUCT: 一组命名的字段
HiveQL操作命令：
数据定义、数据操作
除 dbproperties属性外，数据库的元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置，没有办法删除或重置数据库属性。
https://dblab.xmu.edu.cn/blog/1080-2/
~~~

~~~shell

https://zhuanlan.zhihu.com/p/100163942
https://zhuanlan.zhihu.com/p/103413663

1：Hive 查询原理：
hive 其实是将hql转成MR程序
元数据 是hive 表的一些参数 不是hdfs 上的数据
在执行查询操作时 ,先从元数据库中找到 对应表对应的文件位置
通过 hive的解析器、编译器、优化器 执行器 将 sql 语句 转换成 MR 程序，运行在 Yarn 上，最终得到结果。
Hive 是将数据映射成数据库和一张张的表，库和表的元数据信息一般存在关系型数据库上（比如 MySQL）
Hive 本身并不提供数据的存储功能，数据一般都是存储在 HDFS 上的（对数据完整性、格式要求并不严格）
MetaStore：存储和管理Hive的元数据，使用关系数据库来保存元数据信息。


2： 内部表和外部表
内部表会移动数据到指定位置 ，将数据文件移动到默认位置，一般都是/usr/hive/warehouse/ 目录下
外部表不会移动数据，数据在哪就是哪
内部表删除，数据一起删除
外部表不会删除数据
工作中使用外部表做为数据映射，而统计出的结果一般多使用内部表，因为内部表仅仅用于储存结果或者关联，与 HDFS 数据无关。
desc formatted 表名即可查看。 是否内部表 外部表


分区表
动态分区
静态分区
分桶表 超大数据
分桶表一般在超大数据时才会使用，分桶将整个数据内容按某列属性值取hash值进行区分，具有相同hash值的数据进入到同一个文件中，意味着原本属于一个文件的数据经过分桶后会落到多个文件中。

倾斜表
临时表

复合数据类型：
Hive 中复合数据类型有 Array、Map、Struct 这三种

Array 代表数组，类型相同的数据
Map 映射 k--v 对
Struct 则存储类型不同的一组数据


CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name   --DATABASE|SCHEMA 是等价的
  [COMMENT database_comment] --数据库注释
  [LOCATION hdfs_path] --存储在 HDFS 上的位置
  [WITH DBPROPERTIES (property_name=property_value, ...)]; --指定额外属性
describe database hive_test;

#删除数据库
DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];


CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name     --表名
  [(col_name data_type [COMMENT col_comment],
    ... [constraint_specification])]  --列名 列数据类型
  [COMMENT table_comment]   --表描述
  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]  --分区表分区规则
  [
    CLUSTERED BY (col_name, col_name, ...) 
   [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS
  ]  --分桶表分桶规则
  [SKEWED BY (col_name, col_name, ...) ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)  
   [STORED AS DIRECTORIES] 
  ]  --指定倾斜列和值
  [
   [ROW FORMAT row_format]    
   [STORED AS file_format]
     | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)]  
  ]  -- 指定行分隔符、存储文件格式或采用自定义存储格式
  [LOCATION hdfs_path]  -- 指定表的存储位置
  [TBLPROPERTIES (property_name=property_value, ...)]  --指定表的属性
  [AS select_statement];   --从查询结果创建表

# 通过查询建立表
CREATE TABLE emp_copy AS SELECT * FROM emp WHERE deptno='20';
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name  --创建表表名
   LIKE existing_table_or_view_name  --被复制表的表名
   [LOCATION hdfs_path]; --存储位置
CREATE TEMPORARY EXTERNAL TABLE  IF NOT EXISTS  emp_co  LIKE emp

加载数据到表
load data local inpath "/usr/file/emp.txt" into table emp;
重命名表
ALTER TABLE table_name RENAME TO new_table_name;
修改列

查询结果插入表
INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]]   
select_statement1 FROM from_statement;

INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] 
select_statement1 FROM from_statement


导出数据
INSERT OVERWRITE [LOCAL] DIRECTORY directory1
  [ROW FORMAT row_format] [STORED AS file_format] 
  SELECT ... FROM ...
 
# 累计窗口 函数
sum() over()
avg() over()
# 排序窗口函数
rank() over()
dense_rank() over()
row_number() over()

~~~



~~~
#分列
split_col = f.split(business['categories'], ',')
# 重命名 过滤掉空的数据 
business = business.withColumn("categories", split_col).filter(business["city"] != "").dropna()
# 临时表
business.createOrReplaceTempView("business")
# 筛选出想要的列数据 
 b_etl = spark.sql("SELECT business_id, name, city, state, latitude, longitude, stars, review_count, is_open, categories, attributes FROM business").cache()
 
# 到某店到州的center  的距离 
outlier = spark.sql(
        "SELECT b1.business_id, SQRT(POWER(b1.latitude - b2.avg_lat, 2) + POWER(b1.longitude - b2.avg_long, 2)) \
        as dist FROM b_etl b1 INNER JOIN (SELECT state, AVG(latitude) as avg_lat, AVG(longitude) as avg_long \
        FROM b_etl GROUP BY state) b2 ON b1.state = b2.state ORDER BY dist DESC")

# 内连接 
joined = spark.sql("SELECT b.* FROM b_etl b INNER JOIN outlier o ON b.business_id = o.business_id WHERE o.dist<10")
# 将一[1,2,3] 拆成多行 explode
part_business = spark.sql("SELECT state, city, stars, review_count, explode(categories) AS category FROM business").cache()
#统计全部的类别  
spark.sql("SELECT COUNT(DISTINCT(new_category)) FROM part_business")

#top 10 business categories
spark.sql("SELECT new_category, COUNT(*) as freq FROM part_business GROUP BY new_category ORDER BY freq DESC")

print("## Top business categories - in every city")
spark.sql("SELECT city, new_category, COUNT(*) as freq FROM part_business GROUP BY city, new_category ORDER BY freq DESC")

 print("## Cities with most businesses")
spark.sql("SELECT city, COUNT(business_id) as no_of_busy FROM business GROUP BY city ORDER BY no_of_bus DESC")

df.toJSON()
df.toPandas()
~~~

~~~python
一文学完所有的Hive Sql（两万字最全详解） 五分钟学大数据  
1： 基本概念
存储格式： textfile ,sequencefile(binary insert 加载) rcfile(行列混合压缩) orc


2： 基本操作 
DDL : 
创建，修改数据库
表： 内部表，外部表，分区表。分桶表
单表的查询，关联查询
hive 函数： 聚合函数,条件函数，日期函数 ，字符串函数
行转列,列转行  ： lateral view , explode  reflect
窗口函数 
分析函数
3： 语法知识 

#对数据库的操作 
创建数据库
create database if not exists myhive;
说明：hive的表存放位置模式是由hive-site.xml当中的一个属性指定的:hive.metastore.warehouse.dir

创建数据库并指定hdfs存储位置 :
create database myhive2 location '/myhive2';

查看数据库详细信息
desc database  myhive2;
desc database extended  myhive2; 查看数据库更多详细信息

删除数据库
drop  database  myhive2;
强制删除数据库，包含数据库下面的表一起删除
drop  database  myhive  cascade; 

#对数据表的操作
	建内部表
create  table 
if not exists stu2(id int ,name string) 
row format delimited fields terminated by '\t' 
stored as textfile 
location '/user/stu2';
row format delimited fields terminated by '\t' 指定字段分隔符，默认分隔符为 '\001'
stored as 指定存储格式
location 指定存储位置

根据查询结果创建表
create table stu3 as select * from stu2;
根据已经存在的表结构创建表
create table stu4 like stu2;
查询表的结构
desc stu2;
详细查询
desc formatted  stu2;
查询创建表的语句
show create table stu2;

#对外部表操作
外部表因为是指定其他的hdfs路径的数据加载到表当中来，所以hive表会认为自己不完全独占这份数据，所以删除hive表的时候，数据仍然存放在hdfs当中，不会删掉，只会删除表的元数据

构建外部表
create external table student (s_id string,s_name string) 
row format delimited fields terminated by '\t';
从本地文件系统向表中加载数据
追加操作
load data local inpath '/export/servers/hivedatas/student.csv' into table student;
覆盖操作
load data local inpath '/export/servers/hivedatas/student.csv' overwrite  into table student;
从hdfs文件系统向表中加载数据
load data inpath '/hivedatas/techer.csv' into table techer;
加载数据到指定分区
load data inpath '/hivedatas/techer.csv' into table techer partition(cur_date=20201210);

1.使用 load data local 表示从本地文件系统加载，文件会拷贝到hdfs上
2.使用 load data 表示从hdfs文件系统加载，文件会直接移动到hive相关目录下，注意不是拷贝过去，因为hive认为hdfs文件已经有3副本了，没必要再次拷贝了
3.如果表是分区表，load 时不指定分区会报错
4.如果加载相同文件名的文件，会被自动重命名

#对分区表的操作
创建分区表的语法
create table score(s_id string, s_score int) partitioned by (month string);
创建一个表带多个分区
create table score2 (s_id string, s_score int) partitioned by (year string,month string,day string);
加载数据到一个分区的表中
load data local inpath '/export/servers/hivedatas/score.csv' into table score partition (month='201806');
加载数据到一个多分区的表中去
load data local inpath '/export/servers/hivedatas/score.csv' into table score2 partition(year='2018',month='06',day='01');
当表是分区表时，比如 partitioned by (day string)， 则这个文件夹下的每一个文件夹就是一个分区，且文件夹名为 day=20201123 这种格式，然后使用：msck  repair   table  score; 修复表结构，成功之后即可看到数据已经全部加载到表当中去了

查看分区
show  partitions  score;
添加一个分区
alter table score add partition(month='201805');
同时添加多个分区
alter table score add partition(month='201804') partition(month = '201803');
注意：添加分区之后就可以在hdfs文件系统当中看到表下面多了一个文件夹
删除分区
alter table score drop partition(month = '201806');

#对分桶表操作
将数据按照指定的字段进行分成多个桶中去，就是按照分桶字段进行哈希划分到多个文件当中去
分区就是分文件夹，分桶就是分文件
分桶优点：
1. 提高join查询效率
2. 提高抽样效率
开启hive的捅表功能
set hive.enforce.bucketing=true;
设置reduce的个数
set mapreduce.job.reduces=3;

创建桶表
create table course (c_id string,c_name string) 
clustered by(c_id) into 3 buckets;

桶表的数据加载：由于桶表的数据加载通过hdfs  dfs  -put文件或者通过load  data均不可以，只能通过insert  overwrite 进行加载所以把文件加载到桶表中，需要先创建普通表，并通过insert  overwrite的方式将普通表的数据通过查询的方式加载到桶表当中去

通过insert overwrite给桶表中加载数据
insert overwrite table course 
select * from course_common cluster by(c_id);  -- 最后指定桶字段


#修改表和删除表
修改表名称
alter table  old_table_name  rename to new_table_name;
增加/修改列信息
查询表结构
desc score5;
添加列
alter table score5 add columns (mycol string, mysco string);
更新列
alter table score5 change column mysco mysconew int;
删除表操作
drop table score5;
清空表操作
truncate table score6;
说明：只能清空管理表，也就是内部表；清空外部表，会产生错误

如果 hdfs 开启了回收站，drop 删除的表数据是可以从回收站恢复的，表结构恢复不了，需要自己重新创建；truncate 清空的表是不进回收站的，所以无法恢复truncate清空的表
所以 truncate 一定慎用，一旦清空将无力回天

#向hive表中加载数据
直接向分区表中插入数据
insert into table score partition(month ='201807') values ('001','002','100');
通过load方式加载数据
load data local inpath '/export/servers/hivedatas/score.csv' overwrite into table score partition(month='201806');
通过查询方式加载数据
insert overwrite table score2 partition(month = '201806') select s_id,c_id,s_score from score1;
查询语句中创建表并加载数据
create table score2 as select * from score1;
在创建表是通过location指定加载数据的路径
create external table score6 (s_id string,c_id string,s_score int) 
row format delimited fields terminated by ',' 
location '/myscore';

export导出与import 导入 hive表数据（内部表操作）

create table techer2 like techer; --依据已有表结构创建表
export table techer to  '/export/techer';
import table techer2 from '/export/techer';


#hive表中数据导出

将查询的结果导出到本地
insert overwrite local directory '/export/servers/exporthive'
select * from score;
将查询的结果格式化导出到本地
insert overwrite local directory '/export/servers/exporthive' 
row format delimited fields terminated by '\t' 
collection items terminated by '#' 
select * from student;

将查询的结果导出到HDFS上(没有local)
insert overwrite directory '/export/servers/exporthive' 
row format delimited fields terminated by '\t' 
collection items terminated by '#' 
select * from score;

Hadoop命令导出到本地
dfs -get /export/servers/exporthive/000000_0 /export/servers/exporthive/local.txt;

hive shell 命令导出
基本语法：（hive -f/-e 执行语句或者脚本 > file）
hive -e "select * from myhive.score;" > /export/servers/exporthive/score.txt
hive -f export.sh > /export/servers/exporthive/score.txt

export导出到HDFS上
export table score to '/export/exporthive/score';

#hive的DQL查询语法

单表查询

SELECT [ALL | DISTINCT] select_expr, select_expr, ... 
FROM table_reference
[WHERE where_condition] 
[GROUP BY col_list [HAVING condition]] 
[CLUSTER BY col_list 
  | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list] 
] 
[LIMIT number]


注意：
1、order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。
2、sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks>1，则sort by只保证每个reducer的输出有序，不保证全局有序。
3、distribute by(字段)根据指定的字段将数据分到不同的reducer，且分发算法是hash散列。
4、Cluster by(字段) 除了具有Distribute by的功能外，还会对该字段进行排序。因此，如果分桶和sort字段是同一个时，此时，cluster by = distribute by + sort by

WHERE语句
select * from score where s_score < 60;
GROUP BY 分组
select s_id ,avg(s_score) from score group by s_id;

分组后对数据进行筛选，使用having
select s_id ,avg(s_score) avgscore 
from score 
group by s_id 
having avgscore > 85;

如果使用 group by 分组，则 select 后面只能写分组的字段或者聚合函数where和having区别：
1 having是在group by分完组之后再对数据进行筛选，所以having 要筛选的字段只能是**分组字段**或者**聚合函数**
2 where 是从数据表中的字段直接进行的筛选的，所以不能跟在gruop by后面，也不能使用聚合函数


join 连接:
NNER JOIN 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来
select * from techer t [inner] join course c on t.t_id = c.t_id; -- inner 可省略

LEFT OUTER JOIN 左外连接：左边所有数据会被返回，右边符合条件的被返回
select * from techer t left join course c on t.t_id = c.t_id; -- outer可省略

RIGHT OUTER JOIN 右外连接：右边所有数据会被返回，左边符合条件的被返回、
select * from techer t right join course c on t.t_id = c.t_id;

FULL OUTER JOIN 满外(全外)连接: 将会返回所有表中符合条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。
SELECT * FROM techer t FULL JOIN course c ON t.t_id = c.t_id ;

order by 排序
局排序，只会有一个reduce
ASC（ascend）: 升序（默认） DESC（descend）: 降序
SELECT * FROM students 
LEFT JOIN score sco ON s.s_id = sco.s_id 
ORDER BY sco.s_score DESC;
order by 是全局排序，所以最后只有一个reduce，也就是在一个节点执行，如果数据量太大，就会耗费较长时间

sort by 局部排序
每个MapReduce内部进行排序，对全局结果集来说不是排序。
设置reduce个数
set mapreduce.job.reduces=3;
查看设置reduce个数
set mapreduce.job.reduces;
查询成绩按照成绩降序排列
select * from score sort by s_score;
将查询结果导入到文件中（按照成绩降序排列）
insert overwrite local directory '/export/servers/hivedatas/sort' 
select * from score sort by s_score;

distribute by 分区排序
distribute by：类似MR中partition，进行分区，结合sort by使用
设置reduce的个数，将我们对应的s_id划分到对应的reduce当中去
set mapreduce.job.reduces=7;
通过distribute by  进行数据的分区
select * from score 
distribute by s_id sort by s_score;
注意：Hive要求 distribute by 语句要写在 sort by 语句之前

cluster by
当distribute by和sort by字段相同时，可以使用cluster by方式.
cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是正序排序，不能指定排序规则为ASC或者DESC。
以下两种写法等价
select * from score 
cluster by s_id;

select * from score 
distribute by s_id sort by s_id;

~~~







## 11 spark pyspark

#### 基本概念

~~~markdown
https://www.bilibili.com/video/BV1j4411u7hT?p=6&spm_id_from=pageDriver

spark core 
spark sql
spark streaming 
spark MLlib
图计算 GraphX 

spark + yarm + hdfs  
spark + mysql redis kafka zookeeper 

部署模式：

local 
至少2台
standalone 自带集群模式
yarm   集群模式
mesos  集群模式 

master  worker 
1：配置主节点 到从节点免登录 
2： 关闭防火墙  （大数据都是内网集群）
service iptables status
serverice iptables stop
chkconfig iptables off
3: IP 和hostname 映射关系
ip hostname

spark 目录：
bin   可执行的脚步
conf   配置文件
jars   依赖jar 包
sbin   集群管理命令

修改配置文件：
spark-env.sh
export JAVA_HOME=
export SPARK_MASTER_HoST=hdp-01
export SPARK_MASTER_PORT=7077
7077: 是spark 内部通信端口  worker向master 连接端口
8088： web 访问master 的端口  
slaves.sh
hdp-02/ip  所有的从节点 
# 分发安装包
for i in 2 3 4;
do scp -r spark(安装包)/ hdp-0$i:$PWD ; done 
# 启动
start-master.sh
start-slavers.sh
start-all.sh
# 日志问题
spark-home/logs
master 启动命令 
java -cp *** **.class 参数列表
netstat -natpl | grep 1608
在spark  hadoop  都有start-all.sh 
 /etc/profile  谁先加载 ，谁生效 ，  或者改名 
 
 启动后 : 
 master 地址：  spark://hdp-01:7077 
work 占用资源： 
cores  : 当前机器
memory  ： 当前机器 内存-1g

# spark 提交任务
客服端：
1必须安装spark 安装包 spark-submit
2必须连接到master
spark 提交给maset ,在集群执行

spark-shell : 交互式命令行
spark-submit : 标准的提交命令
spark-submit 选项 jar包 参数列表
--master   yarm spark://host:port local  默认local   local[*] 可以指定核
--class  主方法所在类
--deploy-model  driver 位置  driver的位置
clinet  : driver 运行在客服端
cluster : driver 在集群中的某一台 
jar 架包
在写spark 程序的时候必须有sparkContext
在集群模式下， 不能读取本地的文件 一般读取hdfs 文件 
1: master 先启动 ，检查超时的worker
2： worker 启动 向 master 注册 ， 资源信息 （memory ,core）
3: master 保存worker的相关信息
4： master 向worker 发送注册成功的消息
5： worker 启动定时任务， 报活  ，发送心跳
5: master 修改worker的最后一次心跳时间

1:客户端请求application 
2:master 进行资源分配
   打散策略
  每一个空闲的worker 来参与运行任务，向worker 发送指令 ，启动executor
3: driver :
	解析业务逻辑功能
	切分阶段 stage
	创建task ,条件task  （task 执行的最小单位）
4: excutor 向driver 注册 ，并通信


~~~

~~~shell
常驻进程：
master ： 集群的管理者， 管理worker , 负责客服端的任务请求调度
worker : 向master 通信 ，管理自己的executor
任务执行时：
在哪里提交，在哪里产生 sparksubmit 进程
coarseGrainedExecutorBackend 提供干活的资源 简称executor 所有的task
driver程序： 主控程序 ， 解析业务逻辑代码， 切分阶段stage 创建task 提交task 到executor执行

可以指定资源分配：
--executor-cores  每一个executors（work） 占用的cores
--executor-memory    每一个executors 占用的memory
--total-executor-cores  application 能使用的最多cores数

work  : 启动
    
在提交spark 任务，反射执行自定义main方法
1： 客服端提交 spark-submit 选项 jar包 参数列表
2: 调用spark-class 脚本 通过exec 执行启动一个main方法
3： SparkSubmit.main 反射执行一个类 wordCount.main 
4: new SparkContext 
    创建3个核心的对象 TaskScheduler , DAGScheduler ,SchedulerBackend
SchedulerBackend : coarseGrainedExecutorBackend , StandaloneSchedulerBackend
~~~







#### RDD

~~~shell
分为两类算法：
Transfomer : 转换算子 lazy  产生一个新的rdd
map fliter flatMap goupBykey reduceBykey aggregateByKey ,union ,join ,coalesce
action :    执行 (写文件 ， ) 返回结果给驱动程序 ，或写入文件系统
reduce ,collect ,count  ,first ,take ,countByKey , foreach 

产生job 

Application ->job（DAG）->Stage -> task 
RDD: 弹性分布式数据集合  resillebt distributed dataset（不可变得 ，只读的）
弹性： 容错能力 
分布式： 以分区为单位 ，运行在不同的节点上

1: rdd 的创建：
集合并行化
读取外部系统文件  sc.textFilxt(path)
转换算子
2:  rdd 的分区  默认分区数量 = application 使用的cores 数量
 每一个rdd 都有分区 。 rdd.size 分区的数量
partition :  
hdfs :   分区数量 = 读取hdfs 文件的block 快的数量


RDD依赖关系  父到子
宽依赖  有shuffle操作
窄依赖 
划分阶段
从计算过程：
ont-to-one  所有作业可以流水线  pipeline 速度快 窄依赖
宽依赖必须拿到所有分区的数据才能计算，速度慢
容错： 窄依赖 好 宽依赖差

基于以上2点， 用dependency , 用于封装两种不同的依赖关系  ，然后搞出stage阶段
把所有的窄依赖放在一个阶段中 

窄依赖算子：
map flatMap filter 不会改变分区数量
union coalesce   增加  减少分区数量
所有的窄依赖算子rdd 都在一个阶段中 stage
（一旦遇到宽依赖 ，就切分stage）
宽依赖算子: shuffle 一对多的关系  : 有几个算子 就切分多个阶段
groupByKey  reduceByKey distinct 

Stage ： spark 中对所有的窄依赖的封装 ，提供的一个抽象类
Stage： ResultStage ： 只有一个 ， 就是最后一个stage
ShuffleMapStage :  0 到多个

Stage 是有起始边界：
ShuffleMapStage： 起点  job 中的第一个RDD
					终点： shuffle Write
ResultStage:  起点: Shffle Read 
				终点： job 中最后一个RDD 
job的起始位置：
当触发action 算法的时候，产生job  ， job终止位置 ,就是action算子对应的RDD

DAG ： 有向五环图
有向： 数据的流向， RDD的依赖方向
无环 ： 不是闭环
图：  点 RDD  线 rdd 之间的依赖关系 

1Driver 
在解析业务逻辑代码的时候 ，完成DAG的构建，当触发action 算子的时候，DAG构建完成
2DAGSchedule
基于DAG,切分stage ，创建task ，提交stage
3Taskschedule
调度Task ，提交task 给executor 执行 
~~~

##### 基本算子

~~~markdown
map :  一一映射  对每一条数据执行
mapValues :
mapPartitions: 一一映射,对每个分区 
mapPatitionWithIndex

zip  分区数量必须一致，每个分区的元素也必须一致   a ,b ->（a, b）
zipWithIndex 

groupByKey 
默认的分区器 ：  key hashcode % 分区数
数据重新分发 ： shuffle 
reduceByKey
分区类聚合  
能使用reduceByKey 就不用groupByKey  

foreach  一一映射 ， 没有返回值 ， 常见的打印  每次迭代一个元素  
foreachPartition 

sortByKey   产生 job
zipWithIndex  产生 job
take 产生 job
reduct 元素并归 ， 没有顺序
coutByKey    某个元素的key 的个数
collectAsMAp  ： 用于[k,v] rdd , 以map 集合返回
take()
first()
top()  返回前n 个
takeOrdered(n) 
~~~

##### RDD的高级应用

~~~shell
1：RDD 的缓存和持久化 
默认情况下， 每一个action 操作 ， 都会重新计算上面的转换RDD （重新计算）
如果能把该RDD的结果缓存下来， 那么基于该RDD的操作，就不需要从头计算 
cache() (StoregeLevel )  缓存
persisit(StoregeLevel.MeMORY_AND_DISK) 持久化
执行完业务逻辑，可以手动释放
unpersisit()

2：RDD 的checkpoint 容错方案 
把rdd 中的数据 ，写到分布式文件系统中
sc.setCheckpointDir('hdfs://ip:port/dir')
rdd.checkpoint   
	1:是lazy 算子  ，当触发action算子的时候， 才执行，产生2个job,第一个job	
是 执行业务逻辑， 第二个job 执行写入到数据到hdfs
	2: 当对某一个rdd 执行checkpoint 之后， 该RDD原有的父依赖关系被删除 ，取而代之是checkpointRDD
	3: 以后对该RDD所有的操作，依赖关系从CheckpointRDD开始，真正的数据从hdfs 读取
	
在特别复杂的业务逻辑中，才会用 	

3： RDD 中的共享变量（广播变量 ， 累计器） 广播大变量  在driver端一个副本 

RDD的内存管理方法：
spark1.x 静态内存管理 （各个模块的占比是固定的 ，容易造成不均衡 淘汰）
spark2.x 统一的内存管理  

内存的组成部分：
总的内存 = 系统预留内存（300M） + 可用内存
可用内存  = 统一内存（0.6） + 其他（0.4）
统一内存 = 存储内存(storege)（0.5） + 执行内存 (executiom)（0.5）

Storege 存储内存 ：
缓存RDD ,展开partition ,存放Direct task result ，存放广播变量 ，在spark Streaming reciver 模式中， 存放每个batch 的blocks
execution 执行内存：
用于shuffle , join , sort ,aggregation(聚合) ，buffer等操作
其他：
spark : 内部的元数据 ， OOM错误 ， 

默认配置参数， spark.memory.fraction = 0.6
			 spark.memory.storageFraction=0.5
			 

Torrent 比特洪流技术
广播变量实现类 ： TorrentBroadcast 
利用广播变量， 能保证一个executor的所有task 共用一份反序列化的数据
减少数据的反序列化和GC
使用：
    diver 端广播  blockManagerMaster 
    executor 使用  blockManager 
注意使用： 
1除了RDD 不能被广播， 其他的类 、集合。对象都可以被广播
2： 被广播数据， 可以在任意的executor 中通过value调用
3： 广播变量是只读的，不能被修改 
4： 最大的功能是，就是保证一个executor中所有的task共用一份数据 ，提升效率 


4： 累加器
在driver端定义一个类加器
全局的 可以通过Web 监控界面查看 
acc = sc.LongAccumulator('variable_acc')

总结：
1在driver端定义累加器
2在任意的函数中，executor中通过add方法调用
3累加器是全局(application为单位)的累加 
4在driver端 ，通过value 方法获取值
5在程序的监控界面，通过累加器的名称来获取值

~~~

spark 序列化

~~~sh
spark 中使用的序列化机制 默认是java 序列化 javaSerializer
内存中对象  序列化 二进制文件  网络传输 反序列化

KryoSerializer  : 占用内存少，速度快， 需要注册

1： spark standalone HA  （high avaliable ） 高可用  7 * 24  (集群) 
 单点故障  核心节点
 standlone master worker
 master alive  master standby(热备)
zookeeper 管理节点 高可用 
2： spark on yarn  standalone 两套不同的集群

yarn 集群  
客服端提交任务  1台（满足条件  ，spark 安装包 ， hadoop 的配置 （hdfs  +yarn）） 连接到hadoop集群 

yarn 集群：
ResourceManager 
NodeManager
1: 修改 ${hadoop}/etc/hadoop/capacity-scheduler.xml 中的资源调度

2： yarn.site.xml


spark 的客服端配置 
1：jdk
vim ${spark_home}/conf/spark-env.sh
2: hadoop_conf_dir  /yarn_conf_dir # hadoop的配置文件路径 

spark-submit --master yarn --deploy-model client/cluster 

两种部署模式区别 ：
cluster : Driver 运行在集群中的某一个节点 ， Driver 可以容错 ，提交任务，就可以退出
client :  Driver 运行在客服端 
~~~

##### UDF 自定义函数

~~~python
第1种方式
# 定义自定义函数
 def fun_name():
 	pass
# 注册自定义函数
udf1 = functions.udf(fun_name, FloatType()
# 使用自定义函数
dataframe = dataframe.withColumn(col, udf1(col))
第2种方式
注册udf，在sql中使用
spark.udf.register("udf1", fun_name,StringType()) 
dataframe = dataframe.withColumn(col, udf1(col))
第3种方式 注解形式更方便
@udf(returnType=StringType()) 
def fun_name():
 	pass
df.withColumn("Cureated Name", convertCase(col("Name"))).show(truncate=False)

pandas_udf函数支持定义三种类型的函数，分别为SCALAR、GROUP_MAP、GROUP_AGG，在pyspark.sql.function.PandasUDFType 中做了定义

from pyspark.sql.functions import pandas_udf, PandasUDFType
~~~

由于数据中某些字段包含 json 数据，因此直接使用 DataFrame 进行读取会出现分割错误，所以如果要创建 DataFrame，需要先直接读取文件生成 RDD，再将 RDD 转为 DataFrame。过程中，使用 python3 中的 csv 模块对数据进行解析和转换。

~~~python
df.printSchema()
df.withColumn()
df.withColumnRenamed()
df.sort(col_name1.asc() , col_name.desc())
df.gropuby().agg(sum(), ave(),..).where( col('name')> 100)  # agg 可以同时使用多个聚合函数
df.filter() # 选择 select
df.filter("city == 'beijing' and ctr > 0.2").show() 支持字符串 
dropna() 删除空行
explode（col_name） # 该行拆成多行

DataFrame 的函数
Action 操作
1、 collect() ,返回值是一个数组，返回dataframe集合所有的行
2、 collectAsList() 返回值是一个java类型的数组，返回dataframe集合所有的行
3、 count() 返回一个number类型的，返回dataframe集合的行数
4、 describe(cols: String*) 返回一个通过数学计算的类表值(count, mean, stddev, min, and max)，这个可以传多个参数，中间用逗号分隔，如果有字段为空，那么不参与运算，只这对数值类型的字段。例如df.describe("age", "height").show()
5、 first() 返回第一行 ，类型是row类型
6、 head() 返回第一行 ，类型是row类型
7、 head(n:Int)返回n行  ，类型是row 类型
8、 show()返回dataframe集合的值 默认是20行，返回类型是unit
9、 show(n:Int)返回n行，，返回值类型是unit
10、 table(n:Int) 返回n行  ，类型是row 类型

dataframe的基本操作
1、 cache()同步数据的内存
2、 columns 返回一个string类型的数组，返回值是所有列的名字
3、 dtypes返回一个string类型的二维数组，返回值是所有列的名字以及类型
4、 explan()打印执行计划  物理的
5、 explain(n:Boolean) 输入值为 false 或者true ，返回值是unit  默认是false ，如果输入true 将会打印 逻辑的和物理的
6、 isLocal 返回值是Boolean类型，如果允许模式是local返回true 否则返回false
7、 persist(newlevel:StorageLevel) 返回一个dataframe.this.type 输入存储模型类型
8、 printSchema() 打印出字段名称和类型 按照树状结构来打印
9、 registerTempTable(tablename:String) 返回Unit ，将df的对象只放在一张表里面，这个表随着对象的删除而删除了
10、 schema 返回structType 类型，将字段名称和类型按照结构体类型返回
11、 toDF()返回一个新的dataframe类型的
12、 toDF(colnames：String*)将参数中的几个字段返回一个新的dataframe类型的，
13、 unpersist() 返回dataframe.this.type 类型，去除模式中的数据
14、 unpersist(blocking:Boolean)返回dataframe.this.type类型 true 和unpersist是一样的作用false 是去除RDD

集成查询：
1、 agg(expers:column*) 返回dataframe类型 ，同数学计算求值
df.agg(max("age"), avg("salary"))
df.groupBy().agg(max("age"), avg("salary"))
2、 agg(exprs: Map[String, String])  返回dataframe类型 ，同数学计算求值 map类型的
df.agg(Map("age" -> "max", "salary" -> "avg"))
df.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
3、 agg(aggExpr: (String, String), aggExprs: (String, String)*)  返回dataframe类型 ，同数学计算求值
df.agg(Map("age" -> "max", "salary" -> "avg"))
df.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
4、 apply(colName: String) 返回column类型，捕获输入进去列的对象
5、 as(alias: String) 返回一个新的dataframe类型，就是原来的一个别名
6、 col(colName: String)  返回column类型，捕获输入进去列的对象
7、 cube(col1: String, cols: String*) 返回一个GroupedData类型，根据某些字段来汇总
8、 distinct 去重 返回一个dataframe类型
9、 drop(col: Column) 删除某列 返回dataframe类型
10、 dropDuplicates(colNames: Array[String]) 删除相同的列 返回一个dataframe
11、 except(other: DataFrame) 返回一个dataframe，返回在当前集合存在的在其他集合不存在的
12、 explode[A, B](inputColumn: String, outputColumn: String)(f: (A) ⇒ TraversableOnce[B])(implicit arg0: scala.reflect.api.JavaUniverse.TypeTag[B]) 返回值是dataframe类型，这个 将一个字段进行更多行的拆分
df.explode("name","names") {name :String=> name.split(" ")}.show();
将name字段根据空格来拆分，拆分的字段放在names里面
13、 filter(conditionExpr: String): 刷选部分数据，返回dataframe类型 df.filter("age>10").show();  df.filter(df("age")>10).show();   df.where(df("age")>10).show(); 都可以
14、 groupBy(col1: String, cols: String*) 根据某写字段来汇总返回groupedate类型   df.groupBy("age").agg(Map("age" ->"count")).show();df.groupBy("age").avg().show();都可以
15、 intersect(other: DataFrame) 返回一个dataframe，在2个dataframe都存在的元素
16、 join(right: DataFrame, joinExprs: Column, joinType: String)
一个是关联的dataframe，第二个关联的条件，第三个关联的类型：inner, outer, left_outer, right_outer, leftsemi
df.join(ds,df("name")===ds("name") and  df("age")===ds("age"),"outer").show();
17、 limit(n: Int) 返回dataframe类型  去n 条数据出来
18、 na: DataFrameNaFunctions ，可以调用dataframenafunctions的功能区做过滤 df.na.drop().show(); 删除为空的行
19、 orderBy(sortExprs: Column*) 做alise排序
20、 select(cols:string*) dataframe 做字段的刷选 df.select($"colA", $"colB" + 1)
21、 selectExpr(exprs: String*) 做字段的刷选 df.selectExpr("name","name as names","upper(name)","age+1").show();
22、 sort(sortExprs: Column*) 排序 df.sort(df("age").desc).show(); 默认是asc
23、 unionAll(other:Dataframe) 合并 df.unionAll(ds).show();
24、 withColumnRenamed(existingName: String, newName: String) 修改列表 df.withColumnRenamed("name","names").show();
25、 withColumn(colName: String, col: Column) 增加一列 df.withColumn("aa",df("name")).show();
pyspark.sql.functions里有许多常用的函数，可以满足日常绝大多数的数据处理需求
df.show()  df.count()  df.printSchema() df.columns df.dtypes
df.select()
df.age.alias('new_name','name')
df.filter() #筛选
F.lit() 增加常数列
df.drop_duplicates()
df.distinct()
df.drop('id') # 删除列
df.dropna(subset=['age', 'name'])  # 传入一个list，删除指定字段中存在缺失的记录
df.fillna({'age':10,'name':'abc'})  # 传一个dict进去，对指定的字段填充
df.groupby('name').agg(F.max(df['age']))
df.select(F.max(df.age))
df.select(F.min(df.age))
df.select(F.avg(df.age)) # 也可以用mean，一样的效果
df.select(F.countDistinct(df.age)) # 去重后统计
df.select(F.count(df.age)) # 直接统计，经试验，这个函数会去掉缺失值会再统计

from pyspark.sql import Window
df.withColumn("row_number", F.row_number().over(Window.partitionBy("a","b","c","d").orderBy("time"))).show() # row_number()函数
~~~



~~~
				key 		 v 		
rdd2 =rdd1.map(lambda v: (v.genre, int(v.num_of_sales))).reduceByKey(lambda x, y: x + y).collect()  # collect 返回的是 []
json.dumps(rdd2)
# 注意reducebykey  x,y : x+y    都是 value 

# 两个列求和 
					 key						v-> []
rdd.map(lambda v: (int(v.year_of_pub),  [ int(v.num_of_tracks), 1] ))\
.reduceByKey(lambda x, y: [x[0] + y[0], x[1] + y[1]]).sortByKey()
 = res = [k ,v]
list(map(lambda v: v[0], result))
list(map(lambda v: v[1][0], result))
list(map(lambda v: v[1][1], result))

						 key ()  元组								v 				
rdd.map(lambda v: ( (v.genre, int(v.year_of_pub)), int(v.num_of_sales)))\
.reduceByKey(lambda x, y: x + y).sortByKey().collect()

#多列 聚合操作  
rdd.map(lambda v: (v.genre, (float(v.rolling_stone_critic), float(v.mtv_critic), float(v.music_maniac_critic), 1)))\
.reduceByKey(lambda x, y : (x[0] + y[0], x[1] + y[1], x[2] + y[2], x[3] + y[3]))\
.map(lambda v: (v[0], v[1][0]/v[1][3], v[1][1]/v[1][3], v[1][2]/v[1][3])).collect()

~~~

#### spark sql :

~~~shell
import pyspark.sql.functions as f
f.split(df['categories'], ',') # f
1: withColumn()
使用带有列的PySpark更改列DataType df.withColumn(x, df[x].cast('int'))
更新现有列的值  df.withColumn( "newDate",split(df['Date'], " ")[0] ).drop("Date")
从现有的创建新列
2:重命名列名
df.withColumnRenamed()  df = df.withColumnRenamed('old_name','new_name')
df.drop()
# 创建临时表
3df.createOrReplaceTempView('table_name')

4:explode 将列数据展开
[a,b,[1,2]]
[a,b,1]
[a,b,2]

5：from pyspark.sql.functions import regexp_replace
df.withColumn('address', regexp_replace('address', 'Rd', 'Road')) \
  .show(truncate=False)
6： 写在本地
df.write.parquet("file:///home/hadoop/wangyingmin/yelp-etl/business_etl", mode="overwrite")
df.write.json(path, model)
df = spark.read.parquet(data_path).cache()

def read_json(file_path):
    json_path_names = os.listdir(file_path)
    data = []
    for idx in range(len(json_path_names)):
        json_path = file_path + '/' + json_path_names[idx]
        if json_path.endswith('.json'):
            with open(json_path) as f:
                for line in f:
                    data.append(json.loads(line))
    return data
df = read_json(path)

# 读取json 文件 dataframe
spark.read.csv("input/earthquake_cleaned.csv",  header=True, inferSchema=True)
df.toPandas().to_csv("earthquakeC.csv", encoding='utf-8', index=False)
df.groupBy("Year").count().orderBy("Year")
# 过滤
df.filter("Area is not null")
#排序
df.sort(df["Magnitude"].desc(), df["Year"].desc()).take(500)
#当震源深度相同时，将震级更高的地震排在前面。
df.sort( df["Depth"].desc(), df["Magnitude"].desc() ).take(500)
# 取别名 alias
rawData.agg(*[count(c).alias(c) for c in rawData.columns]).show()

df['y'] # 取值 [0,1]
df_age = df.select(df['age'],df['y'])
bin = [0,30,45,60,75,100]
# 统计各个年龄段 0的人数
df_age.filter(df['age'].between(bin[i],bin[i+1])).filter(df['y']=='0').count()


# 月收入分析
df_income = df.select(df['MonthlyIncome'],df['y'])
# 获取平均值，其中先返回Row对象，再获取其中均值
mean_income = df_income.agg(functions.avg(df_income['MonthlyIncome'])).head()[0]
# 收入分布，105854人没超过均值6670，44146人超过均值6670
df_income.filter(df['MonthlyIncome'] < mean_income).count()

df.select("Job").distinct().show(truncate=False)
df.dropDuplicates((['Job'])).select("Job").show(truncate=False)
~~~

#### dataframe:

~~~shell
fields = [ StructField("date", DateType(),False),StructField("county", StringType(),False),StructField("deaths", IntegerType(),False),]  # false 不能为空
schema = StructType(fields)

# flagMap  对集合中每个元素操作  ，在扁平化  [ [a,b],[a,d]] -> [a ,b,a,d]
# map    对集合中每个元素操作  [ [a,b],[a,d]] -> [[a,b],[a,d]]
df.select(field).filter(mdf[field] != '').rdd.flatMap(lambda g: [(v, 1) for v in map(lambda x: x['name'], json.loads(g[field]))])

df = df.filter(df.price != '面议').withColumn("price",df.price.cast(IntegerType()))

# 对多列进行不同的聚合操作, 并修改相应的列名
df.groupBy('job').agg(
    F.sum("salary").alias("sum_salary"),
    F.avg("salary").alias("avg_salary"),
    F.min("salary").alias("min_salary"),
    F.max("salary").alias("max_salary"),
    F.mean("salary").alias("mean_salary")
).show(truncate=False)      # truncate=False：左对齐

df.agg(F.sum("rain1h").alias("rain24h")).sort(F.desc("rain24h")

df_rain_sum.coalesce(1).write.csv("file:///home/analyse.csv")
df = spark.read.csv(filename,header = True)

F.date_format(df['time'],"yyyy-MM-dd").alias("date")
F.hour(df['time']).alias("hour")

                                              
df.filter(df['hour'].isin([2,8,12,20]))
                                              
F.format_number('avg_temperature',1)                                            

~~~





## 12 hadoop

~~~shell
hdfs dfs -mkdir -p /user/hadoop  #创建文件夹
hdfs dfs -put local_path_file hdfs_path # 上传到hdfs 文件系统中
hdfs dfs -get hdfs_path_file local_path # 下载到本地
~~~









## 13	flink



## 14 kafka 



## 15 zookeeper











## 16 git

~~~shell
原理
工作区（写代码） git add
暂存区（临时存储） git commit
本地库（历史版本） push 	(远程库)
github （外网）
码云   （国内）
gitlab（局域网）
IDEA 集成
工具 ： source tree
~~~

https://blog.csdn.net/qq_36150631/article/details/81038485?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_baidulandingword-0&spm=1001.2101.3001.4242

版本控制工具

1： 协同修改

2： 数据备份

3： 版本管理

4： 权限控制

5： 历史记录

6： 分支管理 

工作区  写代码  git add 

暂存区 临时存储 git commit 

历史版本  本地库 

### 代码托管中心

局域网 gitlab

外网 GitHub  码云 

push   本地库 推到代码托管中心  加入团队

Pull 

Clone  下载 自动初始化

fork 复制其他人库  ，可以pr （pull request ）merger 

项目级别 、仓库级别

git config 

系统用户级别：

git config --global

级别优先级：

就近原则  项目级别优先于系统用户级别 

分支的好处：

同时并行推送多个功能的分支

### 1:新建代码库

```shell
# 在当前目录新建一个Git代码库
$ git init  可以生成 .git 
# 新建一个目录，将其初始化为Git代码库
$ git init [project-name]
# 下载一个项目和它的整个代码历史
$ git clone [url]
```

### 2:配置

~~~shell
# 显示当前的Git配置
$ git config --list
# 编辑Git配置文件
$ git config -e [--global]
 # 设置提交代码时的用户信息
$ git config [--global] user.name "[name]"
$ git config [--global] user.email "[email address]"

~~~

### 3: 增加/删除文件

~~~shell
# 添加指定文件到暂存区
$ git add [file1] [file2] ...
# 添加指定目录到暂存区，包括子目录
$ git add [dir]
# 添加当前目录的所有文件到暂存区
$ git add .
# 添加每个变化前，都会要求确认
# 对于同一个文件的多处变化，可以实现分次提交
$ git add -p
# 删除工作区文件，并且将这次删除放入暂存区
$ git rm [file1] [file2] ...
# 停止追踪指定文件，但该文件会保留在工作区
$ git rm --cached [file]
# 改名文件，并且将这个改名放入暂存区
$ git mv [file-original] [file-renamed]
~~~

### 4：代码提交

~~~shell
# 提交暂存区到仓库区
$ git commit -m [message]
 # 提交暂存区的指定文件到仓库区
$ git commit [file1] [file2] ... -m [message]
 # 提交工作区自上次commit之后的变化，直接到仓库区
$ git commit -a
 # 提交时显示所有diff信息
$ git commit -v
 # 使用一次新的commit，替代上一次提交
# 如果代码没有任何新变化，则用来改写上一次commit的提交信息
$ git commit --amend -m [message]
 # 重做上一次commit，并包括指定文件的新变化
$ git commit --amend [file1] [file2] ..
~~~

### 5: 分支

~~~shell
# 列出所有本地分支
$ git branch
# 列出所有远程分支
$ git branch -r
# 列出所有本地分支和远程分支
$ git branch -a
# 新建一个分支，但依然停留在当前分支
$ git branch [branch-name]
# 新建一个分支，并切换到该分支
$ git checkout -b [branch]
# 新建一个分支，指向指定commit
$ git branch [branch] [commit]
# 新建一个分支，与指定的远程分支建立追踪关系
$ git branch --track [branch] [remote-branch]
# 切换到指定分支，并更新工作区
$ git checkout [branch-name]
# 切换到上一个分支
$ git checkout -
# 建立追踪关系，在现有分支与指定的远程分支之间
$ git branch --set-upstream [branch] [remote-branch]
# 合并指定分支到当前分支
$ git merge [branch]
# 选择一个commit，合并进当前分支
$ git cherry-pick [commit]
# 删除分支
$ git branch -d [branch-name]
# 删除远程分支
$ git push origin --delete [branch-name]
$ git branch -dr [remote/branch]
~~~

### 6: 标签

~~~shell
# 列出所有tag
$ git tag
# 新建一个tag在当前commit
$ git tag [tag]
# 新建一个tag在指定commit
$ git tag [tag] [commit]
# 删除本地tag
$ git tag -d [tag]
# 删除远程tag
$ git push origin :refs/tags/[tagName]
# 查看tag信息
$ git show [tag]
# 提交指定tag
$ git push [remote] [tag]
# 提交所有tag
$ git push [remote] --tags
# 新建一个分支，指向某个tag
$ git checkout -b [branch] [tag]
~~~

### 7: 查看信息

~~~shell
# 显示有变更的文件
$ git status
# 显示当前分支的版本历史
$ git log
# 显示commit历史，以及每次commit发生变更的文件
$ git log --stat
# 搜索提交历史，根据关键词
$ git log -S [keyword]
# 显示某个commit之后的所有变动，每个commit占据一行
$ git log [tag] HEAD --pretty=format:%s
# 显示某个commit之后的所有变动，其"提交说明"必须符合搜索条件
$ git log [tag] HEAD --grep feature
# 显示某个文件的版本历史，包括文件改名
$ git log --follow [file]
$ git whatchanged [file]

# 显示指定文件相关的每一次diff
$ git log -p [file]
# 显示过去5次提交
$ git log -5 --pretty --oneline
# 显示所有提交过的用户，按提交次数排序
$ git shortlog -sn
# 显示指定文件是什么人在什么时间修改过
$ git blame [file]
# 显示暂存区和工作区的差异
$ git diff
# 显示暂存区和上一个commit的差异
$ git diff --cached [file]
# 显示工作区与当前分支最新commit之间的差异
$ git diff HEAD

# 显示两次提交之间的差异
$ git diff [first-branch]...[second-branch]
# 显示今天你写了多少行代码
$ git diff --shortstat "@{0 day ago}"
# 显示某次提交的元数据和内容变化
$ git show [commit]
# 显示某次提交发生变化的文件
$ git show --name-only [commit]
# 显示某次提交时，某个文件的内容
$ git show [commit]:[filename]
# 显示当前分支的最近几次提交
$ git reflog
~~~

### 8: 远程同步

~~~shell
# 下载远程仓库的所有变动
$ git fetch [remote]
# 显示所有远程仓库
$ git remote -v
# 显示某个远程仓库的信息
$ git remote show [remote]
# 增加一个新的远程仓库，并命名
$ git remote add [shortname] [url] 
# 取回远程仓库的变化，并与本地分支合并
$ git pull [remote] [branch]
# 上传本地指定分支到远程仓库
$ git push [remote] [branch]
# 强行推送当前分支到远程仓库，即使有冲突
$ git push [remote] --force
# 推送所有分支到远程仓库
$ git push [remote] --all
~~~

### 9: 撤销

~~~shell
# 恢复暂存区的指定文件到工作区
$ git checkout [file]
 # 恢复某个commit的指定文件到暂存区和工作区
$ git checkout [commit] [file]
 # 恢复暂存区的所有文件到工作区
$ git checkout .
 # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变
$ git reset [file]
 # 重置暂存区与工作区，与上一次commit保持一致
$ git reset --hard
 # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变
$ git reset [commit]
 # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致
$ git reset --hard [commit]
# 重置当前HEAD为指定commit，但保持暂存区和工作区不变
$ git reset --keep [commit]
 # 新建一个commit，用来撤销指定commit
# 后者的所有变化都将被前者抵消，并且应用到当前分支
$ git revert [commit]
 # 暂时将未提交的变化移除，稍后再移入
$ git stash
$ git stash pop
~~~

### 9:远程仓库

~~~
git remote set-url origin <url>
git remote -v 
git remote add origin <url_project>
git remote rm origin 

~~~



ssh -T git@github.com

### 命令使用

~~~

git version 
git config --global --list # 查看全局配置信息
git config --global user.name "New_name"
git config --global user.email "***@**"
 
 # 1优先级  从local > global > system
 git config --local 只对仓库有效
 git config --global 对登录用户的所有仓库有效
 git config --sysytem 对系统所有用户有效 
 
 # 2创建仓库
 git  init  生成.git 文件夹 版本控制  文件存在项目   
 git init project_name  # 文件中每有项目 
 
 # 3提交版本
 git add file   # git add .所有文件  暂存区
 git commit -m '版本信息'
 git commit -a -m ''  #一步提交   不用git add 
 git commit --amend 版本补录，不产生新的版本
 
 
 
 # 4版本状态查看 
 git status 
 git status -s
 git status --short
 红色
 绿色 
 A  ： 添加
 M : 文件修改
 
 # 5查看历史版本
 git log
 git log --pretty=oneline
 git log -p # 显示版本之间的差别
 git log -5 -p 
 git log --stat 查看大体统计信息
 
 #6.gitignore 忽略文件  屏蔽某些文件不放到git（固定文件夹）
 glob 模式匹配
 * 0个或任意一个  [abc]  [0-9]  ？匹配任意一个  /目录  ！取反
 eg: 
 # 屏蔽src文件夹
 src/ 
 # python 缓存文件
 *.py[cod]
 
 #7 git diff 版本对比  在没有添加到暂存区之前 
 git diff 老版本 新版本
 git diff --staged
 
 #8 git rm 删除文件 
 git restore file   
 git restore --staged file  #添加到暂存区
 
 git rm file 本地删除
 git rm --cached file # 版本控制系统删除 本地还在
 git rm *.pyc
 # 9 git move 
 
 #10 git tag 标签
 git tag -a taname -m '提交信息'
 git tag tagname 轻量标签
 git tag -a tagname 补录标签(前几位) -m 'beizhu'
 git show tagname
 git tag # 显示所有标签 
 git tag -l v1.8* # 正则匹配
 git tag -d tagname 删除标签
 
 
 #11  checkout 检出
 git checkout 版本号
 git reflog
 git checkout  标签名
 
 # git alias 别名
 git config --global alias.st 'statues'
 git config --global alias.last 'log -1 HeAD'
 git config --global alias.logol log --pretty=oneline
 
 
 # git remote  远程地址
 git remote -v 
 git remote add project_name url_远程仓库地址  # origin 源头 
 git remote show project_name #查看远程分支的名称
 
 # git 远程操作
 git remote show project_name
 git push 远程仓库名 本地分支（master）  # 推送到远程
 git fetch project_name  # 没有合并分支
 git pull 远程分支  本地分支 # 自动合并
 
 #远程修改
 git remote rename old_name new_name
 git remote rm name # 删除远程地址 
 
 # 版本穿梭
 git reset --hard 地址号
 
 #分支
 git branch name #创建分支
 git branch -v
 git checkout 分支名  # 切换分支
 git merger  name # 把指定的分支合并到当前分支上

~~~

~~~html
https://mp.weixin.qq.com/s?__biz=MzI1NzI5NDM4Mw==&mid=2247484043&idx=1&sn=55c9bd30daffc67fec5be25e5bbb8f46&chksm=ea18ebcbdd6f62dd050259eb46012401b83adfd46adca5e3e5b15c711f3c001e10d5c6773f76&token=1374986981&lang=zh_CN&scene=21#wechat_redirect
~~~

### github 搜索技巧

~~~
xxx in:name  名称包含 项目名包含 xxx 的
xxx in:description 项目描述包含 xxx 的
xxx in:readme 项目的 README 文件中包含 xxx 的
Django stars:>=2048 要查找 Stars 数不小于 2048 的 Django 项目
Django forks:>2048 Forks 大于等于 2048
Django forks:100..200 stars:80..100
项目内搜索:可以在仓库页面上按："T" 
~~~







## 17 tensorflow









## 18 keras



## 19 PyTorch

### 1： basic konwledge

#### tensor

Tensor共有8个属性，上面四个与数据本身相关，下面四个与梯度求导相关

![image-20220627105426571](pic/image-20220627105426571.png)



~~~
data: 被包装的Tensor
grad: data的梯度
grad_fn: fn表示function的意思，记录我么创建的创建张量时用到的方法，比如说加法，乘法，这个操作在求导过程需要用到，Tensor的Function， 是自动求导的关键
requires_grad: 指示是否需要梯度， 有的不需要梯度
is_leaf: 指示是否是叶子节点（张量）
dtype: 张量的数据类型， 如torch.FloatTensor, torch.cuda.FloatTensor, 用的最多的一般是float32和int64(torch.long)
shape: 张量的形状， 如(64, 3, 224, 224)
device: 张量所在的设备， GPU/CPU， 张量放在GPU上才能使用加速。
~~~

tensor的创建 和操作：

~~~
numpy pandas 
torch.Tensor():直接创建
torch.from_numpy(ndarry) 注意：这个创建的Tensor与原ndarray共享内存, 当修改其中一个数据的时候，另一个也会被改动。
torch.zeros(): 依size创建全0的张量
torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False) :
torch.full_like()

torch.cat(tensors, dim=0, out=None): 将张量按维度dim进行拼接, tensors表示张量序列， dim要拼接的维度
torch.stack(tensors, dim=0, out=None): 在新创建的维度dim上进行拼接， tensors表示张量序列， dim要拼接的维度
torch.chunk(input, chunks, dim=0): 将张量按维度dim进行平均切分，
torch.index_select(input, dim, index, out=None): 在维度dim上，按index索引数据，返回值，以index索引数据拼接的张量

torch.reshape(input, shape)
torch.transpose(input, dim0, dim1):
torch.t(input): 2维张量的转置， 对矩阵而言，相当于torch.transpose(inpuot, 0,1)
torch.squeeze(input, dim=None, out=None): 压缩长度为1的维度， dim若为None，移除所有长度为1的轴，若指定维度，当且仅当该轴长度为1时可以被移除
~~~

![image-20220627110410106](pic/image-20220627110410106.png)

![image-20220627110559971](pic/image-20220627110559971.png)



#### Pytorch的动态图机制



计算图的搭建方式，可以将计算图分为动态图和静态图

- 静态图： 先搭建图，后运算。高效，不灵活（TensorFlow）
- 动态图： 运算与搭建同时进行。灵活，易调节（Pytorch）

这个就类似于旅游的时候你找旅行团还是自己去旅游一样，找旅行团的这种就类似于静态图，游览的路线和流程都已经确定，跟着走就行了。  如果你熟悉TensorFlow的话，会知道TensorFlow的计算方式，是先把图搭建好，然后开启一个会话， 在那里面才开始喂入数据进行流动计算



而自己去旅游这个就类似于动态图，游览的路线和流程都不确定，想去哪去哪，随时调整。Pytorch就是采用的这种机制，这种机制就是边建图边执行，从上面的例子中也能看出来，比较灵活， 有错误可以随时改，也更接近我们一般的想法。毕竟没有谁做一件事情之前就能把所有的流程都能规划好，一般人都是有一个大体的框架，然后一步一步边走边调整。

```
backward()背后其实是在调用后面 torch.autograd.backward()
backward()方法，还有一个比较常用的方法叫做：torch.autograd.grad()， 这个方法的功能是求取梯度
```

Pytorch自动求导机制使用的是[torch](https://so.csdn.net/so/search?q=torch&spm=1001.2101.3001.7020).autograd.backward方法， 功能就是自动求取梯度

tensors表示用于求导的张量，如loss。
retain_graph表示保存计算图， 由于Pytorch采用了动态图机制，在每一次反向传播结束之后，计算图都会被释放掉。如果我们不想被释放，就要设置这个参数为True
create_graph表示创建导数计算图，用于高阶求导。
grad_tensors表示多梯度权重。如果有多个loss需要计算梯度的时候，就要设置这些loss的权重比例。



![image-20220627114011350](pic/image-20220627114011350.png)



#### 数据读取机制

![image-20220627114237471](pic/image-20220627114237471.png)



1: Dataset:

~~~
torch.utils.data.Dataset(): 
Dataset抽象类， 所有自定义的Dataset都需要继承它，并且必须复写__getitem__()这个类方法。
_getitem__方法的是Dataset的核心，作用是接收一个索引， 返回一个样本


~~~

2：DataLoader

~~~
torch.utils.data.DataLoader():
构建可迭代的数据装载器, 我们在训练的时候，每一个for循环，每一次iteration，就是从DataLoader中获取一个batch_size大小的数据的。
dataset: Dataset类， 决定数据从哪读取以及如何读取
bathsize: 批大小
num_works: 是否多进程读取机制
shuffle: 每个epoch是否乱序
drop_last: 当样本数不能被batchsize整除时， 是否舍弃最后一批数据
~~~

#### 模型的创建

~~~
    构建子模块， 这个是在自己建立的模型（继承nn.Module）的__init__()方法
    拼接子模块， 这个是在模型的forward()方法中
~~~

![image-20220627115006810](pic/image-20220627115006810.png)



~~~
nn.Parameter
nn.functional
    激活函数系列(F.relu, F.sigmoid, F.tanh, F.softmax)
    模型层系列(F.linear, F.conv2d, F.max_pool2d, F.dropout2d, F.embedding)
    损失函数系列(F.binary_cross_entropy, F.mse_loss, F.cross_entropy)

    为了便于对参数进行管理， 一般通过继承nn.Module转换为类的实现形式， 并直接封装在nn模块下：
    激活函数变成(nn.ReLu, nn.Sigmoid, nn.Tanh, nn.Softmax)
    模型层(nn.Linear, nn.Conv2d, nn.MaxPool2d, nn.Embedding)
    损失函数(nn.BCELoss, nn.MSELoss, nn.CrossEntorpyLoss
~~~

`nn.Module`中，有8个重要的属性， 用于管理整个模型，他们都是以有序字典的形式存在着：

![image-20220627115349937](pic/image-20220627115349937.png)

_parameters： 存储管理属于nn.Parameter类的属性，例如权值，偏置这些参数
_modules: 存储管理nn.Module类， 比如LeNet中，会构建子模块，卷积层，池化层，就会存储在_modules中
_buffers: 存储管理缓冲属性， 如BN层中的running_mean， std等都会存在这里面
***_hooks: 存储管理钩子函数（5个与hooks有关的字典，这个先不用管）

![image-20220627115734188](pic/image-20220627115734188.png)









https://mmfusion.yuque.com/docs/share/b041bcc3-0090-41a7-9358-66f41e53edbc?  代码加速

~~~
class RNNBase(Module):
	...
    def __init__(self, mode, input_size, hidden_size,
                 num_layers=1, bias=True, batch_first=False,
                 dropout=0., bidirectional=False):
                 
 LSTM：
 input:   input, (h_0,c_0)   h_0,c_0 可以自己初始化，或者默认全为0
 (batch,seq_len , input_size)   batch_first=True
 output: (batch, seq_len, num_directions * hidden_size)
 (h_n,c_n) : 最后一个隐藏层的输出， c_n 最后一个细胞状态输出
 https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#bi-lstm-conditional-random-field-discussion
~~~

~~~python
https://blog.csdn.net/qq_39526294/article/details/104055944  bilstm+CRF
https://zhuanlan.zhihu.com/p/462634737
https://zhuanlan.zhihu.com/p/270028097
~~~



~~~

autograd 包提供Tensor所有操作的自动求导方法
class MyDataset(Dataset): 
    def __init__(self): 
        '''
        有两种写法：
        1、将全部数据都加载进内存里，适用于少量数据（上面那个例子就是全部加载）；
        2、当数据量或者标签量很大时，比如图片，就把这些数据或者标签放到文件或数据库里去，只需在此方法中初始化定义这些文件索引的列表即可。
        '''
        pass
    
    # 以下2个方法都是魔法方法
    def __getitem__(self, index): # 表示将来实例化这个对象后，它能支持下标（索引）操作，也就是能通过索引把里面的数据拿出来。
        pass
    def __len__(self):  # 返回数据集的长度
        pass
    
my_dataset = MyDataset() 
train_loader = DataLoader(dataset=my_dataset, # 传递数据集
                          batch_size=32, #一个小批量容量是多少
                          shuffle=True, # 数据集顺序是否要打乱，一般是要的。测试数据集一般没必要
                          num_workers=0) # 需要几个进程来一次性读取这个小批量数据
即Dataloader负责总的调度，命令Sampler定义遍历索引的方式，然后用索引去Dataset中提取元素
~~~

#### 卷积运算与卷积层

~~~
nn.Conv2d
nn.MaxPool2d
nn.AvgPool2d
线性层
nn.Linear(in_features, out_features, bias=True)

clas
~~~

激活函数：

![image-20220627120044294](pic/image-20220627120044294.png)

![image-20220627120059673](pic/image-20220627120059673.png)

![image-20220627120130457](pic/image-20220627120130457.png)

#### 权值初始化

 初始化好了，比如正好初始化到模型的最优解附近，那么模型训练起来速度也会非常的快， 但如果初始化不好，离最优解很远，那么模型就需要更多次迭代，有时候还会引发梯度消失和爆炸现象



方差一致性原则

**所以Xavier权重初始化，有利于缓解带有sigmoid，tanh的这样的饱和激活函数的神经网络的梯度消失和爆炸现象。**

Pytorch里面提供了很多权重初始化的方法，可以分为下面的四大类：

    针对饱和激活函数（sigmoid， tanh）：Xavier均匀分布， Xavier正态分布
    针对非饱和激活函数（relu及变种）：Kaiming均匀分布， Kaiming正态分布
    三个常用的分布初始化方法：均匀分布，正态分布，常数分布
    三个特殊的矩阵初始化方法：正交矩阵初始化，单位矩阵初始化，稀疏矩阵初始化：
------------------------------------------------
#### 损失函数

![image-20220628094809707](pic/image-20220628094809707.png)

##### 交叉熵损失CrossEntropyLoss

weight：    各类别的loss设置权值
ignore_index:   忽略某个类别
reduction：  计算模式，可为none/sum/mean, none表示逐个元素计算，这样有多少个样本就会返回多少个loss。 sum表示所有元素的loss求和，返回标量， mean所有元素的loss求加权平均（加权平均的含义下面会提到），返回标量。看了下面的原理就懂了



![image-20220628095611978](pic/image-20220628095611978.png)

![image-20220628095655257](pic/image-20220628095655257.png)



#### 优化器：

数据 -> 模型 -> 损失 -> 优化器 -> 迭代训练

- 导数： 函数在指定坐标轴上的变化率
- 方向导数： 指定方向上的变化率
- 梯度： 一个向量， 方向为方向导数取得最大值的方向

`zero_grad()`： 清空所管理参数的梯度， 这里注意Pytorch有一个特性就是**张量梯度不自动清零**

`step()`: 执行一步更新

`add_param_group()`: 添加参数组, 我们知道优化器管理很多参数，这些参数是可以分组的，我们对不同组的参数可以设置不同的超参数，finetune

`state_dict()`: 获取优化器当前状态信息**字典**

`load_state_dict()`: 加载状态信息字典，这两个方法用于模型断点的一个续训练， 所以我们在模型训练的时候，一般多少个epoch之后就要保存当前的状态信息。

##### 学习率和动量

![image-20220628101201106](pic/image-20220628101201106.png)

![image-20220628101216296](pic/image-20220628101216296.png)



学习率：

![image-20220628101442236](pic/image-20220628101442236.png)

![image-20220628101553266](pic/image-20220628101553266.png)

#### 正则化

**误差=偏差+方差+噪声**

- **偏差**度量了学习算法的期望预测与真实结果的偏离程度， 即刻画了学习算法本身的拟合能力
- **方差**度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响
- **噪声**则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界。

L1正则化的特点：

    不容易计算， 在零点连续但不可导， 需要分段求导
    L1模型可以将 一些权值缩小到零（稀疏）
    执行隐式变量选择。 这意味着一些变量值对结果的影响降为0， 就像删除它们一样
    其中一些预测因子对应较大的权值， 而其余的（几乎归零）
    由于它可以提供稀疏的解决方案， 因此通常是建模特征数量巨大时的首选模型
    它任意选择高度相关特征中的任何一个， 并将其余特征对应的系数减少到0
    L1范数对于异常值更具提抗力


L2正则化的特点：

```python
容易计算， 可导， 适合基于梯度的方法
将一些权值缩小到接近0
相关的预测特征对应的系数值相似
当特征数量巨大时， 计算量会比较大
对于有相关特征存在的情况， 它会包含所有这些相关的特征， 但是相关特征的权值分布取决于相关性。
对异常值非常敏感
相对于L1正则会更加准确

optim_normal = torch.optim.SGD(net_normal.parameters(), lr=0.01, momentum=0.9)
optim_wdecay = torch.optim.SGD(net_weight_decay.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-2)
```
------------------------------------------------
##### Dropout

只在训练的时候开启Dropout，而测试的时候是不用Dropout的，也就是说模型训练的时候会随机失活一部分神经元， 而测试的时候我们用所有的神经元，那么这时候就会出现这个数据尺度的问题， **所以测试的时候，所有权重都乘以1-drop_prob， 以保证训练和测试时尺度变化一致**

就得先用`.eval()`函数告诉网络一下子，训练的时候就用`.train()`函数告诉网络一下子。



##### BatchNormalization

- nn.BatchNorm1d
- nn.BatchNorm2d
- nn.BatchNorm3d

BN与LN的区别：

- LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；
- BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。



![image-20220628113000406](pic/image-20220628113000406.png)

### Tensorboard

![image-20220628114125799](pic/image-20220628114125799.png)

- log_dir: event file输出文件夹， 如果不设置的话，就会创建一个runs， 通常自己设置
- comment：不指定log_dir时， 文件夹后缀
- filename_suffix: event file文件名后缀

![image-20220628114315319](pic/image-20220628114315319.png)

![image-20220628114409005](pic/image-20220628114409005.png)

3.add_image

4 add_graph

- model: 模型，必须时nn.Module
- input_to_model: 输出给模型的数据
- verbose: 是否打印计算图结构信息

torchsummary

~~~
pip install torchsummary 
from torchsummary import summary
    model: pytorch模型
    input_size: 模型输入size
    batch_size: batch size
    device: “cuda” or “cpu”， 通常选CPU
~~~

#### hook 函数与CAM可视化

hook函数机制： 不改变模型的主体，实现额外功能，像一个挂件和挂钩。

Pytorch提供了四种hook函数：

    torch.Tensor.register_hook(hook): 针对tensor
    torch.nn.Module.register_forward_hook： 后面这三个针对Module
    torch.nn.Module.register_forward_pre_hook
    torch.nn.Module.register_backward_hook
------------------------------------------------


![image-20220628171541507](pic/image-20220628171541507.png)



### 模型的保存和加载



#### 序列化与反序列化

序列化就是说内存中的某一个对象保存到硬盘当中，以二进制序列的形式存储下来，这就是一个序列化的过程。 而反序列化，就是将硬盘中存储的二进制的数，反序列化到内存当中，得到一个相应的对象

Pytorch中序列化和反序列化的方法：

- torch.save(obj, f): obj表示对象， 也就是我们保存的数据，可以是模型，张量， dict等等， f表示输出的路径
- torch.load(f, map_location): f表示文件的路径， map_location指定存放位置， CPU或者GPU， 这个参数挺重要，在使用GPU训练的时候再具体说。

Pytorch的模型保存有两种方法， 一种是保存整个Module， 另外一种是保存模型的参数。

```python
保存和加载整个Module： 
torch.save(net, path)， 
torch.load(fpath)
保存模型参数：
torch.save(net.state_dict(), path), 
net.load_state_dict(torch.load(path))
```
#### 模型断点续训练

训练到了中途出现了一些意外情况，比如断电了，当再次来电的时候，我们肯定是希望模型在中途的那个地方继续往下训练，这就需要我们在模型的训练过程中保存一些断点，**所以模型训练过程中设置checkpoint也是非常重要的**。

### CPU vs GPU 

- torch.cuda.device_count(): 计算当前可见可用的GPU数
- torch.cuda.get_device_name(): 获取GPU名称
- torch.cuda.manual_seed(): 为当前GPU设置随机种子
- torch.cuda.manual_seed_all(): 为所有可见可用GPU设置随机种子
- torch.cuda.set_device(): 设置主GPU（默认GPU）为哪一个物理GPU（不推荐）

```
推荐的方式是设置系统的环境变量
os.environ.setdefault("CUDA_VISIBLE_DEVICES", "2,3")
```

#### 多GPU并行运算

多GPU并行运算的三步：分发 -> 并行计算 -> 收回结果整合

```
torch.nn.DataParallel()
主要参数：
    module: 需要包装分发的模型
    device_ids: 可分发的gpu, 默认分发到所有的可见可用GPU， 通常这个参数不管它，而是在环境变量中管这个。
    output_device: 结果输出设备， 通常是输出到主GPU
```





## 20 各种环境安装

### win

~~~
cleanmgr
~~~



### centos7

静态iP的配置

~~~shell
ip addr
vi /etc/sysconfig/network-scripts/ifcfg-ens33
BOOTPROTO=static静态IP地址
ONBOOT=yes 开机使用配置
PADDR=192.168.100.95（IP地址）
NETMASK=255.255.255.0（子网掩码）
GATEWAY=192.168.100.254（网关）
DNS1=8.8.8.8（首选DNS）
# 虚拟机的设置
https://blog.csdn.net/YeMaZhi/article/details/105043073
~~~

### java 环境

~~~shell
tar -xzvf jdk-8u241-linux-x64.tar.gz #下载安装包
mv jdk-8u241-linux-x64 jdk1.8.0
vi /etc/profile
export JAVA_HOME=/home/hadoop/softs/jdk1.8.0  # 修改这个路径
export JRE_HOME=${JAVA_HOME}/jre
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
export PATH=${JAVA_HOME}/bin:$PATH
source /etc/profile
# 查看java jdk 是否安装成功 
which java
java -version 
~~~

### mysql

~~~shell
https://blog.csdn.net/weixin_43451430/article/details/115553108

yum update -y
sudo yum install -y wget 
wget https://dev.mysql.com/get/mysql80-community-release-el7-3.noarch.rpm
sudo yum localinstall mysql80-community-release-el7-3.noarch.rpm -y
sudo yum install -y yum-utils
yum repolist enabled | grep "mysql.*-community.*"
yum repolist all | grep mysql

关闭MySQL8.0
sudo yum-config-manager --disable mysql80-community
开启MySQL5.7
sudo yum-config-manager --enable mysql57-community
yum repolist enabled | grep mysql
#安装MySQL
sudo yum install -y mysql-community-server
# 出现问题
yum module disable mysql
sudo yum install -y mysql-community-server
如果保存gpk  编辑文件
/etc/yum.repos.d/mysql-community.repo 
gpgcheck=0
启动MySQL
sudo service mysqld start
查看MySQL服务状态
sudo service mysqld status
初始化MySQL
sudo grep 'temporary password' /var/log/mysqld.log  # 查看密码
mysql -u root -p
初始化密码
ALTER USER 'root'@'localhost' IDENTIFIED BY 'Asdf!123456';
SHOW VARIABLES LIKE 'validate_password%'; 
#修改密码验证强度
set global validate_password_policy=LOW; 
set global validate_password_length=6;
#设置MySQL远程连接，在sql里面设置
GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456' WITH GRANT OPTION;
FLUSH PRIVILEGES;
systemctl enable mysqld
#MySQL的配置文件目录：
/etc/my.cnf
#配置防火墙
firewall-cmd --zone=public --add-port=3306/tcp --permanent
测试mysql 是否连通
yum-y install telnet
    1:telnet host port
    2:mysql -h ip -uroot -ppwd  
如果写入是乱码
查看数据库和表的编码 

~~~

### ssh

~~~shell
查看是否安装 
rpm -qa | grep ssh
ssh clinet
ssh server
# 安装 
sudo yum install openssh-clients
sudo yum install openssh-server
# 测试 
ssh localhost  
exit  退出ssh 连接

ssh 无密码登录 
# ssh-keygen 生成秘钥  
cd ~/.ssh/         # 若没有该目录，请先执行一次ssh localhost
ssh-keygen -t rsa   # 回车
cat id_rsa.pub >> authorized_keys  # 加入授权
chmod 600 ./authorized_keys    # 修改文件权限
~~~

### Anaconda

~~~
# conda 镜像配置 

conda config --remove-key channels 
conda config --show channels

conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --set show_channel_urls yes

pip install --index-url https://pypi.douban.com/simple pycocotools-windows
~~~



~~~shell
https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/  # 清华大学开源空间

# 下载
wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2021.05-Linux-x86_64.sh
# 安装
bash Anaconda3-2021.05-Linux-x86_64.sh
export anacoda_path=$PATH:/root/anaconda3/bin:$PATH

/home/wang_boyang/.bashrc

export -p

conda -V
conda init
conda config --set auto_activate_base false  # 不执行 conda 环境

conda info -e  # 查看所有虚拟环境
conda create -n env_name python=3.6
conda activate env_name
conda deactivate  env_name
conda list
conda remove --name env_name --all
conda env export 0n env_name >env.yml #导出环境到yml文件
conda env create -f env.yml  #根据yml文件创建环境
查看当前的环境变量：conda env config vars list
设置环境变量：conda env config vars set my_var=value
取消环境变量：conda env config vars unset my_var
# 源
conda config --show
conda config --get channels
#添加
conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/main/
#Conda Forge
conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/cloud/conda-forge/
#pytorch
conda config --add channels https://mirrors.bfsu.edu.cn/anaconda/cloud/pytorch/
#删除
conda config --remove channels CHANNEL
conda config --remove channels https://mirrors.bfsu.edu.cn/anaconda/pkgs/free/
~~~

conda 使用

~~~
conda --version
conda updata conda 
conda env list   conda info -e
conda create -n env_name python=o3.6
conda remove -n env_name --all
conda search package_name
conda install package_name
conda remove package_name
conda update package_name

conda activate 激活base环境
conda activate env_name
deactivate env_name

conda数据源管理
#显示目前conda的数据源有哪些
conda config --show channels
#添加数据源：例如, 添加清华anaconda镜像：
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --set show_channel_urls yes
#删除数据源
conda config --remove channels 

~~~

生成运行环境所依赖的包

~~~
pip生成requirement.txt

pip freeze > requirements.txt
pip install -r requirements.txt
~~~



### Jupyter notebook

~~~shell
conda install jupyter notebook
# 产生配置文件
jupyter notebook --generate-config

vim ~/.jupyter/jupyter_notebook_config.py
终端输入python:
from notebook.auth import passwd
passwd()
#设置密码 123456
'argon2:$argon2id$v=19$m=10240,t=10,p=8$shvXjdsctxUocNzSTj7HYg$C1rKoPk50/cLC3Px0BbL1Q'
# 在产生的配置文件中添加如下信息 
c.NotebookApp.ip='*'                     # 就是设置所有ip皆可访问  
c.NotebookApp.password = 'argon2:$argon2id$v=19$m=10240,t=10,p=8$ZzueEBxeIuJ3Uyf4Nolv5Q$CgYmdQqXn0RvLgCrVPS84Q'    # 上面复制的那个sha密文'  
c.NotebookApp.open_browser = False       # 禁止自动打开浏览器  
c.NotebookApp.port =8888                 # 端口
c.NotebookApp.notebook_dir = /home/jupyter_datas  #设置Notebook启动进入的目录
c.NotebookApp.allow_root = True # 为了安
systemctl stop firewalld.service
jupyter notebook --allow-root
conda install nb_conda_kernels

~~~

### hadoop

~~~shell
百度网盘 林子
http://dblab.xmu.edu.cn/blog/2441-2/
useradd -m hadoop -s /bin/bash   # 创建新用户hadoop
passwd hadoop  #修改密码
visudo   # 将hadoop 用户进行提权
hadoop ALL=(ALL) ALL

# hadoop伪分布式配置 
export HADOOP_HOME=/home/hadoop/softs/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

伪分布式需要修改2个配置文件 core-site.xml 和 hdfs-site.xml 
./etc/hadoop 文件夹中

core-site.xml文件
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

hdfs-site.xml 文件
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/hadoop/tmp/dfs/data</value>
    </property>
</configuration>


配置完成后，执行 NameNode 的格式化:
./bin/hdfs namenode -format
./sbin/start-dfs.sh


./sbin/start-dfs.sh
出现错误 
but there is no HDFS_NAMENODE_USER defined. Aborting operation.

start-dfs.sh和stop-dfs.sh添加下列参数
HDFS_DATANODE_USER=root
HADOOP_SECURE_DN_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root

start-yarn.sh和stop-yarn.sh文件，添加下列参数
#!/usr/bin/env bash
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
出现错误 
localhost: ERROR: JAVA_HOME is not set and could not be found.
vi ./etc/hadoop/hadoop-env.sh
 添加
 export JAVA_HOME=/home/hadoop/softs/jdk1.8.0
# 配置yarm
mapred-site.xml 文件
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

yarn-site.xml 文件
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
        </property>
</configuration>
然后就可以启动 YARN 了（需要先执行过 ./sbin/start-dfs.sh）
./sbin/start-yarn.sh      $ 启动YARN
./sbin/mr-jobhistory-daemon.sh start historyserver  # 开启历史服务器，才能在Web中查看任务运行情况
~~~



### Hbase

~~~
https://dblab.xmu.edu.cn/blog/2442-2/
单机模式配置
~~~

### hive

~~~sh
# 要把jdbc 复制到hive/lib 
mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar 
./bin/schematool -dbType mysql -initSchema
$ vi hive-site.xml
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.cj.jdbc.Driver</value>
    </property>
修改为
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    
    
hadoop-3.2.1（路径：hadoop\share\hadoop\common\lib）中该jar包为  guava-27.0-jre.jar；而hive-3.1.2(路径
~~~

### spark

~~~shell
cp ./conf/spark-env.sh.template ./conf/spark-env.sh
export SPARK_DIST_CLASSPATH=$(/home/hadoop/softs/hadoop/bin/hadoop classpath)

export SPARK_HOME=/home/hadoop/softs/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME


# 安装pyspark
pip install -U -i https://pypi.tuna.tsinghua.edu.cn/simple pyspark
pip install pyspark -i https://pypi.doubanio.com/simple/
pip install PyMySQL -i https://pypi.doubanio.com/simple/

export SPARK_HOME=/home/hadoop/softs/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME

export PYSPARK_PYTHON=/root/anaconda3/bin/python
export PYSPARK_DRIVER_PYTHON=/root/anaconda3/bin/python
~~~

### 安装linux 图形界面

~~~
# yum groupinstall "GNOME Desktop" "Graphical Administration Tools"
第三步：更新系统的运行级别。
# ln -sf /lib/systemd/system/runlevel5.target /etc/systemd/system/default.target
第四步：重启机器。启动默认进入图形界面。
# reboot
~~~

## 21: 网络架构

### CS

<img src="pic/image-20220507092944188.png" alt="image-20220507092944188" style="zoom:67%;" />

~~~
应用领域：
QQ
大型网络游戏
计算机发展初期用户去取数据，直接就去主机拿，从这里开始就分出了客户端和服务端。
客户端：用户安装的软件；
服务端：统一管理数据库的主机中的软件就叫做服务端，再后来服务端不只是管理数据，外加处理业务逻辑。
CS架构要求
用户操作系统安装客户端；产商操作系统部署服务端
每个用户需要独立安装软件、服务端升级也要每个用户升级

服务端统一处理有更好的安全性和稳定性而且升级比较容易，不过服务器负担就增加了。
客户端将负担分配到每个用户，从而可以节约服务器资源，安全性和稳定性可能会有一定的问题，但是升级比较麻烦，每个安装的客户端程序都需要升级，另外为了节省网络资源，通过网络传输的数据应该尽量减少！
~~~

### BS 

<img src="pic/image-20220507093120624.png" alt="image-20220507093120624" style="zoom:50%;" />

应用领域：

- 淘宝
- 京东

统一客户端即默认安装用户电脑中的浏览器，访问同种类的网站，具体业务的处理根据相应协议和标准提供通用的服务器程序，在不同的服务器处理。

<img src="pic/image-20220507093244435.png" alt="image-20220507093244435" style="zoom:50%;" />

**BS结构中TCP/IP模型中的网络接入层没有响应的协议，网络互联层是IP协议，传输层是TCP协议，应用层是HTTP协议，另外还是用到了DNS结构，而且在HTTP上层还有相应。**

<img src="pic/image-20220507093658678.png" alt="image-20220507093658678" style="zoom:50%;" />

### 计算机网络

实际上还有人把它划成五层、四层。

七层划分为：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层。

五层划分为：应用层、传输层、网络层、数据链路层、物理层。

四层划分为：应用层、传输层、网络层、网络接口层。

<img src="pic/image-20220507093834966.png" alt="image-20220507093834966" style="zoom:50%;" />

每层运行常见的物理设备：

<img src="pic/image-20220507093912877.png" alt="image-20220507093912877" style="zoom:50%;" />

#### 物理层

物理链接可以是光缆、电缆、双绞线、无线电波。中间传的是电信号，即010101...这些二进制位。

物理层功能：

主要是基于电器特性发送高低电压(电信号)，高电压对应数字1，低电压对应数字0

<img src="pic/image-20220507094139555.png" alt="image-20220507094139555" style="zoom:50%;" />



$\textcolor{red}{要让这些010010101001...有意思，人为的分组再适合不过了end}$

#### 数据链路层

数据链路层由来：单纯的电信号0和1没有任何意义，必须规定电信号多少位一组，每组什么意思

数据链路层的功能：定义了电信号的分组方式

数据链路层就是来对电信号来做分组的。以前每个公司都有自己的分组方式，后来形成了统一的标准，即以太网协议ethernet

ethernet规定：

一组电信号构成一个数据报，叫做'帧'，每一数据帧分成：报头head和数据data两部分

- head包含：(固定18个字节)

发送者／源地址，6个字节

接收者／目标地址，6个字节

数据类型，6个字节

- data包含：(最短46字节，最长1500字节)

- 数据报的具体内容：head长度＋data长度＝最短64字节，最长1518字节，超过最大限制就分片发送

**head**中包含的源和目标地址由来：ethernet规定接入internet的设备都必须具备网卡，发送端和接收端的地址便是指网卡的地址，即Mac地址

##### Mac地址

- Mac地址：每块网卡出厂时都被烧制上一个世界唯一的Mac地址，长度为48位2进制，通常由12位16进制数表示（前六位是厂商编号，后六位是流水线号）

##### 广播地址

计算机底层，只要在一个教室里（一个局域网），都是靠广播的方式，吼

<img src="pic/image-20220507095736942.png" alt="image-20220507095736942" style="zoom:80%;" />

广播出去以后，所有人都听得见，所有人都会拆开这个包，读发送者是谁，接收者是谁，只要接收者不是自己就丢弃掉。对计算机来说，它会看自己的Mac地址



比如说局域网1的pc1与局域网2的pc10如何通信？你在教室1（局域网1）吼，教室2（局域网2）的人肯定是听不见的。这就是跨网络进行通信，数据链路层就解决不了这个问题了，这就得靠网络层出面了。

#### 网络层

网络层功能：引入一套新的地址用来区分不同的广播域／子网，这套地址即网络地址

网络层的由来：有了ethernet、Mac地址、广播的发送方式，世界上的计算机就可以彼此通信了，问题是世界范围的互联网是由 一个个彼此隔离的小的局域网组成的，那么如果所有的通信都采用以太网的广播方式，那么一台机器发送的包全世界都会收到

![image-20220507095923820](pic/image-20220507095923820.png)



##### IP协议

网络层定义了一个IP协议：

Mac地址是用来标识你这个教室的某个位置，IP地址是用来标识你在哪个教室（哪个局域网

![image-20220507100112427](pic/image-20220507100112427.png)

最终变成

| -        |  -   |      - |
| :------- | :--: | -----: |
| 以太网头 | IP头 | IP数据 |

广泛采用的v4版本即IPv4，它规定网络地址由32位2进制表示
范围0.0.0.0-255.255.255.255

- 一个IP地址通常写成四段十进制数，例：172.16.10.1

 IP地址的两部分

1. 网络部分：标识子网
2. 主机部分：标识主机

子网掩码:
就是表示子网络特征的一个参数。它在形式上等同于IP地址，也是一个32位二进制数字

网络部分全部为1，主机部分全部为0

IP数据包也分为head和data部分，无须为IP包定义单独的栏位，直接放入以太网包的data部分

- head：长度为20到60字节
- data：最长为65,515字节

**以太网数据包的"数据"部分，最长只有1500字节。因此，如果IP数据包超过了1500字节，它就需要分割成几个以太网数据包，分开发送了。**



##### ARP协议

现在来看另一个问题，在吼之前怎么知道对方的Mac地址？

这就得靠ARP协议。

ARP协议功能：广播的方式发送数据包，获取目标主机的Mac地址

协议工作方式：每台主机IP都是已知的，例如：主机172.16.10.10/24访问172.16.10.11/24

1.首先通过IP地址和子网掩码区分出自己所处的子网

|   场景   |       数据包地址        |
| :------: | :---------------------: |
| 同一子网 | 目标主机Mac，目标主机IP |
| 不同子网 |   网关Mac，目标主机IP   |

2.分析172.16.10.10/24与172.16.10.11/24处于同一网络(如果不是同一网络，那么下表中目标IP为172.16.10.1，通过arp获取的是网关的Mac)

|     -      |   源Mac   |      目标Mac      |      源IP       |     目标IP      | 数据部分 |
| :--------: | :-------: | :---------------: | :-------------: | :-------------: | :------: |
| 发送端主机 | 发送端Mac | FF:FF:FF:FF:FF:FF | 172.16.10.10/24 | 172.16.10.11/24 |   数据   |

3.这个包会以广播的方式在发送端所处的自网内传输，所有主机接收后拆开包，发现目标IP为自己的，就响应，返回自己的Mac

ARP也是靠广播的方式发，ARP发送广播包的方式如下：

|     -      |   源Mac   |      目标Mac      |      源IP       |     目标IP      | 数据部分 |
| :--------: | :-------: | :---------------: | :-------------: | :-------------: | :------: |
| 发送端主机 | 发送端Mac | FF:FF:FF:FF:FF:FF | 172.16.10.10/24 | 172.16.10.11/24 |   数据   |

**目标Mac为12个F，我们叫广播地址，表达的意思是我想要获取这个目标IP地址**



#### 传输层

传输层的由来：网络层的IP帮我们区分子网，以太网层的Mac帮我们找到主机，然后大家使用的都是应用程序，你的电脑上可能同时开启qq，暴风影音，等多个应用程序。

那么我们通过IP和Mac找到了一台特定的主机，如何标识这台主机上的应用程序，答案就是端口，端口即应用程序与网卡关联的编号。

**传输层功能：建立端口到端口的通**信

端口范围0-65535，0-1023为系统占用端口

- 有了Mac地址+IP地址+端口，我们就能确定世界上独一无二的一台计算机上的应用程序

##### TCP协议

- 可靠传输，TCP数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常TCP数据包的长度不会超过IP数据包的长度，以确保单个TCP数据包不必再分割。

| -        |  -   |   -   |    - |
| :------- | :--: | :---: | ---: |
| 以太网头 | IP头 | TCP头 | 数据 |

|  应用程序  |  FTP  | TFTP | TELNET | SMTP | DNS  | HTTP | SSH  | MYSQL |
| :--------: | :---: | :--: | :----: | :--: | :--: | :--: | :--: | :---: |
|  熟知端口  | 21,20 |  69  |   23   |  25  |  53  |  80  |  22  | 3306  |
| 传输层协议 |  TCP  | UDP  |  TCP   | TCP  | UDP  | TCP  | TCP  |  TCP  |

TCP把连接作为最基本的对象，每一条TCP连接都有两个端点，这种端点我们叫作套接字（socket），它的定义为端口号拼接到IP地址即构成了套接字，例如，若IP地址为192.3.4.16 而端口号为80，那么得到的套接字为192.3.4.16:80。

**TCP报文首部**

1. 源端口和目的端口，各占2个字节，分别写入源端口和目的端口；
2. 序号，占4个字节，TCP连接中传送的字节流中的每个字节都按顺序编号。例如，一段报文的序号字段值是 301 ，而携带的数据共有100字段，显然下一个报文段（如果还有的话）的数据序号应该从401开始；
3. 确认号，占4个字节，是期望收到对方下一个报文的第一个数据字节的序号。例如，B收到了A发送过来的报文，其序列号字段是501，而数据长度是200字节，这表明B正确的收到了A发送的到序号700为止的数据。因此，B期望收到A的下一个数据序号是701，于是B在发送给A的确认报文段中把确认号置为701；
4. 数据偏移，占4位，它指出TCP报文的数据距离TCP报文段的起始处有多远；
5. 保留，占6位，保留今后使用，但目前应都位0；
6. 紧急URG，当URG=1，表明紧急指针字段有效。告诉系统此报文段中有紧急数据；
7. 确认ACK，仅当ACK=1时，确认号字段才有效。TCP规定，在连接建立后所有报文的传输都必须把ACK置1；
8. 推送PSH，当两个应用进程进行交互式通信时，有时在一端的应用进程希望在键入一个命令后立即就能收到对方的响应，这时候就将PSH=1；
9. 复位RST，当RST=1，表明TCP连接中出现严重差错，必须释放连接，然后再重新建立连接；
10. 同步SYN，在连接建立时用来同步序号。当SYN=1，ACK=0，表明是连接请求报文，若同意连接，则响应报文中应该使SYN=1，ACK=1；
11. 终止FIN，用来释放连接。当FIN=1，表明此报文的发送方的数据已经发送完毕，并且要求释放；
12. 窗口，占2字节，指的是通知接收方，发送本报文你需要有多大的空间来接受；
13. 检验和，占2字节，校验首部和数据这两部分；
14. 紧急指针，占2字节，指出本报文段中的紧急数据的字节数；
15. 选项，长度可变，定义一些其他的可选的参数。

**TCP三次握手和四次挥手**

![](pic/image-20220507101637007.png)

TCP**三次握手**

- 最开始的时候客户端和服务器都是处于CLOSED状态。主动打开连接的为客户端，被动打开连接的是服务器。

1. TCP服务器进程先创建传输控制块TCB，时刻准备接受客户进程的连接请求，此时服务器就进入了LISTEN（监听）状态；
2. TCP客户进程也是先创建传输控制块TCB，然后向服务器发出连接请求报文，这是报文首部中的同部位SYN=1，同时选择一个初始序列号 seq=x ，此时，TCP客户端进程进入了 SYN-SENT（同步已发送状态）状态。TCP规定，SYN报文段（SYN=1的报文段）不能携带数据，但需要消耗掉一个序号。
3. TCP服务器收到请求报文后，如果同意连接，则发出确认报文。确认报文中应该 ACK=1，SYN=1，确认号是ack=x+1，同时也要为自己初始化一个序列号 seq=y，此时，TCP服务器进程进入了SYN-RCVD（同步收到）状态。这个报文也不能携带数据，但是同样要消耗一个序号。
4. TCP客户进程收到确认后，还要向服务器给出确认。确认报文的ACK=1，ack=y+1，自己的序列号seq=x+1，此时，TCP连接建立，客户端进入ESTABLISHED（已建立连接）状态。TCP规定，ACK报文段可以携带数据，但是如果不携带数据则不消耗序号。
5. 当服务器收到客户端的确认后也进入ESTABLISHED状态，此后双方就可以开始通信了。

<img src="pic/image-20220507102837385.png" alt="image-20220507102837385" style="zoom:50%;" />



**TCP四次挥手**

- 数据传输完毕后，双方都可释放连接。最开始的时候，客户端和服务器都是处于ESTABLISHED状态，然后客户端主动关闭，服务器被动关闭。

1. 客户端进程发出连接释放报文，并且停止发送数据。释放数据报文首部，FIN=1，其序列号为seq=u（等于前面已经传送过来的数据的最后一个字节的序号加1），此时，客户端进入FIN-WAIT-1（终止等待1）状态。 TCP规定，FIN报文段即使不携带数据，也要消耗一个序号。
2. 服务器收到连接释放报文，发出确认报文，ACK=1，ack=u+1，并且带上自己的序列号seq=v，此时，服务端就进入了CLOSE-WAIT（关闭等待）状态。TCP服务器通知高层的应用进程，客户端向服务器的方向就释放了，这时候处于半关闭状态，即客户端已经没有数据要发送了，但是服务器若发送数据，客户端依然要接受。这个状态还要持续一段时间，也就是整个CLOSE-WAIT状态持续的时间。
3. 客户端收到服务器的确认请求后，此时，客户端就进入FIN-WAIT-2（终止等待2）状态，等待服务器发送连接释放报文（在这之前还需要接受服务器发送的最后的数据）。
4. 服务器将最后的数据发送完毕后，就向客户端发送连接释放报文，FIN=1，ack=u+1，由于在半关闭状态，服务器很可能又发送了一些数据，假定此时的序列号为seq=w，此时，服务器就进入了LAST-ACK（最后确认）状态，等待客户端的确认。
5. 客户端收到服务器的连接释放报文后，必须发出确认，ACK=1，ack=w+1，而自己的序列号是seq=u+1，此时，客户端就进入了TIME-WAIT（时间等待）状态。注意此时TCP连接还没有释放，必须经过2∗ *∗MSL（最长报文段寿命）的时间后，当客户端撤销相应的TCB后，才进入CLOSED状态。
6. 服务器只要收到了客户端发出的确认，立即进入CLOSED状态。同样，撤销TCB后，就结束了这次的TCP连接。可以看到，服务器结束TCP连接的时间要比客户端早一些。

<img src="pic/image-20220507103020688.png" alt="image-20220507103020688" style="zoom:50%;" />



为什么客户端最后还要等待2MSL？

MSL（Maximum Segment Lifetime），TCP允许不同的实现可以设置不同的MSL值。

1. 保证客户端发送的最后一个ACK报文能够到达服务器，因为这个ACK报文可能丢失，站在服务器的角度看来，我已经发送了FIN+ACK报文请求断开了，客户端还没有给我回应，应该是我发送的请求断开报文它没有收到，于是服务器又会重新发送一次，而客户端就能在这个2MSL时间段内收到这个重传的报文，接着给出回应报文，并且会重启2MSL计时器。
2. 防止类似与“三次握手”中提到了的“已经失效的连接请求报文段”出现在本连接中。客户端发送完最后一个确认报文后，在这个2MSL时间中，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样新的连接中不会出现旧连接的请求报文。

为什么建立连接是三次握手，关闭连接确是四次挥手呢？

建立连接的时候，服务器在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，服务器收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，而自己也未必全部数据都发送给对方了，所以己方可以立即关闭，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送，从而导致多了一次。如果已

经建立了连接，但是客户端突然出现故障了怎么办？

TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。





##### UDP协议

- 不可靠传输，”报头”部分一共只有8个字节，总长度不超过65,535字节，正好放进一个IP数据包。

| -        |  -   |   -   |    - |
| :------- | :--: | :---: | ---: |
| 以太网头 | IP头 | UDP头 | 数据 |

$\textcolor{red}{TCP报文}$

<img src="pic/image-20220507101345377.png" alt="image-20220507101345377" style="zoom: 80%;" />





#### 应用层

应用层由来：用户使用的都是应用程序，均工作于应用层，互联网是开发的，大家都可以开发自己的应用程序，数据多种多样，必须规定好数据的组织形式

**应用层功能：规定应用程序的数据格式。**

TCP协议可以为各种各样的程序传递数据，比如Email、WWW、FTP等等。那么，必须有不同协议规定电子邮件、网页、FTP数据的格式，这些应用程序协议就构成了”应用层”

<img src="pic/image-20220507101830311.png" alt="image-20220507101830311" style="zoom:80%;" />

![image-20220507101933934](pic/image-20220507101933934.png)

#### Socket抽象层

Socket是在应用层和传输层之间的一个抽象层，它把TCP/IP层复杂的操作抽象为几个简单的接口供应用层调用已实现进程在网络中通信。

![image-20220507103342604](pic/image-20220507103342604.png)

~~~
https://storage.googleapis.com/alphafold-databases/casp14_versions/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt.tar.gz

SOURCE_URL=http://wwwuser.gwdg.de/~compbiol/data/hhsuite/databases/hhsuite_dbs/old-releases/pdb70_from_mmcif_200401.tar.gz


https://storage.googleapis.com/alphafold-databases/reduced_dbs/bfd-first_non_consensus_sequences.fasta.gz

https://storage.googleapis.com/alphafold-databases/casp14_versions/uniclust30_2018_08_hhsuite.tar.gz

SOURCE_URL="https://storage.googleapis.com/alphafold-databases/reduced_dbs/bfd-first_non_consensus_sequences.fasta.gz"

# 解压
gunzip -c bfd-first_non_consensus_sequences.fasta.gz > bfd_small
du -h file_path
~~~

## 22-Selenium

~~~python
diver.get(url)
diver.current_url
driver.back() # 返回上一页
driver.forward() # 下一页
driver.refresh() # 刷新页面
driver.title() # 获取标题
win+shfit+s
~~~

<img src="pic/image-20220714210834210.png" alt="image-20220714210834210" style="zoom:50%;" />

![image-20220714210955984](pic/image-20220714210955984.png)

![image-20220714211109116](pic/image-20220714211109116.png)

![image-20220714211234486](pic/image-20220714211234486.png)

![image-20220714224952604](pic/image-20220714224952604.png)

![image-20220714231046113](pic/image-20220714231046113.png)

![image-20220714234507779](pic/image-20220714234507779.png)

