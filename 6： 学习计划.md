#  学习计划

## 机器学习方法梳理

### 回归

~~~

~~~



1. 分类

   ~~~
   
   ~~~

   

2. 聚类和降维

3. 经典算法

4. 序列建模

5. 其他

## 深度学习



https://developer.huawei.com/consumer/cn/forum/topic/0202744487054880144?fid=0101592429757310384

深度学习的网络设计如下图所示：

- **神经网络常见层**
    全连接层、激活层、BN层、Dropout层、卷积层、池化层、循环层、Embedding层、Merege层等
- **网络配置**
    损失函数、优化器、激活函数、性能评估、初始化方法、正则项等
- **网络训练流程**
    预训练模型、训练流程、数据预处理（归一化、Embedding）、数据增强（图片翻转旋转曝光生成海量样本）等

![image-20220508112545595](pic/image-20220508112545595.png)

全连接层：

隐藏层的输入和输出都有关联，即全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。

![image-20220508112700073](pic/image-20220508112700073.png)

激活函数相当于一个过滤器或激励器，它把特有的信息或特征激活，常见的激活函数包括softplus、sigmoid、relu、softmax、elu、tanh等。

- 对于隐藏层，我们可以使用relu、tanh、softplus等非线性关系；
- 对于分类问题，我们可以使用sigmoid（值越小越接近于0，值越大越接近于1）、softmax函数，对每个类求概率，最后以最大的概率作为结果；
- 对于回归问题，可以使用线性函数（linear function）来实验

![image-20220508112920687](pic/image-20220508112920687.png)

常用的激活函数Sigmoid、tanh、ReLU、Leaky ReLU曲线如下图所示：

<img src="pic/image-20220508112957824.png" alt="image-20220508112957824" style="zoom:50%;" />

**优化器选择:**

存在梯度变化后，会有一个迭代的方案，这种方案会有很多选择。优化器有很多种，但大体分两类：

- 一种优化器是跟着梯度走，每次只观察自己的梯度，它不带重量
- 一种优化器是带重量的

它也是机器学习中最重要或最基础的线性优化。七种常见的优化器包括：

- class tf.train.GradientDescentOptimizer
- class tf.train.AdagradOptimizer
- class tf.train.AdadeltaOptimizer
- class tf.train.MomentumOptimizer
- class tf.train.AdamOptimizer
- class tf.train.FtrlOptimizer
- class tf.train.RMSPropOptimizer

CNN 

RNN 

LSTM

GRU

Glove

Wored2vec

FasterText

ELOM

Text2CNN 

Attention

**Transformer**

### **Bert** 系列模型

#### BERT

两阶段新模式（预训练+Finetuning）







Denoising Autoencoder  DAE

Bert中的一些细节:

- 在输入上，Bert的输入是两个segment，其中每个segment可以包含多个句子，两个segment用[SEP]拼接起来。

- 模型结构上，使用Transformer，这点跟Roberta是一致的。

- 学习目标上，使用两个目标： 

- - Masked Language Model(MLM): 其中15%的token要被Mask，在这15%里，有80%被替换成[Mask]标记，有10%被随机替换成其他token，有10%保持不变。
  - Next Sentence Prediction: 判断segment对中第二个是不是第一个的后续。随机采样出50%是和50%不是。

- Optimizations: 

- - Adam, beta1=0.9, beta2=0.999, epsilon=1e-6, L2 weight decay=0.01
  - learning rate, 前10000步会增长到1e-4, 之后再线性下降。
  - dropout=0.1
  - GELU激活函数
  - 训练步数：1M
  - mini-batch: 256
  - 输入长度: 512

- Data 

- - BookCorpus + English Wiki = 16GB

**BPE:**

#### **BERT WWM** 

 **WWM (Whole Word Masking)**

**遮盖整个词**

 整词掩码





#### RoBERTa 

Dynamic masking  ： 动态mask 

​	each sequence is masked in 10 different ways over the 40 epochs of training

​	original masking is performed during data processing 

Roberta尝试了一种动态的方式，说是动态，其实也是用静态的方式实现的，把数据复制10份，每一份中采用不同的Mask。这样就有了10种不同的Mask数据。

optimization hyperparameters : 优化超参数 

batch-size : 变大 256 ->2K or 8K  **训练步数从1M降到500K**

将adam 的 beta2 从0.999 -> 0.98

在更长的序列上训练，修改输入格式：FULL-SENTENCES+移除NSP任务

Text Encoding：采用更大的byte-level的BPE[词典](https://www.zhihu.com/search?q=词典&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"170656789"})   

Data :  增加了数据

 

Roberta在如下几个方面对Bert进行了调优：

- Masking策略——静态与动态
- 模型输入格式与Next Sentence Prediction
- Large-Batch
- 输入编码
- 大语料与更长的训练步数

#### **ERNIE 模型**

就引入命名实体（Named Entity）外部知识，**遮盖掉实体单元**，进行训练

#### **SpanBERT **

适用于指代消解 ，**在抽取式问答上表现好**

提出了**更好的 Span Mask 方案**，也再次展示了随机遮盖连续一段字要比随机遮盖掉分散字好；

通过**加入 Span Boundary Objective (SBO) 训练目标**，增强了 BERT 的性能，特别在一些与 Span 相关的任务，如抽取式问答；

用实验获得了和 XLNet 类似的结果，发现**不加入 Next Sentence Prediction (NSP) 任务，直接用连续一长句训练效果更好**。

相同点：模型架构相同

不同点：bert训练任务用了  **Masked LM，Next Sentence Prediction (NSP)**

spanbert训练用了 **Masked LM，Span Boundary Objective (SBO)**



**span masking** 

**A random process to mask spans of tokens** 

Single sentence training 

**a single contiguous segment of text for each training sample** 

**Span boudary objectiove  SBO** 

对于为什么 NSP 没有用，这里，SpanBERT 作者们给出了下面两个解释：

1. 相比起两句拼接，一句长句，**模型可以获得更长上下文**（类似 XLNet 的一部分效果）；
2. 在 NSP 的负例情况下，基于另一个文档的句子来预测词，会**给 MLM 任务带来很大噪音**。

于是 SpanBERT 就没采用 NSP 任务，**直接一句长句，然后 MLM 加上 SBO 任务来进行预训练**。

SpanBERT对Bert的改进主要体现在对mask方式的改进，丢弃NSP任务和增加SBO（Span Boundary Objective）任务。其改进点如下：

Bert是随机mask输入序列中的字，这样能很简单地推测出字之间的搭配，这样会让本来应该有强相关的一些连在一起的字词，在训练时是割裂开来的。难以建立词中各个字之间的关联信息。针对这一短板Bert-wwm与ERNIE分别对更改了mask策略，Bert-wwm是mask所有能够连续组成词的字，ERNIE是mask所有能够连续组成实体和短语的字。



SpanBERT的做法是根据几何分布，先随机选择一段（span）的长度，之后再根据均匀分布随机选择这一段的起始位置，最后按照长度遮盖。文中使用几何分布取 p=0.2，最大长度只能是 10，利用此方案获得平均采样长度分布。



Span Boundary Objective 是该论文加入的新训练目标，希望被遮盖 Span 边界的词向量，能学习到 Span 的内容。或许作者想通过这个目标，让模型在一些需要 Span 的下游任务取得更好表现。具体做法是，在训练时取 Span 前后边界的两个词，这两个词不在 Span 内，然后用这两个词向量加上 Span 中被遮盖掉词的位置向量，来预测原词。

这样做的目的是：增强了 BERT 的性能，为了让模型让模型在一些需要 Span 的下游任务取得更好表现，特别在一些与 Span 相关的任务，如抽取式问答。

**去除NSP任务**

XLNet 中发现NSP不是必要的，而且两句拼接在一起使单句子不能俘获长距离的语义关系，所以作者剔除了NSP任务，直接一句长句做MLM任务和SBO任务。这样做的目的是：剔除没有必要的预训练任务，并且使模型获取更长距离的语义依赖信息。

#### MASS   

弥补BERT在NLP中生成能力不足的问题

MASS在一系列包括机器翻译，文本总结，对话生成的三个生成任务上进行了微调

#### **BART ：**

**Token掩码**：与BERT的MLM一致，即随机选择token并进行mask；

**Token删除**：随机选择token进行删除，模型学习的是哪些位置删除了token;

**文本填充**：对文本进行Span Masking，与SpanBERT不同的是，这里每个Span被替换为一个 ![[公式]](https://www.zhihu.com/equation?tex=%5B%5Cmathbb+M%5D) 符号。文本填充的目的是使模型学习Span中有多个个token被mask掉了；

**句子排列**：文章按照句子终止符（句号，叹号等）分成句子的序列，然后按照这个序列进行打乱；

**文档旋转**：随机选择一个位置，然后将这个位置旋转至文章开始，这个任务是让模型学习文章的开始位置。

#### XLM 



#### ALBERT

  compact Model 

Factorized embedding parameterization :

1:wordpiece embedding  size E is tied with the hidden layer size H 

​	VxE  ExH  

2: Cross-layer sharing 

3: inter-sentence coherence loss 

​	NSP contains both topical and ordering information 

​	Topical cues help more  model utilizes more

​	Sop focuses on ordering not topical cues

4: additional data and removing dropout 

 





### Transformer-XL

Transformer 存在issue：

context fragmentation 

把文本拆成一个一个的句子，然后将所有的句子统一成一样的长度，多退少补的感觉。这样就导致文本的语义信息断开了.

transformer-xl:

- segment-level recurrence
- relative postional encoding

将前面片段的隐层信息进行缓存，计算当前片段隐层信息的时候再进行使用

导致片段的绝对位置都是一样，提出了相对位置编码

![image-20220512100600889](pic/image-20220512100600889.png)

![image-20220512105740184](pic/image-20220512105740184.png)

### XLnet

**自回归语言模型（Autoregressive LM）**

**自编码语言模型（Autoencoder LM）**

AR : 

- full auto-regressive dependence
- free from ariticial Noise
- no Bidirectional context

AE:

- independecent assumption (Maskz之间)
- artificial Noise : [MASK]
- Natural bidirectioanl Context

**issues :**

1. independence assumption : ignore the dependency between masks  (两个mask 之间可能存在relation )
2. inputs  noise : discrepancy between pre-traing and fine-tuning



**Permutation Language Model : XLnet**

Goal : use AR and bidirectional contexts for prediction

- Sample a factorization order  对语序进行排列， 然后采样。
- determine the attention masks based on the order 采取attention 掩码机制
- optimize a standard language modeing objective  

Sequence order is not shuffled .

attention masks are changed to reflect factorization order.

**Two-stream self-attention**

标准的AR 没有位置 信息， 从左向右 

reduced to predicting a bag of  words 

Solution : condition the distribution on the position

对某个token 位置：

如果预测 某个位置， 可以用它的位置， 但是不包含它的context 

如果要预测其他位置， 应该包含这个位置

Content Stream:  包含自己  predict other tokens

Query Stream   ： 不encoder 自己  predict the current token 



每一层都要计算 context stream 和query stream  后面才能计算 

![image-20220512175638165](pic/image-20220512175638165.png)

e :  token embedding 

w : postional embedding  + 学习的参数 



<img src="pic/image-20220512162834973.png" alt="image-20220512162834973" style="zoom:80%;" />

![image-20220512163019529](pic/image-20220512163019529.png)

预训练阶段最终预测只使用query stream，因为content stream已经见过当前token了。在精调阶段使用content stream，又回到了传统的self-attention结构。

另外，因为不像MLM只用预测部分token，还需要计算permutation，XLNet的计算量更大了，因此作者提出了partial prediction进行简化，即只预测后面1/K个token。

为了学习到更长距离的信息，作者沿用了自己的Transformer-XL。

总体读下来，还是感觉有很多地方可以自己琢磨一下，其实作者最后的实验也不是太充分，没有和BERT做充分的平等比较。不过新的想法还是难得的：

1. Permutation Language Modeling：先给我们统一了之前语言模型的思想框架（AR or AE），再一个permutation把两者的优点结合起来，而且整体框架又回归到了AR，感觉生成模型的新SOTA指日可待。
2. Transformer-XL + Relative segment encoding：这个不是作者重点强调的，但却让我觉得很有用处，目前短文本的任务还好，文本一长难度就会上去，段落级甚至文章级，这两个操作让我看到了NLU在长文本上取得更大成果的可能。

Apply Deep learning 

~~~~
学习记录
2022-5-11： 
8-1
attention 
encoder-decoder  attention  querty decoder 的时间点
self-attention   Q ，K，V 自己学

multi-head attention  多角度学习信息
idea : allow words to interact with one another
	Map  K,Q,V to lower dimensional sapeces
	Apply attention ,concatenate outputs
	Liner transformer
MulitHead(Q,k,V) = Concat(head1, ...headh)W
headi = attention(QW,kW,VW)

Why Scaled Dot-product attention ?
problem : when dk gets larger, the variance of qTk increase 
q and k are random variables with mean 0 and variance 1
qTk has mean 0 and virance dk
virance 1 is preferred 
solution :  scale by sqrt(dk)
Transformer 
non-recurrent encoder-decoder for MT(machine translate)

BERT:
contextualized word representations 
bidirection lstm  不是同时看到两边 
~~~~

~~~
NER : 

word2vec
BERT: Bidirecitonal Encoder representations from transformers 
ERNIE : Enhandced representation through knoledge integration

~~~







~~~
SRCNN
《Learning a Deep Convolutional Network for Image Super-Resolution》
整篇论文的创新点有：
(1) 使用了一个卷积神经网络来进行超分，端到端的学习低分辨率与超分辨率之间的映射。
(2) 将提出的神经网络模型与传统的稀疏编码方法之间建立联系，这种联系还指导用来设计神经网络模型。
(3) 实验结果表明深度学习方法可以用于超分中，可以获得较好的质量和较快的速度。

整个的模型架构非常的简单，先是对于输入图片进行双三次插值采样到高分辨空间，然后使用一层卷积进行特征提取，再用ReLU进行非线性映射，最后使用一个卷积来进行重建，使用MSE来作为重建损失。中间一个插曲是将传统用于超分的稀疏编码算法进行了延伸，可以看作是一种具有不同非线性映射的卷积神经网络模型。

~~~

Encoder :  BERT  SpanBERT

Generating tasks : GPT-2 GPT-3  Lamda

GPT-J code generate 

LaMDA :  Language Models for Dialog Applications 









## AutoEncoder 



## VAE

## GAN

~~~
https://www.bilibili.com/video/BV1Bg4y187WC?spm_id_from=333.337.search-card.all.click

https://developer.huawei.com/consumer/cn/forum/topic/0202744487054880144?fid=0101592429757310384

https://zhuanlan.zhihu.com/p/28853704 gan 的证明
~~~

GANs（Generativeadversarial networks）对抗式生成网络

- Generative：生成式模型
- Adversarial：采取对抗的策略
- Networks：网络

![image-20220508115412966](pic/image-20220508115412966.png)

整个公式的具体含义如下：

- 式子由两项构成，x表示真实图片，z表示输入G网络的噪声，而G(z)表示G网络生成的图片。
- D(x)表示D网络判断**真实图片是否真实**的概率（因为x就是真实的，所以对于D来说，这个值越接近1越好）。
- D(G(z))是**D网络判断G生成的图片是否真实的概率。**
- G的目的：G应该希望自己生成的的图片越接近真实越好。
- D的目的：D的能力越强，D(x)应该越大，D(G(x))应该越小，这时V(D,G)会变大，因此式子对于D来说是求最大（max_D）。
- trick：为了前期加快训练，生成器的训练可以把log(1-D(G(z)))换成-log(D(G(z)))损失函数。

 GAN图片生成:

**第一步（左图）**：希望判决器尽可能地分开真实数据和我生成的数据。那么，怎么实现呢？我的真实数据就是input1（Real World  images），我生成的数据是input2（Generator）。input1的正常输出是1，input2的正常输出是0，对于一个判决器（Discriminator）而言，我希望它判决好，首先把生成器固定住（虚线T），然后生成一批样本和真实数据混合给判决器去判断。此时，经过训练的判决器变强，即固定生成器且训练判决器。

**第二步（右图）**：固定住判决器（虚线T），我想办法去混淆它，刚才经过训练的判决器很厉害，此时我们想办法调整生成器，从而混淆判别器，即通过固定判决器并调整生成器，使得最后的输出output让生成的数据也输出1（第一步为0）

~~~python
GAN的核心就是这些，再简单总结下，即：
    步骤1是在生成器固定的时候，我让它产生一批样本，然后让判决器正确区分真实样本和生成样本。（生成器标签0、真实样本标签1）
    步骤2是固定判决器，通过调整生成器去尽可能的瞒混判决器，所以实际上此时训练的是生成器。（生成器的标签需要让判决器识别为1，即真实样本）
    
for 迭代 in range(迭代总数):
    for batch in range(batch_size):
        新batch = input1的batch + input2的batch (batch加倍)
        for 轮数 in range(判别器中轮数):
           步骤一 训练D
        步骤二 训练G
        
https://www.infoq.cn/article/2w5ew5i3g*vkgc5elqgy
    
~~~

~~~
https://reiinakano.github.io
https://reiinakano.com/
~~~

![image-20220510104339221](pic/image-20220510104339221.png)







## 关系抽取

~~~
https://www.bilibili.com/video/BV1pT4y1D7H5/?spm_id_from=333.788
~~~

## 知识图谱

~~~
https://www.bilibili.com/video/BV1ev4y1o7zj?p=1&share_medium=android&share_plat=android&share_source=COPY&share_tag=s_i&timestamp=1613741813&unique_k=PxZy11


https://github.com/wangle1218/KBQA-for-Diagnosis
~~~

~~~
https://www.bilibili.com/video/BV1CQ4y1z7Y6/?spm_id_from=333.788.recommend_more_video.18  point network
~~~



## 时间序列模型

~~~
https://www.bilibili.com/video/BV1F441187xt?spm_id_from=333.337.search-card.all.click
~~~



https://github.com/dragen1860/TensorFlow-2.x-Tutorials

https://developer.huawei.com/consumer/cn/forum/topic/0202744487054880144?fid=0101592429757310384



## 强化学习

Reinforcement Learning 

分类：

不理解环境 Model-Free RL

理解环境  Model-Based RL



基于概率 policy-Based RL 

基于价值 Value-Based RL 



回合更新

单步更新



在线学习

离线学习

- Actor-Critc
- Q  Learning
- Sarsa
- Policy Gradients



openAI gym

Tkinter





#### Q-learing 

agent : 智能体

environment  : 环境

envirnoment : observation reward 

state:  状态  就是智能体**观察**到的当前环境的**部分或者全部特征**。 

Pi  策略  pilicy
$$
\pi(a|s) = P(A=a|S=s)
$$


**状态空间**就是智能体能够观察到的特征数量。

环境的特征可能有许多，但只有智能体能够观察到的特征才算是状态。所以我们也用Observation（观察的英文）表示状态。

未来是充满不确定性的，不确定性既包含在我们的策略，也包含在环境之中

action: 行为

reward 矩阵 R :  状态为行，行为为列的

状态转移：

<img src="pic/image-20220516115527577.png" alt="image-20220516115527577" style="zoom:80%;" />





value function  is a prediction of future reward 

Q-value function gives expected total reward  

不确定性来自两个方面：1.智能体的行动选择（策略）。2.环境的不确定性。

<img src="pic/image-20220514173209278.png" alt="image-20220514173209278" style="zoom: 50%;" />



 在强化学习中，我们会用奖励R作为智能体学习的引导，期望智能体获得尽可能多的奖励。

但更多的时候，我们并不能单纯通过R来衡量一个动作的好坏

我们必须用长远的眼光来看待问题。我们要把未来的奖励也计算到当前状态下，再进行决策。

为了方便，我们希望可以有一种方法衡量我做出每种选择价值。这样，我只要看一下标记，以后的事情我也不用理了，我选择那个动作价值更大，就选那个动作就可以了。



 评估**动作**的价值，我们称为**Q值**，它代表了智能体选择这个动作后，一直到最终状态**奖励总和**的**期望**； 

评估**状态**的价值，我们称为**V值**：它代表了智能体在这个状态下，一直到最终状态的**奖励总和**的**期望**。

**V值是会根据不同的策略有所变化的！**

与V值不同，Q值和策略并没有直接相关，而与环境的状态转移概率相关，而环境的状态转移概率是不变的。



构建一个Q 矩阵：表示agent 从经验中学到的知识

![image-20220511161959902](pic/image-20220511161959902.png)



#### Sersa

![image-20220512210931289](pic/image-20220512210931289.png)



![image-20220512214126657](pic/image-20220512214126657.png)





~~~
https://zhuanlan.zhihu.com/c_1215667894253830144

~~~



DQN 

TD算法存在两个缺点：

~~~
1： waste of experience    
丢弃
2： correlated updates 
Consecutive state 连续状态   strongly correlate which is bad 


~~~



DQN：

1. 记忆库
2. 神经网络计算Q值
3. 暂时冻结q_target参数 切断相关性

优化过程：

Experience Replay : 经验回放

Prioritizedd Experience Replay  优先经验回放

Use importance sampling instead of uniform sampling .  根据重要程度进行采样

~~~
重要程度进行排序 ： TD error  
big TD  error ->high probability  >small learning rate 


SumTree 排序  提高排序效率
叶子节点是存的P 重要程度值 
https://zhuanlan.zhihu.com/p/47578210

https://zhuanlan.zhihu.com/p/47578210
~~~

SumTree 算法：

~~~


~~~

Double DQN ： 在选择动作的时候使用

![image-20220614160357348](pic/image-20220614160357348.png)



时间序列预测统计方法：

ARIMA是一种非常流行的时间序列预测统计方法

### 1\.推荐基本知识入门

~~~
1： 重点在review 梳理

LR 

矩阵分解

协同过滤

apori关联分析

FP-groth

fm

推荐系统：
一种主动的信息过滤系统，将信息过滤的过程由用户主动搜索变成系统主动推送
信息超载 长尾现象

~~~







### 4\.传统机器学习方法梳理

1. 回归问题

   - ~~~
     多元回归 线性和非线性
     
     
     逻辑回归 广义上的回归
     ~~~

2. 分类问题

   - ~~~
     感知机 percptron
     
     朴素贝叶斯 
     
     SVM  Support Vector Machine

     决策树  Decision Tree
     ~~~
     
   - ~~~
     集成学习 ensenble learning
     
     随机森林 
     
     Aadboost
     
     GBDT 
     
     XGBoost
     
     LightGBM
     ~~~

3. 聚类和降维问题

   - ~~~
     KNN
     K-means
     PCA
     SVD 矩阵分解
     LDA 潜在语义分析
     ~~~

   

4. 经典算法

- ~~~
  EM算法
  
  最大熵算法
  
  ~~~



​	5\. 序列建模

- ~~~
  HMM
  MEMM
  CRF
  ~~~

### 5\. 深度学习梳理

1. ~~~
   DNN 
   
   前向传播
   

   反向传播
   
   https://www.csie.ntu.edu.tw/~miulab/s109-adl/
   ~~~
~~~
   
   
   
2. ~~~
   CNN
   
   卷积核
   
   池化
   
   感受野
   
   空洞卷积
   
vgg
   
   renet
   
   yoole
   
   
   目标检查
~~~

3. ~~~
   RNN：
   
   attention:
   https://zhuanlan.zhihu.com/p/35739040
   
   
   LSTM
   
   
   GRU
   
   
   Tree-LSTM
   
   Autoencoder
   
   
   
   ~~~
   
   
   
4. ~~~
   NLP 模型
   语言模型：
   	n-gram 
   向量表示
       one-hot
       word bags 词袋模型
       tf-idf
       共现矩阵
       Glove
   特征提取器：
   	transformer
   Encoder-Decoder结构
       seq2seq
       attention  
   word Embedding：
       神经网络模型 全连接
       word2Vec
       fastText
       textcnn
       elmo
       gpt
       bert
       bobert
       aibert
       xlnet
   GLUE 数据集
   
   
   ~~~
   
5. ### 应用和模型结构

   - ~~~
     文本分类和情感分析
     
     
     ~~~

   - ~~~
     命名实体识别 词性 分词 
     
     
     ~~~

   - ~~~
     关系抽取
     
     
     ~~~

   - ~~~
     机器阅读理解 MRC 
     
     抽取式
     
     生成式
     ~~~

   - ~~~
     富文本知识抽取 
     LayoutLM
     LauoutLMv2
     PIck
     ~~~

   - ~~~
     推荐 和搜索 
     
     
     ~~~

   - ~~~
     问答 
     
     
     ~~~

   - ~~~
     知识图谱
     
     
     ~~~



### 







### 7：图

#### 图的基本性质

~~~
图：
有向图
无向图
图的表示： 邻接矩阵 adjacency 
图的性质： 
度： degree 
子图：
联通路： 任意两个节点有联通路径
非连通图： 联通分量 图的最大联通子图

最短路径：
度的中心性：N_度 /(n-1)
特征向量的中心性： 某个节点的
中介中心性： 经过该节点的最短路径 / 其余两两节点的最短距离
连接中心性： n-1 / 节点到其他节点最短路径之和
PageRank:  
HITS : 
~~~

#### Graph Embedding:

1. DeepWalk 无向图

2. LINE Large-scale Information Network Embedding 

   - 一阶 局部结构信息
   - 二阶 节点的邻居 共享邻居节点的可能性

3.  node2vec

   同质性 ： homophily

   结构等价性： structural equivalence 

   BFS 

   DFS

4. struct2vec

   

5. SDNE Structural Deep Network

   

图神经网络

GCN：

GraphSAGE: SAmple and aggreGate:

采样和聚合

https://github.com/ttttong/BERT-BILSTM-GCN-CRF-for-NER/tree/master/bilstm_crf

https://www.freesion.com/article/35141157682/



HAN:





### 8： 强化学习







### 9：GAN 对比学习







### 11: NLP 基本技术

##### 1：数据增强

~~~
pip install textda

1. 同义词替换（SR: Synonyms Replace）：不考虑stopwords，在句子中随机抽取n个词，然后从同义词词典中随机抽取同义词，并进行替换。
2. 随机插入(RI: Randomly Insert)：不考虑stopwords，随机抽取一个词，然后在该词的同义词集合中随机选择一个，插入原句子中的随机位置。该过程可以重复n次。
3. 随机交换(RS: Randomly Swap)：句子中，随机选择两个词，位置交换。该过程可以重复n次。
4. 随机删除(RD: Randomly Delete)：句子中的每个词，以概率p随机删除。



可能有人会问：使用EDA技术后，改变后的数据和原数据的特征空间分布与其标签是否一致？（通过训练词向量， 看前后是否接近）
作者同样做了一个实验，在产品评论数据集上，使用RNN结合EDA数据增强进行训练，将最后一层的输出作为文本特征，使用tSNE进行可视化，得到如下结果：可以发现，数据增强后的数据分布与原始数据的分布非常吻合。这也意味着EDA对于句子的修改，并没有改变数据分布以及标签的分布。

# 可以看效果 
α参数粗略地表示“每次扩充改变的句子中单词的百分比”，纵轴是模型增益。

5.回译

6：Similar Embedding  替换
7： BERT  mask  生成不同的句子 
9： seq2seq2


~~~

EDA：EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks》

~~~
1. 同义词替换（SR: Synonyms Replace）：不考虑stopwords，在句子中随机抽取n个词，然后从同义词词典中随机抽取同义词，并进行替换。

2随机插入(RI: Randomly Insert)：不考虑stopwords，随机抽取一个词，然后在该词的同义词集合中随机选择一个，插入原句子中的随机位置。该过程可以重复n次。
3. 随机交换(RS: Randomly Swap)：句子中，随机选择两个词，位置交换。该过程可以重复n次。
4. 随机删除(RD: Randomly Delete)：句子中的每个词，以概率p随机删除。

当训练数据很小时，模型更容易过拟合，这时建议多生成一些数据增强的样本。当训练数据很大时，大量增加数据增强样本可能没有帮助，因为模型本身可能已经能够泛化。实验结果如下：

一条句子生成4条新句子效果最佳；
一个句子改变自身百分之10左右的词效果最佳
~~~









2: 数据不平衡 （长尾分布）

~~~
1： 从数据层面
重采样（re-sampling）
欠采样 为平衡数据类分布的移除大类数据的非启发式的方法
过采样 尝试通过增加稀有样本的数量来平衡数据集

但因过采样容易overfit到minor class，无法学到更鲁棒易泛化的特征，往往在非常不平衡数据上表现会更差；而欠采样则会造成major class严重的信息损失，导致欠拟合发生。
SMOTE（Synthetic Minority Oversampling Technique），合成少数类过采样技术 smote方法：
思想概括起来就是在少数类样本之间进行插值来产生额外的样本

SMOTE会随机选取少数类样本用以合成新样本，而不考虑周边样本的情况，这样容易带来两个问题：

    如果选取的少数类样本周围也都是少数类样本，则新合成的样本不会提供太多有用信息。这就像支持向量机中远离margin的点对决策边界影响不大。
    如果选取的少数类样本周围都是多数类样本，这类的样本可能是噪音，则新合成的样本会与周围的多数类样本产生大部分重叠，致使分类困难。
 
 针对 SMOTE 算法存在的边缘化和盲目性等问题,很多人纷纷提出了新的改进办法,在一定程度上改进了算法的性能,但还存在许多需要解决的问题。
 衍生出 SMOTEBoost、Borderline-SMOTE、Kmeans-SMOTE等  
    
2： 数据合成
经典方法SMOTE  是基于随机过采样算法的一种改进方案
mixup




3 分类算法层面
单类学习：在正负样本极度不平衡的情况下，只为其中一类建模，类似于异常检测，one-class svm (其实就用规则单独为某一类做处理也行啊)
代价敏感学习：关注错误代价较高类别的样本,以分类错误总代价最低为诊断算法的优化目标
集成学习方法：建立n个模型，每个模型使用稀有类别的所有样本和丰富类别的n个不同样本 balanceCascade
Focal Loss
类别加权Loss


4：重加权（re-weighting）：对不同类别（甚至不同样本）分配不同权重。注意这里的权重可以是自适应的。此类方法的变种有很多，有最简单的按照类别数目的倒数来做加权[7]，按照“有效”样本数加权[8]，根据样本数优化分类间距的loss加权[9]，等等。

dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是 「暂时」，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。
Dropout类似于bagging ensemble减少variance。也就是投通过投票来减少可变性。
全连接层部分使用dropout
~~~

~~~
https://zhuanlan.zhihu.com/p/348373259  优化器
https://zhuanlan.zhihu.com/p/256393726
~~~

![image-20220525101004442](pic/image-20220525101004442.png)





Weight Normalization和Batch Normalization都属于参数重写（Reparameterization）的方法

Weight Normalization是对网络权值W进行normalization（L2 norm）

Batch Normalization是对网络某一层输入数据进行normalization

Batch Normalization  **纵向规范化**

~~~
Batch Normalization不能直接用于RNN，进行normalization操作
1、RNN处理的Sequence是变长的；
2、RNN是基于time step计算，如果直接使用Batch Normalization处理需要保存每个time step下mini btach的均值和方差，效率低且占内存

Batch Normalization基于一个mini batch的数据计算均值和方差而不是基于整个Training set来做相当于进行梯度计算式引入噪声
因此，Batch Normalization不适用于对噪声敏感的强化学习、生成模型（Generative model：GAN，VAE）使用

BN 比较适用的场景是：每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多
~~~

Weight Normalization：

~~~
重写深度学习网络的权重W的方式来加速深度学习网络参数收敛，没有引入minbatch的依赖，适用于RNN（LSTM）网络
不需要额外的存储空间来保存mini batch的均值和方差同时实现Weight Normalization时，对深度学习网络进行正向信号传播和反向梯度计算带来的额外计算开销也很小因此，要比采用Batch Normalization进行normalization操作时，速度快。
~~~



Layer Normalization   **横向规范化**

~~~
LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间


~~~







春招：

~~~
1： 网易  推荐算法工程师  2022-02-22 杭州 笔试挂
2： 字节 算法工程师-今日头条/西瓜视频/番茄小说 2022-02-22 简历挂 上海
3： 长安 车联网 大数据  重庆 
4： 药明生物 
5： 

~~~



### 12： 数学基本知识



Random variabe :  

Probability Denstity Function : PDF  概率密度分布







1 : 3门问题

蒙特卡洛方法： 随机采样 求平均

布丰投针  求圆周率 pi

MCMC采样

https://www.cnblogs.com/pinard/p/6625739.html

https://www.bilibili.com/video/BV17D4y1o7J2/?p=2&spm_id_from=pageDriver

马尔科夫链模型的状态转移矩阵收敛到的稳定概率分布与我们的初始状态概率分布无关



https://github.com/youhuangla/Note/blob/main/web/github.md



数据增强：

硬化LOSS

软化LOSS

Focal Loss 



数据采样：

github.com/wmathor



交叉熵：

最大释然估计









#### 















#### 



#### 











12: NER 



13: 关系抽取



14： 知识图谱





